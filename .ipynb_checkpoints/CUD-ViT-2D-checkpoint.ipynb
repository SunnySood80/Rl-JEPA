{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7ca0ae4-dfec-45d9-be52-2da2be04f654",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f61c141b-d594-4ab5-b119-17cb37c381a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "ImageNet-100 already exists. Skipping download.\n",
      "ADE20K already exists. Skipping download.\n",
      "Ready.\n"
     ]
    }
   ],
   "source": [
    "import os, torch\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # syncs CUDA so you see the true Python stack\n",
    "torch.autograd.set_detect_anomaly(True)   # pinpoints the backward op\n",
    "\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import socket\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Check if datasets already exist\n",
    "\n",
    "imagenet_path = os.path.join('.', 'imagenet-100')\n",
    "ade_path = os.path.join('.', 'ADEChallengeData2016')\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "\n",
    "# Load ImageNet-100 and save to visible folder\n",
    "if os.path.exists(imagenet_path):\n",
    "    print(\"ImageNet-100 already exists. Skipping download.\")\n",
    "else:\n",
    "    print(\"Downloading ImageNet-100...\")\n",
    "    try:\n",
    "        img100 = load_dataset(\"clane9/imagenet-100\")\n",
    "        img100.save_to_disk(imagenet_path)\n",
    "        print(\"ImageNet-100 saved to ./imagenet-100/\")\n",
    "        print(f\"Train samples: {len(img100['train'])}\")\n",
    "        print(f\"Val samples: {len(img100['validation'])}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ImageNet-100 failed: {e}\")\n",
    "\n",
    "# Download ADE20K manually if not exists\n",
    "if os.path.exists(ade_path):\n",
    "    print(\"ADE20K already exists. Skipping download.\")\n",
    "else:\n",
    "    print(\"Downloading ADE20K...\")\n",
    "    try:\n",
    "        socket.setdefaulttimeout(60)\n",
    "        url = \"http://data.csail.mit.edu/places/ADEchallenge/ADEChallengeData2016.zip\"\n",
    "        zip_path = \"ADEChallengeData2016.zip\"\n",
    "        urllib.request.urlretrieve(url, zip_path)\n",
    "        \n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall('.')\n",
    "        os.remove(zip_path)\n",
    "        print(\"ADE20K downloaded\")\n",
    "    except Exception as e:\n",
    "        print(f\"ADE20K download failed: {e}\")\n",
    "\n",
    "print(\"Ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d3c5b2f-9fdd-45de-ac71-2b36f25cbe62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from PIL import Image\n",
    "from datasets import load_from_disk\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from math import ceil\n",
    "\n",
    "# Reproducibility\n",
    "seed = 1337\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "IMG_SIZE = 128\n",
    "\n",
    "# =============================================================================\n",
    "# JEPA ImageNet Dataset for Pretraining\n",
    "# =============================================================================\n",
    "class JEPADataset(Dataset):\n",
    "    def __init__(self, root=\"./imagenet-100\", split=\"train\", img_size=128):\n",
    "        self.dataset = load_from_disk(root)[split]\n",
    "        self.img_size = img_size\n",
    "        # Image transforms\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(img_size, scale=(0.2, 1.0)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load and transform image\n",
    "        img = self.dataset[idx][\"image\"].convert(\"RGB\")\n",
    "        img_tensor = self.transform(img)\n",
    "        return {\n",
    "            \"image\": img_tensor\n",
    "        }\n",
    "\n",
    "# =============================================================================\n",
    "# ADE20K Dataset for Segmentation\n",
    "# =============================================================================\n",
    "class ADE20KDataset(Dataset):\n",
    "    def __init__(self, root=\"ADEChallengeData2016\", split=\"training\", img_size=128):\n",
    "        self.img_dir = os.path.join(root, \"images\", split)\n",
    "        self.ann_dir = os.path.join(root, \"annotations\", split)\n",
    "        self.img_size = img_size\n",
    "        self.items = []\n",
    "        \n",
    "        # Find image-mask pairs\n",
    "        for img_path in glob.glob(os.path.join(self.img_dir, \"*.jpg\")):\n",
    "            stem = os.path.splitext(os.path.basename(img_path))[0]\n",
    "            ann_path = os.path.join(self.ann_dir, stem + \".png\")\n",
    "            if os.path.exists(ann_path):\n",
    "                self.items.append((img_path, ann_path))\n",
    "        self.items.sort()\n",
    "        \n",
    "        # Image transforms\n",
    "        self.img_transform = transforms.Compose([\n",
    "            transforms.Resize((img_size, img_size), interpolation=Image.BILINEAR),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, ann_path = self.items[idx]\n",
    "        \n",
    "        # Load image and mask\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(ann_path)\n",
    "        \n",
    "        # Apply transforms\n",
    "        img = self.img_transform(img)\n",
    "        mask = mask.resize((self.img_size, self.img_size), resample=Image.NEAREST)\n",
    "        mask = torch.from_numpy(np.array(mask, dtype=\"int64\"))\n",
    "        \n",
    "        return img, mask\n",
    "\n",
    "def jepa_collate(batch):\n",
    "    images = torch.stack([item[\"image\"] for item in batch])\n",
    "    return {\"images\": images}\n",
    "\n",
    "def ade_collate(batch):\n",
    "    imgs, masks = zip(*batch)\n",
    "    return {\"images\": torch.stack(imgs), \"masks\": torch.stack(masks)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1b4e19-61ea-4ecc-b5cc-805adefcfd79",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Setup Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c1a374-0879-4594-87f2-4393d27b32bc",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b07abc4-50b4-4d6c-86df-40de563a7aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def compute_patch_grid(image_shape, patch_size):\n",
    "    \"\"\"\n",
    "    image_shape expected: (C, H, W)\n",
    "    \"\"\"\n",
    "    _, H, W = image_shape\n",
    "    n_h = H // patch_size\n",
    "    n_w = W // patch_size\n",
    "    P = n_h * n_w\n",
    "    cropped_shape = (n_h * patch_size, n_w * patch_size)\n",
    "    return n_h, n_w, P, cropped_shape\n",
    "\n",
    "\n",
    "def extract_patch_embeddings_from_feature_map(feats: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    feats: [N, D, n_h, n_w]  -> returns [N, P, D]  (P = n_h * n_w)\n",
    "    \"\"\"\n",
    "    N, D, n_h, n_w = feats.shape\n",
    "    # Move D to last, then flatten spatial dims\n",
    "    return feats.permute(0, 2, 3, 1).reshape(N, -1, D)\n",
    "\n",
    "\n",
    "def compute_denoising_loss(self, denoised_prediction, original_input):\n",
    "    # Downsample target to match denoised prediction size\n",
    "    target_downsampled = F.interpolate(\n",
    "        original_input, \n",
    "        size=denoised_prediction.shape[-2:], \n",
    "        mode='bilinear', \n",
    "        align_corners=False\n",
    "    )\n",
    "    return F.mse_loss(denoised_prediction, target_downsampled)\n",
    "\n",
    "def compute_reconstruction_loss(preds: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute reconstruction loss for predicted vs target embeddings\n",
    "    \"\"\"\n",
    "    return F.mse_loss(preds, targets, reduction=\"mean\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def update_ema(target_net: nn.Module, online_net: nn.Module, tau: float):\n",
    "    \"\"\"\n",
    "    Update target network parameters using exponential moving average\n",
    "    \"\"\"\n",
    "    for t_param, s_param in zip(target_net.parameters(), online_net.parameters()):\n",
    "        t_param.data.mul_(tau).add_(s_param.data, alpha=1 - tau)\n",
    "\n",
    "def unpatchify_embeddings(emb: torch.Tensor, n_h: int, n_w: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Convert patch embeddings back to 2D feature map\n",
    "    emb: [N, P, D] -> [N, D, n_h, n_w]\n",
    "    \"\"\"\n",
    "    N, P, D = emb.shape\n",
    "    emb_4d = emb.view(N, n_h, n_w, D)\n",
    "    return emb_4d.permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "def generate_fi1_mask(fi1_shape: tuple, mask_ratio: float = 0.5, patch_size: int = 8, device='cuda'):\n",
    "    B, D, H8, W8 = fi1_shape  # e.g., [B, D, 28, 28] for 224x224 images\n",
    "    \n",
    "    # Calculate number of patches\n",
    "    n_patches_h = H8 // patch_size  # 28/8 = 3\n",
    "    n_patches_w = W8 // patch_size  # 28/8 = 3\n",
    "    total_patches = n_patches_h * n_patches_w  # 9 patches total\n",
    "    \n",
    "    num_masked = int(mask_ratio * total_patches)  # e.g., 4 patches masked\n",
    "    \n",
    "    # Generate mask\n",
    "    fi1_mask = torch.zeros(B, H8 * W8, dtype=torch.bool, device=device)\n",
    "    \n",
    "    for b in range(B):\n",
    "        # Randomly select which patches to mask\n",
    "        masked_patch_ids = torch.randperm(total_patches, device=device)[:num_masked]\n",
    "        \n",
    "        for patch_id in masked_patch_ids:\n",
    "            # Convert patch_id to patch coordinates\n",
    "            ph = patch_id // n_patches_w\n",
    "            pw = patch_id % n_patches_w\n",
    "            \n",
    "            # Convert to pixel coordinates in Fi1\n",
    "            h_start = ph * patch_size\n",
    "            h_end = min(h_start + patch_size, H8)\n",
    "            w_start = pw * patch_size  \n",
    "            w_end = min(w_start + patch_size, W8)\n",
    "            \n",
    "            # Mask this patch in flattened Fi1\n",
    "            for h in range(h_start, h_end):\n",
    "                for w in range(w_start, w_end):\n",
    "                    fi1_mask[b, h * W8 + w] = True\n",
    "    \n",
    "    return fi1_mask  # [B, H8*W8]\n",
    "\n",
    "def apply_fi1_mask_tokens(fi1_features: torch.Tensor, fi1_mask: torch.Tensor, mask_token: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Apply masking to Fi1 features using learned mask tokens\n",
    "    \n",
    "    Args:\n",
    "        fi1_features: (B, D, H8, W8) Fi1 feature maps\n",
    "        fi1_mask: (B, H8*W8) boolean mask\n",
    "        mask_token: (1, D, 1, 1) learned mask token\n",
    "    \n",
    "    Returns:\n",
    "        masked_fi1: Fi1 with mask tokens at masked positions\n",
    "    \"\"\"\n",
    "    B, D, H8, W8 = fi1_features.shape\n",
    "    \n",
    "    # Reshape mask to match feature dimensions\n",
    "    mask_2d = fi1_mask.reshape(B, H8, W8).unsqueeze(1).expand(-1, D, -1, -1)\n",
    "    \n",
    "    # Replace masked positions with mask token\n",
    "    masked_fi1 = torch.where(mask_2d, mask_token.expand(B, D, H8, W8), fi1_features)\n",
    "    \n",
    "    return masked_fi1\n",
    "\n",
    "def visualize_jepa_patch_quality(\n",
    "    original: torch.Tensor,\n",
    "    predicted_features: torch.Tensor,\n",
    "    target_features: torch.Tensor,\n",
    "    patch_mask: torch.Tensor,\n",
    "    epoch: int,\n",
    "    save_path: str,\n",
    "    patch_size: int = 16,\n",
    "):\n",
    "\n",
    "    # ----- robust image-to-display -----\n",
    "    def _to_display_img(x: torch.Tensor) -> np.ndarray:\n",
    "        x = x.detach().cpu()\n",
    "        if x.ndim == 3 and x.shape[0] in (1, 3):\n",
    "            xc = x.clone()\n",
    "            mn, mx = float(xc.min()), float(xc.max())\n",
    "            if 0.0 <= mn and mx <= 1.0:\n",
    "                pass  # already [0,1]\n",
    "            elif -3.5 <= mn <= 3.5 and -3.5 <= mx <= 3.5:\n",
    "                # assume ImageNet norm\n",
    "                mean = torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n",
    "                std  = torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)\n",
    "                xc = xc * std + mean\n",
    "            else:\n",
    "                # min-max to [0,1]\n",
    "                xc = (xc - mn) / (max(mx - mn, 1e-6))\n",
    "            img = xc.permute(1, 2, 0).numpy()\n",
    "        else:\n",
    "            arr = x.numpy()\n",
    "            arr = (arr - arr.min()) / (arr.max() - arr.min() + 1e-6)\n",
    "            img = arr\n",
    "        return np.clip(img, 0.0, 1.0)\n",
    "\n",
    "    # ---- prep first image ----\n",
    "    original_np = _to_display_img(original[0])\n",
    "    H, W = original_np.shape[:2]\n",
    "    n_h, n_w = H // patch_size, W // patch_size\n",
    "\n",
    "    # ---- per-masked-tile losses ----\n",
    "    pred0 = predicted_features[0].detach().cpu().numpy()   # [M, D]\n",
    "    tgt0  = target_features[0].detach().cpu().numpy()      # [M, D]\n",
    "    if pred0.size == 0:\n",
    "        per_patch_losses = np.zeros((0,), dtype=np.float32)\n",
    "    else:\n",
    "        diff = pred0 - tgt0\n",
    "        per_patch_losses = (diff * diff).mean(axis=-1)     # [M]\n",
    "\n",
    "    if per_patch_losses.size > 0:\n",
    "        lo, hi = float(per_patch_losses.min()), float(per_patch_losses.max())\n",
    "        denom = (hi - lo) if (hi > lo) else 1.0\n",
    "        normalized_quality = 1.0 - ((per_patch_losses - lo) / denom)\n",
    "    else:\n",
    "        normalized_quality = np.zeros((0,), dtype=np.float32)\n",
    "\n",
    "    # ---- masked indices ----\n",
    "    mask0 = patch_mask[0].detach().cpu().view(-1)          # [P]\n",
    "    masked_indices = mask0.nonzero(as_tuple=False).squeeze(1).numpy()  # [K]\n",
    "\n",
    "    # ---- figure ----\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(14, 5))\n",
    "\n",
    "    # Left: original\n",
    "    axs[0].imshow(original_np, interpolation='nearest')\n",
    "    axs[0].set_title(\"Original Image\")\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    # Center: mask overlay (red) with black grid\n",
    "    masked_img = original_np.copy()\n",
    "    overlay = masked_img.copy()\n",
    "    red = np.array([1.0, 0.0, 0.0], dtype=np.float32)\n",
    "\n",
    "    for pidx in masked_indices:\n",
    "        if pidx < 0 or pidx >= n_h * n_w:\n",
    "            continue\n",
    "        ih, iw = divmod(int(pidx), n_w)\n",
    "        h0, h1 = ih * patch_size, (ih + 1) * patch_size\n",
    "        w0, w1 = iw * patch_size, (iw + 1) * patch_size\n",
    "        overlay[h0:h1, w0:w1, :] = red\n",
    "\n",
    "    alpha_center = 0.35\n",
    "    masked_img = (1 - alpha_center) * masked_img + alpha_center * overlay\n",
    "    axs[1].imshow(np.clip(masked_img, 0.0, 1.0), interpolation='nearest')\n",
    "    axs[1].set_title(f\"Epoch {epoch} - JEPA Analysis\\nMasked Patches (Red)\\n{int(mask0.sum())}/{mask0.numel()} masked\")\n",
    "    axs[1].axis('off')\n",
    "\n",
    "    # Right: reconstruction quality (bold colored squares)\n",
    "    quality_img = original_np.copy()\n",
    "    colormap = plt.get_cmap('RdYlGn')  # green=good, red=bad\n",
    "    alpha_patch = 0.85                 # strong overlay for bold squares\n",
    "    grid_thick = max(1, patch_size // 16)  # thicker grid lines\n",
    "\n",
    "    limit = min(len(normalized_quality), len(masked_indices))\n",
    "    for idx in range(limit):\n",
    "        patch_idx = int(masked_indices[idx])\n",
    "        if patch_idx < 0 or patch_idx >= n_h * n_w:\n",
    "            continue\n",
    "\n",
    "        ih, iw = divmod(patch_idx, n_w)\n",
    "        h0, h1 = ih * patch_size, (ih + 1) * patch_size\n",
    "        w0, w1 = iw * patch_size, (iw + 1) * patch_size\n",
    "\n",
    "        q = float(np.asarray(normalized_quality[idx]).mean())\n",
    "        if not np.isfinite(q):\n",
    "            q = 0.0\n",
    "        q = float(np.clip(q, 0.0, 1.0))\n",
    "\n",
    "        color = np.asarray(colormap(q))[:3]  # (3,)\n",
    "        patch = quality_img[h0:h1, w0:w1, :]\n",
    "        quality_img[h0:h1, w0:w1, :] = (1 - alpha_patch) * patch + alpha_patch * color[None, None, :]\n",
    "\n",
    "        # thicker black grid lines\n",
    "        quality_img[h0:h0+grid_thick, w0:w1, :] = 0.0\n",
    "        quality_img[h0:h1, w0:w0+grid_thick, :] = 0.0\n",
    "\n",
    "    axs[2].imshow(np.clip(quality_img, 0.0, 1.0), interpolation='nearest')\n",
    "    axs[2].set_title(\"Reconstruction Quality\\n(Green=Good, Red=Poor)\")\n",
    "    axs[2].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc29848b-d9ec-470f-ab48-cf0ef0ebc91e",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1482914c-c5cc-4bdc-a542-9a16af4d0681",
   "metadata": {},
   "source": [
    "#### Patch Embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32492f0b-d551-454b-98a1-8eaeba92041b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PatchEmbed2D: remove undefined pos_embed add (or implement it properly) ---\n",
    "class PatchEmbed2D(nn.Module):\n",
    "    def __init__(self, in_chans: int, embed_dim: int, patch_size: int):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)                              # [B, D, n_h, n_w]\n",
    "        B, D, n_h, n_w = x.shape\n",
    "        x = x.view(B, D, n_h * n_w).transpose(1, 2)   # [B, P, D]\n",
    "        # (no pos_embed here)\n",
    "        return self.norm(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c48cc20-ccaa-406d-96b5-152b82a28c9c",
   "metadata": {},
   "source": [
    "#### Context Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a0b79e6-3f28-41d4-8c84-b24b0d0ee056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D Image → Patches → 2D Feature Map → ViT → 2D Feature Map → Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "092cafac-bc3b-4eb5-8023-14af08ef28a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "\n",
    "class ContextEncoder2D(nn.Module):\n",
    "    \"\"\"\n",
    "    Swin Transformer v2 Tiny context encoder configured for 384×384.\n",
    "    Returns patch tokens (no CLS) and the token grid size (Ht, Wt).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"swinv2_tiny_window8_256\",\n",
    "        pretrained: bool = True,\n",
    "        img_size: int = 384,\n",
    "        strict_img_size: bool = False,\n",
    "        dynamic_img_pad: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Build timm Swin v2; set img_size=384 and allow dynamic padding\n",
    "        self.swin = timm.create_model(\n",
    "            model_name,\n",
    "            pretrained=pretrained,\n",
    "            img_size=img_size,          # <- run at 384\n",
    "            num_classes=0,              # no classifier head\n",
    "            global_pool='',             # keep token grid\n",
    "            features_only=False,\n",
    "            strict_img_size=strict_img_size,\n",
    "            dynamic_img_pad=dynamic_img_pad,\n",
    "        )\n",
    "        self.embed_dim = self.swin.num_features\n",
    "        self.patch_size = 4  # Swin uses patch4 embed\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        x: [B, C, H, W]  (H,W multiples of 32 recommended; 384 works)\n",
    "        returns:\n",
    "          tokens: [B, P, D]  (CLS-free)\n",
    "          (Ht, Wt): token grid size at the final stage (~ H/32, W/32)\n",
    "        \"\"\"\n",
    "        feats = self.swin.forward_features(x)   # [B, L, D] or sometimes [B, Ht, Wt, D] / [B, D, Ht, Wt]\n",
    "\n",
    "        if feats.dim() == 3:                    # [B, L, D]\n",
    "            tokens = feats\n",
    "            P = tokens.shape[1]\n",
    "            Ht = int(math.sqrt(P))\n",
    "            Wt = P // Ht\n",
    "        else:\n",
    "            # Handle both [B, D, Ht, Wt] and [B, Ht, Wt, D]\n",
    "            if feats.shape[1] == self.embed_dim:        # [B, D, Ht, Wt]\n",
    "                B, D, Ht, Wt = feats.shape\n",
    "                tokens = feats.permute(0, 2, 3, 1).reshape(B, Ht * Wt, D)\n",
    "            else:                                       # [B, Ht, Wt, D]\n",
    "                B, Ht, Wt, D = feats.shape\n",
    "                tokens = feats.reshape(B, Ht * Wt, D)\n",
    "\n",
    "        return tokens, (Ht, Wt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daef4278-2a43-4310-a0e8-3b84c86fbd77",
   "metadata": {},
   "source": [
    "#### Pixel Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f58d95b-49dc-436b-b3d8-bacfa5e0bcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === CELL A: REPLACE YOUR WHOLE PixelDecoder2D CLASS WITH THIS ===\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from typing import List, Tuple, Optional\n",
    "\n",
    "# def _build_2d_sincos_pos_like(x: torch.Tensor) -> torch.Tensor:\n",
    "#     \"\"\"Fixed 2D sin/cos positional embedding with the same shape as x: [B, D, H, W].\"\"\"\n",
    "#     B, D, H, W = x.shape\n",
    "#     device, dtype = x.device, x.dtype\n",
    "#     y = torch.linspace(-1, 1, H, device=device, dtype=dtype)\n",
    "#     z = torch.linspace(-1, 1, W, device=device, dtype=dtype)\n",
    "#     yy, zz = torch.meshgrid(y, z, indexing='ij')               # [H,W]\n",
    "#     pos = torch.stack([yy, zz], dim=0).unsqueeze(0).expand(B, -1, -1, -1)  # [B,2,H,W]\n",
    "#     half = D // 2\n",
    "#     sin_bank = pos[:, :1].expand(B, half, H, W)                # y\n",
    "#     cos_bank = pos[:, 1:2].expand(B, D - half, H, W)           # x\n",
    "#     pe = torch.cat([torch.sin(sin_bank), torch.cos(cos_bank)], dim=1)      # [B,D,H,W]\n",
    "#     if pe.shape[1] < D:  # pad if D is odd\n",
    "#         pe = F.pad(pe, (0,0,0,0,0, D - pe.shape[1]))\n",
    "#     return pe\n",
    "\n",
    "# def _make_groupnorm(C: int) -> nn.GroupNorm:\n",
    "#     \"\"\"Pick a GroupNorm that divides C (fallback to 1). Safe for D=192,256 etc.\"\"\"\n",
    "#     for g in (32, 16, 8, 4, 2, 1):\n",
    "#         if C % g == 0:\n",
    "#             return nn.GroupNorm(g, C)\n",
    "#     return nn.GroupNorm(1, C)\n",
    "\n",
    "# class _DeformableAttn2D(nn.Module):\n",
    "#     \"\"\"\n",
    "#     MSDeformAttn-lite:\n",
    "#       - q:               [B, D, Hq, Wq]  (query map; produces offsets & weights)\n",
    "#       - feats_per_level: list of L tensors, each [B, D, Hi, Wi] (already projected + pos + level id)\n",
    "#       - returns:         [B, D, Hq, Wq]\n",
    "#     \"\"\"\n",
    "#     def __init__(self, embed_dim: int, num_points: int = 4, num_levels: int = 3, num_heads: int = 8):\n",
    "#         super().__init__()\n",
    "#         assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "#         self.D = embed_dim\n",
    "#         self.H = num_heads\n",
    "#         self.Dh = embed_dim // num_heads\n",
    "#         self.K = num_points\n",
    "#         self.L = num_levels\n",
    "\n",
    "#         # per-level value projections (like MSDeformAttn’s value proj)\n",
    "#         self.value_proj = nn.ModuleList([nn.Conv2d(self.D, self.D, 1) for _ in range(self.L)])\n",
    "\n",
    "#         # sampling offsets & attention weights predicted from q\n",
    "#         self.sampling_offsets = nn.Conv2d(self.D, self.H * self.L * self.K * 2, kernel_size=3, padding=1)\n",
    "#         self.attention_weights = nn.Conv2d(self.D, self.H * self.L * self.K,   kernel_size=3, padding=1)\n",
    "\n",
    "#         self.out_proj = nn.Conv2d(self.D, self.D, 1)\n",
    "#         self.norm = _make_groupnorm(self.D)\n",
    "\n",
    "#     @staticmethod\n",
    "#     def _make_ref_points(B, Hq, Wq, device, dtype):\n",
    "#         # reference points at the centers of each query location in [0,1] coords\n",
    "#         ys = (torch.arange(Hq, device=device, dtype=dtype) + 0.5) / Hq\n",
    "#         xs = (torch.arange(Wq, device=device, dtype=dtype) + 0.5) / Wq\n",
    "#         yy, xx = torch.meshgrid(ys, xs, indexing='ij')  # [Hq,Wq]\n",
    "#         ref = torch.stack([xx, yy], dim=-1)             # [Hq,Wq,2], (x,y) in [0,1]\n",
    "#         ref = ref.unsqueeze(0).expand(B, Hq, Wq, 2)\n",
    "#         return ref\n",
    "\n",
    "#     @staticmethod\n",
    "#     def _to_grid_sample(ref_xy_01):\n",
    "#         # convert [0,1] → [-1,1] for grid_sample\n",
    "#         return ref_xy_01 * 2.0 - 1.0\n",
    "\n",
    "#     def forward(self, q: torch.Tensor, feats_per_level: list[torch.Tensor]) -> torch.Tensor:\n",
    "#         B, D, Hq, Wq = q.shape\n",
    "#         device, dtype = q.device, q.dtype\n",
    "\n",
    "#         # predict offsets & weights\n",
    "#         offsets = self.sampling_offsets(q)   # [B, H*L*K*2, Hq, Wq]\n",
    "#         weights = self.attention_weights(q)  # [B, H*L*K,   Hq, Wq]\n",
    "\n",
    "#         # reshape\n",
    "#         offsets = offsets.view(B, self.H, self.L, self.K, 2, Hq, Wq)  # (B,H,L,K,2,Hq,Wq)\n",
    "#         # softmax over levels×points for each head/location\n",
    "#         weights = weights.view(B, self.H, self.L, self.K, Hq, Wq)     # (B,H,L,K,Hq,Wq)\n",
    "#         weights = weights.permute(0,1,4,5,2,3)                        # (B,H,Hq,Wq,L,K)\n",
    "#         weights = torch.softmax(weights.reshape(B, self.H, Hq, Wq, self.L * self.K), dim=-1)\n",
    "#         weights = weights.view(B, self.H, Hq, Wq, self.L, self.K).permute(0,1,4,5,2,3)  # (B,H,L,K,Hq,Wq)\n",
    "\n",
    "#         # reference points per query location (shared across heads)\n",
    "#         ref = self._make_ref_points(B, Hq, Wq, device, dtype)         # (B,Hq,Wq,2) in [0,1]\n",
    "\n",
    "#         # project values per level, split heads\n",
    "#         vals = []\n",
    "#         shapes = []\n",
    "#         for l, x in enumerate(feats_per_level):\n",
    "#             v = self.value_proj[l](x)                                 # (B,D,Hi,Wi)\n",
    "#             Bi, Di, Hi, Wi = v.shape\n",
    "#             v = v.view(Bi, self.H, self.Dh, Hi, Wi)                   # (B,H,Dh,Hi,Wi)\n",
    "#             vals.append(v)\n",
    "#             shapes.append((Hi, Wi))\n",
    "\n",
    "#         out = torch.zeros(B, self.H, self.Dh, Hq, Wq, device=device, dtype=dtype)\n",
    "\n",
    "#         # accumulate over levels and points\n",
    "#         for l in range(self.L):\n",
    "#             Hi, Wi = shapes[l]\n",
    "#             # precompute normalization for this level\n",
    "#             # offsets are predicted in (approx) pixel units; normalize by feature map size\n",
    "#             norm_x = Wi\n",
    "#             norm_y = Hi\n",
    "\n",
    "#             for k in range(self.K):\n",
    "#                 # offsets for all heads at this (l,k)\n",
    "#                 off_lk = offsets[:, :, l, k]                           # (B,H,2,Hq,Wq)\n",
    "#                 # to [0,1]-space: add normalized offsets to ref\n",
    "#                 dx = off_lk[:, :, 0] / max(1.0, float(norm_x))         # (B,H,Hq,Wq)\n",
    "#                 dy = off_lk[:, :, 1] / max(1.0, float(norm_y))         # (B,H,Hq,Wq)\n",
    "#                 ref_xy = torch.stack([ref[..., 0].unsqueeze(1) + dx,\n",
    "#                                       ref[..., 1].unsqueeze(1) + dy], dim=-1)  # (B,H,Hq,Wq,2) in [0,1]\n",
    "#                 # to grid_sample coordinates\n",
    "#                 grid = self._to_grid_sample(ref_xy).clamp(-1.0, 1.0)   # (B,H,Hq,Wq,2)\n",
    "\n",
    "#                 # sample vals[l] for each head; vectorize by merging (B,H)\n",
    "#                 v_l = vals[l]                                          # (B,H,Dh,Hi,Wi)\n",
    "#                 v_l = v_l.reshape(B * self.H, self.Dh, Hi, Wi)\n",
    "#                 grid_l = grid.reshape(B * self.H, Hq, Wq, 2)\n",
    "\n",
    "#                 sampled = F.grid_sample(\n",
    "#                     v_l, grid_l, mode='bilinear', align_corners=False\n",
    "#                 )  # (B*H, Dh, Hq, Wq)\n",
    "\n",
    "#                 sampled = sampled.view(B, self.H, self.Dh, Hq, Wq)     # (B,H,Dh,Hq,Wq)\n",
    "\n",
    "#                 # attention weights for this (l,k)\n",
    "#                 att_lk = weights[:, :, l, k].unsqueeze(2)              # (B,H,1,Hq,Wq)\n",
    "#                 out = out + sampled * att_lk\n",
    "\n",
    "#         out = out.view(B, self.D, Hq, Wq)                              # merge heads\n",
    "#         out = self.out_proj(self.norm(F.relu(out)))\n",
    "#         return out\n",
    "\n",
    "# class PixelDecoder2D(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Mask2Former-style pixel decoder (multi-level + deformable attention).\n",
    "#     (Name unchanged for drop-in.)\n",
    "\n",
    "#     __init__ keeps your signature:\n",
    "#       - in_channels: int (ignored if in_channels_per_level is provided)\n",
    "#       - embed_dim:   int\n",
    "\n",
    "#     Extra kwarg:\n",
    "#       - in_channels_per_level: Tuple[int,int,int] for [C3,C4,C5]. If omitted, use (in_channels,)*3.\n",
    "#     \"\"\"\n",
    "#     def __init__(self,\n",
    "#                  in_channels: int,\n",
    "#                  embed_dim: int,\n",
    "#                  *args, **kwargs):\n",
    "#         super().__init__()\n",
    "#         self.embed_dim = embed_dim\n",
    "\n",
    "#         in_chs: Optional[Tuple[int,int,int]] = kwargs.get(\"in_channels_per_level\", None)\n",
    "#         if in_chs is None:\n",
    "#             in_chs = (in_channels, in_channels, in_channels)\n",
    "\n",
    "#         # per-level projection to shared D\n",
    "#         self.in_proj = nn.ModuleList([nn.Conv2d(c, embed_dim, 1) for c in in_chs])\n",
    "\n",
    "#         # learned level embeddings\n",
    "#         self.level_embed = nn.Parameter(torch.randn(3, embed_dim))\n",
    "\n",
    "#         # deformable encoder (3 layers)\n",
    "#         self.enc_layers = nn.ModuleList([\n",
    "#             nn.ModuleDict({\n",
    "#                 \"attn\": _DeformableAttn2D(embed_dim, num_points=4, num_levels=3),\n",
    "#                 \"ffn\": nn.Sequential(\n",
    "#                     nn.Conv2d(embed_dim, 4*embed_dim, 1),\n",
    "#                     nn.GELU(),\n",
    "#                     nn.Conv2d(4*embed_dim, embed_dim, 1),\n",
    "#                 ),\n",
    "#                 \"norm1\": _make_groupnorm(embed_dim),\n",
    "#                 \"norm2\": _make_groupnorm(embed_dim),\n",
    "#             }) for _ in range(3)\n",
    "#         ])\n",
    "\n",
    "#         # heads for Fi1 (s=8) and F_last (s=4)\n",
    "#         self.fi1_head   = nn.Conv2d(embed_dim, embed_dim, 1)\n",
    "#         self.flast_head = nn.Conv2d(embed_dim, embed_dim, 1)\n",
    "\n",
    "#     def _prepare_levels(self, feats_multi: List[torch.Tensor]) -> List[torch.Tensor]:\n",
    "#         outs = []\n",
    "#         for lvl, (x, proj) in enumerate(zip(feats_multi, self.in_proj)):\n",
    "#             x = proj(x)                                  # [B,D,Hi,Wi]\n",
    "#             x = x + _build_2d_sincos_pos_like(x)         # pos enc\n",
    "#             x = x + self.level_embed[lvl].view(1,-1,1,1) # level id\n",
    "#             outs.append(x)\n",
    "#         return outs  # [C3@1/8, C4@1/16, C5@1/32], all [B,D,Hi,Wi]\n",
    "\n",
    "#     def forward(self, feats_multi: List[torch.Tensor], input_hw: Tuple[int,int]):\n",
    "#         \"\"\"\n",
    "#         feats_multi: [C3, C4, C5] at strides [1/8, 1/16, 1/32], each [B,Ci,Hi,Wi]\n",
    "#         input_hw: (H, W) full-res size\n",
    "#         returns:\n",
    "#           Fi1:   [B,D,H/8, W/8]\n",
    "#           F_last:[B,D,H/4, W/4]\n",
    "#         \"\"\"\n",
    "#         H, W = input_hw\n",
    "\n",
    "#         # 1) project + pos + level embeddings\n",
    "#         levels = self._prepare_levels(feats_multi)\n",
    "\n",
    "#         # 2) use 1/8 map as query grid for encoder\n",
    "#         q = levels[0]  # [B,D,H/8,W/8]\n",
    "\n",
    "#         # 3) deformable encoder layers\n",
    "#         for l in self.enc_layers:\n",
    "#             q = q + l[\"attn\"](l[\"norm1\"](q), levels)\n",
    "#             q = q + l[\"ffn\"](l[\"norm2\"](q))\n",
    "\n",
    "#         # 4) heads at required strides\n",
    "#         Fi1 = self.fi1_head(q)  # stride 8 already\n",
    "#         q4 = F.interpolate(q, size=(H // 4, W // 4), mode='bilinear', align_corners=False)\n",
    "#         F_last = self.flast_head(q4)\n",
    "\n",
    "#         return Fi1, F_last\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d7ccba7-09af-4c08-92c1-7cb6d70bd33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## test pixel decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0ed2a18-45ac-4a06-8a93-857858d792d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "def _simple_pos_embed_2d(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Simple 2D positional embedding - just scaled coordinates\"\"\"\n",
    "    B, D, H, W = x.shape\n",
    "    device, dtype = x.device, x.dtype\n",
    "    \n",
    "    # Create coordinate grids\n",
    "    y_coords = torch.linspace(0, 1, H, device=device, dtype=dtype)\n",
    "    x_coords = torch.linspace(0, 1, W, device=device, dtype=dtype) \n",
    "    yy, xx = torch.meshgrid(y_coords, x_coords, indexing='ij')\n",
    "    \n",
    "    # Simple embedding: just use x,y coordinates repeated\n",
    "    pos_embed = torch.stack([xx, yy], dim=0).unsqueeze(0)  # [1, 2, H, W]\n",
    "    pos_embed = pos_embed.expand(B, -1, -1, -1)            # [B, 2, H, W]\n",
    "    \n",
    "    # Repeat to match channel dimension\n",
    "    pos_embed = pos_embed.repeat(1, D//2, 1, 1)            # [B, D, H, W]\n",
    "    if pos_embed.shape[1] < D:\n",
    "        pos_embed = F.pad(pos_embed, (0, 0, 0, 0, 0, D - pos_embed.shape[1]))\n",
    "    \n",
    "    return pos_embed * 0.1  # Scale down to not overwhelm features\n",
    "\n",
    "class PixelDecoder2D(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple FPN-style pixel decoder. Much cleaner than deformable attention.\n",
    "    Keeps same interface as the complex version.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 embed_dim: int,\n",
    "                 *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # Handle per-level channels (C3, C4, C5 might be different)\n",
    "        in_chs: Optional[Tuple[int,int,int]] = kwargs.get(\"in_channels_per_level\", None)\n",
    "        if in_chs is None:\n",
    "            in_chs = (in_channels, in_channels, in_channels)\n",
    "\n",
    "        # Project each level to common embedding dimension\n",
    "        self.lateral_convs = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(c, embed_dim, kernel_size=1),\n",
    "                nn.GroupNorm(32 if embed_dim >= 32 else 1, embed_dim),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ) for c in in_chs\n",
    "        ])\n",
    "\n",
    "        # FPN fusion layers (reduce aliasing during upsampling)\n",
    "        self.fpn_convs = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1),\n",
    "                nn.GroupNorm(32 if embed_dim >= 32 else 1, embed_dim),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ) for _ in range(3)\n",
    "        ])\n",
    "\n",
    "        # Output heads\n",
    "        self.fi1_head = nn.Sequential(\n",
    "            nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(32 if embed_dim >= 32 else 1, embed_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(embed_dim, embed_dim, kernel_size=1)\n",
    "        )\n",
    "        \n",
    "        self.flast_head = nn.Sequential(\n",
    "            nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(32 if embed_dim >= 32 else 1, embed_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(embed_dim, embed_dim, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, feats_multi: List[torch.Tensor], input_hw: Tuple[int, int]):\n",
    "        \"\"\"\n",
    "        Simple FPN forward pass.\n",
    "        \n",
    "        feats_multi: [C3, C4, C5] at strides [1/8, 1/16, 1/32]\n",
    "        input_hw: (H, W) full resolution size\n",
    "        \n",
    "        Returns:\n",
    "            Fi1: [B, D, H/8, W/8] - feature at stride 8\n",
    "            F_last: [B, D, H/4, W/4] - feature at stride 4\n",
    "        \"\"\"\n",
    "        H, W = input_hw\n",
    "        c3, c4, c5 = feats_multi\n",
    "        \n",
    "        # 1. Lateral connections - project to common dimension\n",
    "        p5 = self.lateral_convs[2](c5)  # [B, D, H/32, W/32]\n",
    "        p4 = self.lateral_convs[1](c4)  # [B, D, H/16, W/16]  \n",
    "        p3 = self.lateral_convs[0](c3)  # [B, D, H/8, W/8]\n",
    "\n",
    "        # Add simple positional encoding\n",
    "        p5 = p5 + _simple_pos_embed_2d(p5)\n",
    "        p4 = p4 + _simple_pos_embed_2d(p4)\n",
    "        p3 = p3 + _simple_pos_embed_2d(p3)\n",
    "\n",
    "        # 2. Top-down pathway (FPN fusion)\n",
    "        # P5 -> P4\n",
    "        p5_up = F.interpolate(p5, size=p4.shape[-2:], mode='bilinear', align_corners=False)\n",
    "        p4 = p4 + p5_up\n",
    "        p4 = self.fpn_convs[1](p4)\n",
    "\n",
    "        # P4 -> P3  \n",
    "        p4_up = F.interpolate(p4, size=p3.shape[-2:], mode='bilinear', align_corners=False)\n",
    "        p3 = p3 + p4_up\n",
    "        p3 = self.fpn_convs[0](p3)  # This is our main feature at 1/8 stride\n",
    "\n",
    "        # 3. Generate outputs\n",
    "        # Fi1 at stride 8 (same as p3)\n",
    "        Fi1 = self.fi1_head(p3)  # [B, D, H/8, W/8]\n",
    "\n",
    "        # F_last at stride 4 (upsample p3)\n",
    "        p3_upsampled = F.interpolate(p3, size=(H // 4, W // 4), mode='bilinear', align_corners=False)\n",
    "        F_last = self.flast_head(p3_upsampled)  # [B, D, H/4, W/4]\n",
    "\n",
    "        return Fi1, F_last"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc70616-0541-46bf-b758-f3a21837c90f",
   "metadata": {},
   "source": [
    "#### PredictorHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "134dab00-bff5-4170-8920-5edbd39a2d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CrossAttentionBlock2D(nn.Module):\n",
    "    \"\"\"Cross-attention block for JEPA predictor\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim: int, num_heads: int = 8):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.cross_attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Feedforward\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_dim * 4, embed_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, queries, features):\n",
    "        # Cross-attention: queries attend to features\n",
    "        attn_out, _ = self.cross_attn(queries, features, features)\n",
    "        queries = self.norm1(queries + attn_out)\n",
    "        \n",
    "        # Feedforward\n",
    "        ffn_out = self.ffn(queries)\n",
    "        queries = self.norm2(queries + ffn_out)\n",
    "        \n",
    "        return queries\n",
    "\n",
    "class SelfAttentionBlock2D(nn.Module):\n",
    "    \"\"\"Self-attention block for query refinement\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim: int, num_heads: int = 8):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Feedforward  \n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_dim * 4, embed_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, queries):\n",
    "        # Self-attention: queries attend to themselves\n",
    "        attn_out, _ = self.self_attn(queries, queries, queries)\n",
    "        queries = self.norm1(queries + attn_out)\n",
    "        \n",
    "        # Feedforward\n",
    "        ffn_out = self.ffn(queries)\n",
    "        queries = self.norm2(queries + ffn_out)\n",
    "        \n",
    "        return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8588178-79a2-4f49-a13c-096521121c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Predictor2D(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_dim: int,\n",
    "                 num_queries: int,\n",
    "                 num_heads: int = None,\n",
    "                 # accept both old and new arg names:\n",
    "                 num_cross_blocks: int = None,\n",
    "                 num_self_blocks: int = None,\n",
    "                 num_cross_attn: int = None,\n",
    "                 num_self_attn: int = None):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_queries = num_queries\n",
    "\n",
    "        # choose a valid num_heads if not provided\n",
    "        if num_heads is None:\n",
    "            for h in (16, 12, 8, 6, 4, 3, 2, 1):\n",
    "                if embed_dim % h == 0:\n",
    "                    num_heads = h\n",
    "                    break\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # harmonize naming: prefer *attn if provided, else *blocks, else paper defaults (9/2)\n",
    "        if num_cross_attn is not None:\n",
    "            L = num_cross_attn\n",
    "        elif num_cross_blocks is not None:\n",
    "            L = num_cross_blocks\n",
    "        else:\n",
    "            L = 9\n",
    "        if num_self_attn is not None:\n",
    "            M = num_self_attn\n",
    "        elif num_self_blocks is not None:\n",
    "            M = num_self_blocks\n",
    "        else:\n",
    "            M = 2\n",
    "\n",
    "        # learnable queries\n",
    "        self.query_embed = nn.Parameter(torch.zeros(1, num_queries, embed_dim))\n",
    "        nn.init.trunc_normal_(self.query_embed, std=0.02)\n",
    "\n",
    "        # Cross-attention blocks (Mask2Former-ish: norm -> cross-attn -> add, norm -> FFN -> add)\n",
    "        self.cross_blocks = nn.ModuleList([\n",
    "            nn.ModuleDict(dict(\n",
    "                attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True),\n",
    "                ffn  = nn.Sequential(\n",
    "                    nn.Linear(embed_dim, 4*embed_dim),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Linear(4*embed_dim, embed_dim)\n",
    "                ),\n",
    "                norm1 = nn.LayerNorm(embed_dim),\n",
    "                norm2 = nn.LayerNorm(embed_dim),\n",
    "            )) for _ in range(L)\n",
    "        ])\n",
    "\n",
    "        # Extra self-attention blocks on queries\n",
    "        self.self_blocks = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, batch_first=True)\n",
    "            for _ in range(M)\n",
    "        ])\n",
    "\n",
    "        # projection head f_L to map query outputs back to Fi1 embedding space\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        # learnable mask token for masked Fi1 tiles (used to build K/V when Fi1 is masked)\n",
    "        self.kv_mask_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        nn.init.trunc_normal_(self.kv_mask_token, std=0.02)\n",
    "\n",
    "    @staticmethod\n",
    "    def _add_2d_sincos_pos(feat_2d: torch.Tensor):\n",
    "        \"\"\"\n",
    "        feat_2d: [B, D, H, W] -> returns [B, H*W, D] with fixed 2D sin/cos pos enc added\n",
    "        \"\"\"\n",
    "        import torch.nn.functional as F\n",
    "        B, D, H, W = feat_2d.shape\n",
    "        device = feat_2d.device\n",
    "    \n",
    "        # build sincos grid\n",
    "        y = torch.linspace(-1, 1, steps=H, device=device)\n",
    "        x = torch.linspace(-1, 1, steps=W, device=device)\n",
    "        yy, xx = torch.meshgrid(y, x, indexing='ij')\n",
    "        pos = torch.stack([xx, yy], dim=-1).reshape(1, H, W, 2)  # [1,H,W,2]\n",
    "    \n",
    "        # project to channel dim D using sin/cos\n",
    "        half = D // 2\n",
    "        sin_in = pos[..., 0:1].repeat(1, 1, 1, half)\n",
    "        cos_in = pos[..., 1:2].repeat(1, 1, 1, D - half)\n",
    "        pos_embed = torch.cat([torch.sin(sin_in), torch.cos(cos_in)], dim=-1)  # [1,H,W,D]\n",
    "        if pos_embed.shape[-1] != D:\n",
    "            pos_embed = F.pad(pos_embed, (0, D - pos_embed.shape[-1]))[:, :, :, :D]\n",
    "    \n",
    "        # >>> key fix: match [B, D, H, W] before addition\n",
    "        pos_embed = pos_embed.permute(0, 3, 1, 2)  # [1, D, H, W]\n",
    "    \n",
    "        feat = feat_2d + pos_embed.to(feat_2d.dtype)  # [B, D, H, W]\n",
    "        feat = feat.flatten(2).transpose(1, 2)        # [B, H*W, D]\n",
    "        return feat\n",
    "\n",
    "    def forward(self, Fi1_online: torch.Tensor, Fi1_mask: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Fi1_online: [B, D, H8, W8]   (online pixel-decoder Fi1)\n",
    "        Fi1_mask:   [B, H8*W8] bool  (True=masked positions in Fi1 to reconstruct)\n",
    "        Returns:\n",
    "           pred_masked_feats: [B, M, D] predictions for masked Fi1 positions (M = #masked)\n",
    "           masked_indices:    [B, M] indices of the masked Fi1 positions (or -1 padded)\n",
    "           query_feats:       [B, Q, D] final query embeddings\n",
    "        \"\"\"\n",
    "        B, D, H8, W8 = Fi1_online.shape\n",
    "        Q = self.num_queries\n",
    "\n",
    "        # Build K/V from Fi1 with 2D sincos pos; replace masked tiles with kv_mask_token (NOT zeros)\n",
    "        kv = Fi1_online.clone()  # [B,D,H8,W8]\n",
    "        if Fi1_mask is not None:\n",
    "            mask_2d = Fi1_mask.reshape(B, H8, W8).unsqueeze(1).expand(-1, D, -1, -1)  # [B,D,H8,W8]\n",
    "            kv = torch.where(mask_2d, self.kv_mask_token.view(1, D, 1, 1).expand(B, D, H8, W8), kv)\n",
    "\n",
    "        kv_seq = self._add_2d_sincos_pos(kv)  # [B, H8*W8, D] as K/V\n",
    "\n",
    "        # Queries\n",
    "        q = self.query_embed.expand(B, Q, D)  # [B,Q,D]\n",
    "\n",
    "        # L cross-attention blocks\n",
    "        for blk in self.cross_blocks:\n",
    "            qn = blk['norm1'](q)\n",
    "            attn_out, _ = blk['attn'](qn, kv_seq, kv_seq)  # cross-attn to Fi1+pos\n",
    "            q = q + attn_out\n",
    "            q = q + blk['ffn'](blk['norm2'](q))\n",
    "\n",
    "        # M self-attn blocks on queries\n",
    "        for sblk in self.self_blocks:\n",
    "            q = sblk(q)\n",
    "\n",
    "        # map to Fi1 embedding space\n",
    "        q_proj = self.proj(q)  # [B,Q,D]\n",
    "\n",
    "        # Route query embeddings to masked tiles (simple attention routing)\n",
    "        scores = torch.einsum('bpd,bqd->bpq', kv_seq, q_proj) / (D ** 0.5)  # [B,P,Q]\n",
    "        probs = scores.softmax(dim=-1)  # over queries\n",
    "\n",
    "        if Fi1_mask is not None and Fi1_mask.any():\n",
    "            pred_list, idx_list = [], []\n",
    "            for b in range(B):\n",
    "                mask_b = Fi1_mask[b]  # [P]\n",
    "                if mask_b.any():\n",
    "                    prob_b = probs[b, mask_b]     # [Mb, Q]\n",
    "                    q_b    = q_proj[b]            # [Q, D]\n",
    "                    pred_b = prob_b @ q_b         # [Mb, D]\n",
    "                    pred_list.append(pred_b)\n",
    "                    idx_list.append(mask_b.nonzero(as_tuple=False).squeeze(1))\n",
    "                else:\n",
    "                    pred_list.append(q_proj.new_zeros((0, D)))\n",
    "                    idx_list.append(torch.zeros((0,), dtype=torch.long, device=q_proj.device))\n",
    "            maxM = max([p.size(0) for p in pred_list])\n",
    "            if maxM == 0:\n",
    "                pred_masked_feats = q_proj.new_zeros((B, 0, D))\n",
    "                masked_indices    = q_proj.new_zeros((B, 0), dtype=torch.long)\n",
    "            else:\n",
    "                pred_masked_feats, masked_indices = [], []\n",
    "                for b in range(B):\n",
    "                    m = pred_list[b].size(0)\n",
    "                    pad = maxM - m\n",
    "                    if pad > 0:\n",
    "                        pred_masked_feats.append(F.pad(pred_list[b], (0,0,0,pad)))\n",
    "                        masked_indices.append(F.pad(idx_list[b], (0,pad), value=-1))\n",
    "                    else:\n",
    "                        pred_masked_feats.append(pred_list[b])\n",
    "                        masked_indices.append(idx_list[b])\n",
    "                pred_masked_feats = torch.stack(pred_masked_feats, dim=0)  # [B, M, D]\n",
    "                masked_indices    = torch.stack(masked_indices, dim=0)     # [B, M]\n",
    "        else:\n",
    "            pred_masked_feats = q_proj.new_zeros((B, 0, D))\n",
    "            masked_indices    = q_proj.new_zeros((B, 0), dtype=torch.long)\n",
    "\n",
    "        return pred_masked_feats, masked_indices, q_proj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e858251-e914-43bb-a3ee-fed2dd2938a6",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b57873f-a33a-493c-8f12-7007d2f61b41",
   "metadata": {},
   "source": [
    "#### Dataset & Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9f9b608-bb95-4d53-9c4c-026dbcf591c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1c39cab469f42d39daf245fe6d14b69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes:\n",
      "JEPA (ImageNet): 126,689 samples\n",
      "ADE20K train: 20,210 samples\n",
      "ADE20K val: 2,000 samples\n",
      "\n",
      "DataLoader info:\n",
      "Pretrain: 299 batches\n",
      "Train: 422 batches\n",
      "Val: 42 batches\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Create Datasets and DataLoaders\n",
    "# =============================================================================\n",
    "\n",
    "# Training configuration\n",
    "batch_size_pretrain = 424\n",
    "batch_size_downstream = 48\n",
    "\n",
    "# Create dataset instances\n",
    "jepa_dataset = JEPADataset()\n",
    "ade_train_dataset = ADE20KDataset(split=\"training\")\n",
    "ade_val_dataset = ADE20KDataset(split=\"validation\")\n",
    "\n",
    "print(f\"Dataset sizes:\")\n",
    "print(f\"JEPA (ImageNet): {len(jepa_dataset):,} samples\")\n",
    "print(f\"ADE20K train: {len(ade_train_dataset):,} samples\")\n",
    "print(f\"ADE20K val: {len(ade_val_dataset):,} samples\")\n",
    "\n",
    "# JEPA pretraining loader\n",
    "pretrain_loader = DataLoader(\n",
    "    jepa_dataset,\n",
    "    batch_size=batch_size_pretrain,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    collate_fn=jepa_collate\n",
    ")\n",
    "\n",
    "# Downstream fine-tuning loaders\n",
    "downstream_train_loader = DataLoader(\n",
    "    ade_train_dataset,\n",
    "    batch_size=batch_size_downstream,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    collate_fn=ade_collate\n",
    ")\n",
    "\n",
    "downstream_val_loader = DataLoader(\n",
    "    ade_val_dataset,\n",
    "    batch_size=batch_size_downstream,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    collate_fn=ade_collate\n",
    ")\n",
    "\n",
    "print(f\"\\nDataLoader info:\")\n",
    "print(f\"Pretrain: {len(pretrain_loader)} batches\")\n",
    "print(f\"Train: {len(downstream_train_loader)} batches\")\n",
    "print(f\"Val: {len(downstream_val_loader)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8cbec5c-70ba-4fc1-80c2-3913a1866666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([424, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "# Test batch shapes for 2D JEPA\n",
    "batch = next(iter(pretrain_loader))\n",
    "imgs = batch[\"images\"] # (B, C, H, W)\n",
    "\n",
    "print(imgs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dc9778-1db1-4f21-9545-0a9f3df83328",
   "metadata": {},
   "source": [
    "#### Model Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2745fb11-60b9-4acb-8952-522b1f38e261",
   "metadata": {},
   "source": [
    "#### Mask-JEPA setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "800eda25-be4d-4b9d-bace-308655ddba82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "from typing import Tuple, Optional\n",
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "class MaskJEPA2D(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_chans: int,\n",
    "                 num_queries: int = 32,\n",
    "                 num_cross_attn: int = 2,\n",
    "                 num_self_attn: int = 1,\n",
    "                 tau: float = 0.996,\n",
    "                 fi1_mask_ratio: float = 0.5,\n",
    "                 patch_size: int = 8,\n",
    "                 model_name: str = \"swin_tiny_patch4_window7_224\",\n",
    "                 pretrained: bool = True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        \n",
    "        # === Context encoder (timm ViT with pos_embed) ===\n",
    "        self.context_encoder = ContextEncoder2D(model_name=model_name, pretrained=pretrained)\n",
    "\n",
    "        # pull embed_dim & patch_size from the encoder backbone\n",
    "        self.embed_dim = self.context_encoder.embed_dim\n",
    "        self.in_chans = in_chans  # needed for denoising head output\n",
    "\n",
    "        # === Target encoder (EMA, frozen) ===\n",
    "        self.target_encoder = copy.deepcopy(self.context_encoder)\n",
    "        for p in self.target_encoder.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # === Pixel decoders ===\n",
    "        self.pixel_decoder = PixelDecoder2D(\n",
    "            in_channels=self.embed_dim,\n",
    "            embed_dim=self.embed_dim\n",
    "        )\n",
    "        self.pixel_decoder_ema = copy.deepcopy(self.pixel_decoder)\n",
    "        for p in self.pixel_decoder_ema.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # === Downsampling convs for C4 / C5 ===\n",
    "        self.ds16 = nn.Sequential(\n",
    "            nn.Conv2d(self.embed_dim, self.embed_dim, 3, stride=2, padding=1), nn.GELU(),\n",
    "            nn.Conv2d(self.embed_dim, self.embed_dim, 3, padding=1),           nn.GELU(),\n",
    "        )\n",
    "        self.ds32 = nn.Sequential(\n",
    "            nn.Conv2d(self.embed_dim, self.embed_dim, 3, stride=2, padding=1), nn.GELU(),\n",
    "            nn.Conv2d(self.embed_dim, self.embed_dim, 3, padding=1),           nn.GELU(),\n",
    "        )\n",
    "\n",
    "        # === JEPA Predictor ===\n",
    "        self.predictor = Predictor2D(\n",
    "            embed_dim=self.embed_dim,\n",
    "            num_queries=num_queries,\n",
    "            num_cross_attn=num_cross_attn,\n",
    "            num_self_attn=num_self_attn\n",
    "        )\n",
    "\n",
    "        # === Denoising head ===\n",
    "        self.denoising_head = nn.Conv2d(self.embed_dim, in_chans, kernel_size=1)\n",
    "\n",
    "        # === EMA tau config ===\n",
    "        self.tau = tau\n",
    "        self.tau_base  = tau\n",
    "        self.tau_final = 1.0\n",
    "\n",
    "        self.fi1_mask_ratio = fi1_mask_ratio\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Paper-correct noise path:\n",
    "          - Add Gaussian noise at s_last=4 and expand\n",
    "          - Online branch sees noisy input\n",
    "          - Target branch sees clean input\n",
    "        \"\"\"\n",
    "        B, C, H, W = x.shape\n",
    "        device = x.device\n",
    "    \n",
    "        # ---------- (A) add noise ----------\n",
    "        s_last = 4\n",
    "        H4, W4 = H // s_last, W // s_last\n",
    "        sigma = 0.4\n",
    "    \n",
    "        eps_lr = torch.randn(B, C, H4, W4, device=device, dtype=x.dtype) * sigma\n",
    "        eps_full = eps_lr.repeat_interleave(s_last, dim=2).repeat_interleave(s_last, dim=3)\n",
    "        x_noisy = x + eps_full\n",
    "    \n",
    "        # ---------- (B) ONLINE BRANCH ----------\n",
    "        tokens_online, (enc_h, enc_w) = self.context_encoder(x_noisy)  # [B,P,D], (Ht,Wt)\n",
    "        feat_online = tokens_online.transpose(1, 2).reshape(\n",
    "            B, self.embed_dim, enc_h, enc_w\n",
    "        )\n",
    "        \n",
    "        C3 = F.interpolate(feat_online, size=(H//8, W//8), mode='bilinear', align_corners=False)\n",
    "        x16 = self.ds16(C3)\n",
    "        x32 = self.ds32(x16)\n",
    "        C4  = F.interpolate(x16, size=(H//16, W//16), mode='bilinear', align_corners=False)\n",
    "        C5  = F.interpolate(x32, size=(H//32, W//32), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        f_i1_online, f_last_online = self.pixel_decoder([C3, C4, C5], (H, W))\n",
    "\n",
    "        # ---------- (C) TARGET BRANCH ----------\n",
    "        with torch.no_grad():\n",
    "            tokens_target, _ = self.target_encoder(x)\n",
    "            feat_target = tokens_target.transpose(1, 2).reshape(\n",
    "                B, self.embed_dim, enc_h, enc_w\n",
    "            )\n",
    "        \n",
    "            C3t = F.interpolate(feat_target, size=(H//8, W//8), mode='bilinear', align_corners=False)\n",
    "            x16t = self.ds16(C3t)\n",
    "            x32t = self.ds32(x16t)\n",
    "            C4t  = F.interpolate(x16t, size=(H//16, W//16), mode='bilinear', align_corners=False)\n",
    "            C5t  = F.interpolate(x32t, size=(H//32, W//32), mode='bilinear', align_corners=False)\n",
    "        \n",
    "            f_i1_target, _ = self.pixel_decoder_ema([C3t, C4t, C5t], (H, W))\n",
    "\n",
    "        # ---------- (D) Fi1 MASK ----------\n",
    "        fi1_mask = generate_fi1_mask(\n",
    "            fi1_shape=f_i1_online.shape,\n",
    "            mask_ratio=self.fi1_mask_ratio,\n",
    "            patch_size = self.patch_size,\n",
    "            device=device\n",
    "        )  # [B, H8*W8] bool\n",
    "    \n",
    "        # ---------- (E) PREDICTOR ----------\n",
    "        predicted_features, masked_indices, q_proj = self.predictor(f_i1_online, fi1_mask)\n",
    "    \n",
    "        # ---------- (F) TARGET GATHER + LN ----------\n",
    "        D = f_i1_target.shape[1]\n",
    "        fi1_h, fi1_w = f_i1_target.shape[-2:]\n",
    "        target_seq = f_i1_target.permute(0, 2, 3, 1).reshape(B, fi1_h * fi1_w, D)\n",
    "        target_seq = F.layer_norm(target_seq, (D,))\n",
    "    \n",
    "        if masked_indices.numel() > 0:\n",
    "            pad_mask = (masked_indices >= 0)\n",
    "            safe_idx = masked_indices.clamp_min(0)\n",
    "            b_idx = torch.arange(B, device=device).unsqueeze(-1).expand_as(safe_idx)\n",
    "            target_masked_full = target_seq[b_idx, safe_idx]\n",
    "            target_masked = target_masked_full * pad_mask.unsqueeze(-1).to(target_masked_full.dtype)\n",
    "        else:\n",
    "            target_masked = target_seq.new_zeros((B, 0, D))\n",
    "    \n",
    "        # ---------- (G) DENOISING ----------\n",
    "        denoised_prediction = self.denoising_head(f_last_online)\n",
    "    \n",
    "        return {\n",
    "            'predicted_features': predicted_features,\n",
    "            'target_masked':      target_masked,\n",
    "            'mask_info':          (q_proj, masked_indices),\n",
    "            'denoised_prediction': denoised_prediction,\n",
    "            'original_input':     x,\n",
    "            'fi1_mask':           fi1_mask,\n",
    "            'mask_indices':       masked_indices,\n",
    "            'eps_target':         eps_lr\n",
    "        }\n",
    "\n",
    "    def set_ema_tau(self, tau: float):\n",
    "        self.tau = float(tau)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_ema(self):\n",
    "        update_ema(self.target_encoder, self.context_encoder, tau=self.tau)\n",
    "        update_ema(self.pixel_decoder_ema, self.pixel_decoder, tau=self.tau)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41bb53d2-b969-4f34-bcae-7394969fbc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import copy\n",
    "# import math\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from torch.optim import AdamW\n",
    "# from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "# # Learning rate schedule (linear warmup + cosine decay)\n",
    "# def lr_lambda(epoch: int) -> float:\n",
    "#     if epoch < warmup_epochs:\n",
    "#         # Linear warmup: scale from 0 → 1 over warmup_epochs\n",
    "#         return float(epoch) / float(max(1, warmup_epochs))\n",
    "#     else:\n",
    "#         # Cosine decay: 1 → 0 after warmup\n",
    "#         decay_epoch = epoch - warmup_epochs\n",
    "#         decay_total = num_epochs - warmup_epochs\n",
    "#         cosine = 0.5 * (1 + math.cos(math.pi * decay_epoch / decay_total))\n",
    "#         return cosine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82445529-dfbd-4c51-a79a-cdf937ce49e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_lambda(epoch: int) -> float:\n",
    "    if epoch < warmup_epochs:\n",
    "        # Linear warmup: scale from 0 → 1 over warmup_epochs\n",
    "        return float(epoch) / float(max(1, warmup_epochs))\n",
    "    elif epoch < warmup_epochs + 3:\n",
    "        # Aggressive phase: stay at full rate for 3 epochs after warmup\n",
    "        return 1.0\n",
    "    else:\n",
    "        # Exponential decay phase\n",
    "        decay_start = warmup_epochs + 3\n",
    "        decay_factor = 0.7 ** (epoch - decay_start)\n",
    "        return max(decay_factor, 0.3)  # Don't decay below 30% of base_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "893ffc30-3b03-4286-8a8c-92bd38599e77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model...\n",
      "Gradient checkpointing enabled\n",
      "Model parameters: 119,820,413\n",
      "Batch size: 424\n",
      "Model config: patch_size=8, mask_ratio=0.5, queries=8, cross_attn=5, self_attn=1\n",
      "Epoch 1/20\n",
      "[probe] target: mean=-0.041 std=1.135\n",
      "[probe] pred  : mean=-0.009 std=0.267\n",
      "  Batch 0/299 - TotalSc: 2.6922, ReconSc: 1.3513, DenoiseSc: 1.3409\n",
      "  Batch 50/299 - TotalSc: 1.2750, ReconSc: 0.2401, DenoiseSc: 1.0349\n",
      "  Batch 100/299 - TotalSc: 0.9345, ReconSc: 0.1465, DenoiseSc: 0.7880\n",
      "  Batch 150/299 - TotalSc: 0.7808, ReconSc: 0.1086, DenoiseSc: 0.6722\n",
      "  Batch 200/299 - TotalSc: 0.6919, ReconSc: 0.0913, DenoiseSc: 0.6006\n",
      "  Batch 250/299 - TotalSc: 0.6364, ReconSc: 0.0858, DenoiseSc: 0.5506\n",
      "  Avg losses - TotalSc: 0.6069, ReconSc: 0.0897, DenoiseSc: 0.5172\n",
      "  LR: 1.00e-04\n",
      "  GPU Memory: 2.06GB\n",
      "  Time for epoch 1: 25.47 minutes\n",
      "  Running evaluation...\n",
      "    Saved visualization: ./jepa_training_output/reconstruction_epoch_001.png\n",
      "    [best] New best TotalSc=0.6069. Saved: ./jepa_training_output/best_jepa_model.pt\n",
      "Epoch 2/20\n",
      "[probe] target: mean=0.022 std=1.183\n",
      "[probe] pred  : mean=0.046 std=1.026\n",
      "  Batch 0/299 - TotalSc: 0.4961, ReconSc: 0.1473, DenoiseSc: 0.3489\n",
      "  Batch 50/299 - TotalSc: 0.5193, ReconSc: 0.1727, DenoiseSc: 0.3466\n",
      "  Batch 100/299 - TotalSc: 0.5646, ReconSc: 0.1966, DenoiseSc: 0.3680\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 152\u001b[0m\n\u001b[1;32m    149\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m recon_loss \u001b[38;5;241m+\u001b[39m denoise_loss\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_loss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# Gradient clipping\u001b[39;00m\n\u001b[1;32m    155\u001b[0m scaler\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.9/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.9/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.9/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.amp import GradScaler, autocast\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gc\n",
    "import math\n",
    "import time   # for epoch timing\n",
    "use_bf16 = torch.cuda.is_bf16_supported()  # True on A100/RTX 40xx/etc\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Training configuration\n",
    "num_epochs = 20\n",
    "warmup_epochs = 0\n",
    "base_lr = 1e-4\n",
    "weight_decay = 0.05\n",
    "\n",
    "# Early stopping config\n",
    "early_stop_patience = 5\n",
    "best_total_sc = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "# Enable memory optimizations\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "\n",
    "def enable_gradient_checkpointing(model):\n",
    "    \"\"\"Enable gradient checkpointing for timm ViT blocks\"\"\"\n",
    "    if hasattr(model.context_encoder, 'vit') and hasattr(model.context_encoder.vit, 'blocks'):\n",
    "        for block in model.context_encoder.vit.blocks:\n",
    "            if hasattr(block, 'set_grad_checkpointing'):\n",
    "                block.set_grad_checkpointing(True)\n",
    "    if hasattr(model.target_encoder, 'vit') and hasattr(model.target_encoder.vit, 'blocks'):\n",
    "        for block in model.target_encoder.vit.blocks:\n",
    "            if hasattr(block, 'set_grad_checkpointing'):\n",
    "                block.set_grad_checkpointing(True)\n",
    "\n",
    "# Create model\n",
    "print(\"Creating model...\")\n",
    "model = MaskJEPA2D(\n",
    "    in_chans=3,\n",
    "    tau=0.996,\n",
    "    fi1_mask_ratio=0.5,\n",
    "    num_queries=8,\n",
    "    num_cross_attn=5,\n",
    "    num_self_attn=1,\n",
    "    patch_size=8\n",
    ").to(device)\n",
    "\n",
    "D = model.embed_dim\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "enable_gradient_checkpointing(model)\n",
    "print(\"Gradient checkpointing enabled\")\n",
    "\n",
    "# Mixed precision scaler\n",
    "scaler = GradScaler('cuda')\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=base_lr, weight_decay=weight_decay)\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "# Create save directory\n",
    "save_dir = \"./jepa_training_output\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "best_ckpt_path = os.path.join(save_dir, \"best_jepa_model.pt\")\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Memory cleanup\"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# Print model info\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "print(f\"Batch size: {batch_size_pretrain}\")\n",
    "\n",
    "ps = model.patch_size\n",
    "mr = model.fi1_mask_ratio\n",
    "nq = model.predictor.num_queries\n",
    "nca = len(model.predictor.cross_blocks)\n",
    "nsa = len(model.predictor.self_blocks)\n",
    "\n",
    "print(f\"Model config: patch_size={ps}, mask_ratio={mr}, queries={nq}, cross_attn={nca}, self_attn={nsa}\")\n",
    "\n",
    "# EMA ramp setup\n",
    "planned_updates_per_epoch = len(pretrain_loader)\n",
    "max_updates = num_epochs * planned_updates_per_epoch\n",
    "global_update = 0\n",
    "\n",
    "# Training loop\n",
    "train_losses = []\n",
    "best_snapshot = None  # will hold best weights for final save\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()   # start timer\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    epoch_recon_loss = 0.0\n",
    "    epoch_denoise_loss = 0.0\n",
    "    \n",
    "    clear_memory()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for batch_idx, batch in enumerate(pretrain_loader):\n",
    "        images = batch[\"images\"].to(device, non_blocking=True)\n",
    "        \n",
    "        # Forward pass with mixed precision (bf16 if available, else fp16)\n",
    "        with autocast(device_type='cuda', enabled=False):\n",
    "            outputs = model(images)\n",
    "            \n",
    "            pred = outputs['predicted_features']\n",
    "            tgt = outputs['target_masked']\n",
    "            idx = outputs['mask_indices']\n",
    "            valid = (idx >= 0).unsqueeze(-1)\n",
    "            \n",
    "            # Reconstruction loss\n",
    "            if pred.numel() == 0 or valid.sum() == 0:\n",
    "                recon_loss = pred.new_zeros(())\n",
    "            else:\n",
    "                diff = (pred - tgt) * valid\n",
    "                recon_loss = diff.pow(2).sum() / valid.sum().clamp_min(1)\n",
    "\n",
    "            # Option 2: Predict clean image x (current default)\n",
    "            x4 = F.interpolate(\n",
    "                outputs['original_input'],\n",
    "                size=outputs['denoised_prediction'].shape[-2:],\n",
    "                mode='bilinear', \n",
    "                align_corners=False\n",
    "            )\n",
    "            denoise_loss = F.mse_loss(outputs['denoised_prediction'], x4)\n",
    "            \n",
    "            # Debug info (first batch only)\n",
    "            if batch_idx == 0:\n",
    "                td = F.interpolate(images, size=outputs['denoised_prediction'].shape[-2:],\n",
    "                                   mode='bilinear', align_corners=False)\n",
    "                pd = outputs['denoised_prediction'].detach()\n",
    "                print(f\"[probe] target: mean={td.mean().item():.3f} std={td.std().item():.3f}\")\n",
    "                print(f\"[probe] pred  : mean={pd.mean().item():.3f} std={pd.std().item():.3f}\")\n",
    "\n",
    "            total_loss = recon_loss + denoise_loss\n",
    "        \n",
    "        # Backward pass\n",
    "        scaler.scale(total_loss).backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        # Optimizer step\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # EMA update with ramp\n",
    "        progress = global_update / max(1, max_updates - 1)\n",
    "        tau_now = model.tau_base + (model.tau_final - model.tau_base) * progress\n",
    "        model.set_ema_tau(tau_now)\n",
    "        model.update_ema()\n",
    "        global_update += 1\n",
    "        \n",
    "        # Track losses\n",
    "        epoch_loss += total_loss.item()\n",
    "        epoch_recon_loss += recon_loss.item()\n",
    "        epoch_denoise_loss += denoise_loss.item()\n",
    "        \n",
    "        # Cleanup\n",
    "        del outputs, recon_loss, denoise_loss, total_loss, images\n",
    "        \n",
    "        # Progress logging\n",
    "        if batch_idx % 50 == 0:\n",
    "            recon_sc = (epoch_recon_loss / (batch_idx + 1)) / D\n",
    "            denoise_sc = (epoch_denoise_loss / (batch_idx + 1))\n",
    "            total_sc = recon_sc + denoise_sc\n",
    "            \n",
    "            print(f\"  Batch {batch_idx}/{len(pretrain_loader)} - \"\n",
    "                  f\"TotalSc: {total_sc:.4f}, ReconSc: {recon_sc:.4f}, DenoiseSc: {denoise_sc:.4f}\")\n",
    "    \n",
    "    # Average losses\n",
    "    epoch_loss /= len(pretrain_loader)\n",
    "    epoch_recon_loss /= len(pretrain_loader)\n",
    "    epoch_denoise_loss /= len(pretrain_loader)\n",
    "    \n",
    "    train_losses.append(epoch_loss)\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Epoch summary\n",
    "    recon_sc_epoch = epoch_recon_loss / D\n",
    "    denoise_sc_epoch = epoch_denoise_loss\n",
    "    total_sc_epoch = recon_sc_epoch + denoise_sc_epoch\n",
    "    \n",
    "    epoch_end_time = time.time()   # end timer\n",
    "    epoch_duration = epoch_end_time - epoch_start_time\n",
    "    print(f\"  Avg losses - TotalSc: {total_sc_epoch:.4f}, \"\n",
    "          f\"ReconSc: {recon_sc_epoch:.4f}, DenoiseSc: {denoise_sc_epoch:.4f}\")\n",
    "    print(f\"  LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "    print(f\"  GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f}GB\")\n",
    "    print(f\"  Time for epoch {epoch+1}: {epoch_duration/60:.2f} minutes\")\n",
    "    \n",
    "    # Periodic evaluation (every 1 epoch here)\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        print(\"  Running evaluation...\")\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            eval_batch = next(iter(pretrain_loader))\n",
    "            eval_images = eval_batch[\"images\"][:4].to(device)\n",
    "            \n",
    "            with autocast(device_type='cuda', enabled=False):\n",
    "                eval_outputs = model(eval_images)\n",
    "            \n",
    "            # Visualization\n",
    "            H, W = eval_images.shape[-2:]\n",
    "            fi1_tile = max(H // (H // 8), 1)\n",
    "            \n",
    "            vis_path = os.path.join(save_dir, f\"reconstruction_epoch_{epoch+1:03d}.png\")\n",
    "            \n",
    "            visualize_jepa_patch_quality(\n",
    "                original=eval_images,\n",
    "                predicted_features=eval_outputs['predicted_features'].float(),\n",
    "                target_features=eval_outputs['target_masked'].float(),\n",
    "                patch_mask=eval_outputs['fi1_mask'],\n",
    "                epoch=epoch+1,\n",
    "                save_path=vis_path,\n",
    "                patch_size=fi1_tile\n",
    "            )\n",
    "            \n",
    "            del eval_batch, eval_images, eval_outputs\n",
    "        \n",
    "        print(f\"    Saved visualization: {vis_path}\")\n",
    "        model.train()\n",
    "        clear_memory()\n",
    "    \n",
    "    # ---- Save ONLY when we have a new best TotalSc ----\n",
    "    if total_sc_epoch < best_total_sc:\n",
    "        best_total_sc = total_sc_epoch\n",
    "        epochs_no_improve = 0\n",
    "        best_snapshot = {\n",
    "            'backbone_state_dict': model.context_encoder.state_dict(),\n",
    "            'pixel_decoder_state_dict': model.pixel_decoder.state_dict(),\n",
    "            'transformer_decoder_cross_blocks_state_dict': model.predictor.cross_blocks.state_dict(),\n",
    "        }\n",
    "        torch.save(best_snapshot, best_ckpt_path)\n",
    "        print(f\"    [best] New best TotalSc={best_total_sc:.4f}. Saved: {best_ckpt_path}\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"  [early-stop] No improvement ({epochs_no_improve}/{early_stop_patience}).\")\n",
    "        if epochs_no_improve >= early_stop_patience:\n",
    "            print(f\"  [early-stop] Patience exceeded. Stopping training early.\")\n",
    "            break\n",
    "    # --------------------------------------\n",
    "\n",
    "print(\"Training completed!\")\n",
    "\n",
    "# Save final pretrained weights for downstream use (best-only)\n",
    "final_weights_path = os.path.join(save_dir, 'mask_jepa_pretrained_weights.pt')\n",
    "if best_snapshot is not None:\n",
    "    torch.save(best_snapshot, final_weights_path)\n",
    "else:\n",
    "    # Fallback: save current (shouldn't happen unless no batches ran)\n",
    "    torch.save({\n",
    "        'backbone_state_dict': model.context_encoder.state_dict(),\n",
    "        'pixel_decoder_state_dict': model.pixel_decoder.state_dict(),\n",
    "        'transformer_decoder_cross_blocks_state_dict': model.predictor.cross_blocks.state_dict(),\n",
    "    }, final_weights_path)\n",
    "print(f\"Final pretrained weights saved (best-only): {final_weights_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e91e0e1-edab-4d02-977c-ae09378a1c5b",
   "metadata": {},
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b23d3c4-ac61-43ff-9774-2e7a6433cdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === NaN-Fixed Mask-JEPA fine-tuning @ 224x224 ===\n",
    "# import os, re, math, numpy as np, torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from collections import OrderedDict\n",
    "# from torch.optim import AdamW\n",
    "# from torch.optim.lr_scheduler import LambdaLR\n",
    "# from torch.amp import GradScaler, autocast\n",
    "# import matplotlib.pyplot as plt\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Enable anomaly detection for debugging\n",
    "# torch.autograd.set_detect_anomaly(False)  # Turn off for performance, enable if debugging\n",
    "\n",
    "# # -------------------------\n",
    "# # CONFIG\n",
    "# # -------------------------\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# finetune_epochs = 40\n",
    "# finetune_lr = 1e-3            # Reduced LR for stability\n",
    "# finetune_weight_decay = 0.05\n",
    "# num_classes = 150\n",
    "# BACKBONE_IM_SIZE = 224\n",
    "# IGNORE_INDEX = 255\n",
    "\n",
    "# print(f\"Fine-tuning for {finetune_epochs} epochs\")\n",
    "# vis_dir = \"./jepa_finetuning_visualizations\"\n",
    "# os.makedirs(vis_dir, exist_ok=True)\n",
    "\n",
    "# # -------------------------\n",
    "# # NUMERICALLY STABLE SEGMENTATION MODEL\n",
    "# # -------------------------\n",
    "# class SegmentationModel(nn.Module):\n",
    "#     def __init__(self, backbone, pixel_decoder, transformer_cross_blocks, num_classes, num_queries=32):\n",
    "#         super().__init__()\n",
    "#         self.backbone = backbone\n",
    "#         self.pixel_decoder = pixel_decoder\n",
    "#         self.transformer_decoder = nn.ModuleDict({'cross_blocks': transformer_cross_blocks})\n",
    "\n",
    "#         # Ensure embed_dim is properly set\n",
    "#         if not hasattr(self.pixel_decoder, \"embed_dim\"):\n",
    "#             self.pixel_decoder.embed_dim = getattr(self.backbone, \"embed_dim\")\n",
    "#         embed_dim = self.pixel_decoder.embed_dim\n",
    "\n",
    "#         self.num_queries = num_queries\n",
    "        \n",
    "#         # STABLE: Much smaller initialization to prevent NaN\n",
    "#         self.query_embed = nn.Parameter(torch.zeros(1, num_queries, embed_dim))\n",
    "#         nn.init.normal_(self.query_embed, std=0.01)  # Very small std\n",
    "        \n",
    "#         # Add learnable positional encodings for queries\n",
    "#         self.query_pos = nn.Parameter(torch.zeros(1, num_queries, embed_dim))\n",
    "#         nn.init.normal_(self.query_pos, std=0.01)\n",
    "\n",
    "#         # STABLE: Simpler, more robust heads\n",
    "#         self.class_head = nn.Sequential(\n",
    "#             nn.LayerNorm(embed_dim),\n",
    "#             nn.Linear(embed_dim, num_classes)\n",
    "#         )\n",
    "        \n",
    "#         self.mask_head = nn.Sequential(\n",
    "#             nn.LayerNorm(embed_dim),\n",
    "#             nn.Linear(embed_dim, embed_dim)\n",
    "#         )\n",
    "\n",
    "#         # Pyramid feature processing\n",
    "#         self.ds16 = nn.Sequential(\n",
    "#             nn.Conv2d(embed_dim, embed_dim, 3, stride=2, padding=1), \n",
    "#             nn.GroupNorm(min(32, embed_dim), embed_dim),\n",
    "#             nn.GELU(),\n",
    "#         )\n",
    "#         self.ds32 = nn.Sequential(\n",
    "#             nn.Conv2d(embed_dim, embed_dim, 3, stride=2, padding=1), \n",
    "#             nn.GroupNorm(min(32, embed_dim), embed_dim),\n",
    "#             nn.GELU(),\n",
    "#         )\n",
    "\n",
    "#         # STABLE: Proper normalization for stability\n",
    "#         self.kv_norm = nn.LayerNorm(embed_dim)\n",
    "#         self.query_norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "#         # STABLE: Fixed temperature (no learnable parameter to avoid instability)\n",
    "#         self.register_buffer('mask_temp', torch.tensor(10.0))\n",
    "\n",
    "#         # STABLE: Conservative initialization\n",
    "#         self._init_weights()\n",
    "\n",
    "#     def _init_weights(self):\n",
    "#         # Very conservative initialization to prevent NaN\n",
    "#         for m in [self.class_head, self.mask_head]:\n",
    "#             for layer in m.modules():\n",
    "#                 if isinstance(layer, nn.Linear):\n",
    "#                     nn.init.xavier_uniform_(layer.weight, gain=0.01)  # Very small gain\n",
    "#                     if layer.bias is not None:\n",
    "#                         nn.init.constant_(layer.bias, 0)\n",
    "        \n",
    "#         # Set class head bias for better convergence  \n",
    "#         if hasattr(self.class_head[-1], 'bias') and self.class_head[-1].bias is not None:\n",
    "#             nn.init.constant_(self.class_head[-1].bias, -math.log(num_classes - 1))\n",
    "\n",
    "#     @staticmethod\n",
    "#     def _to_tokens(x: torch.Tensor) -> torch.Tensor:\n",
    "#         \"\"\"[B,D,H,W] -> [B,HW,D]\"\"\"\n",
    "#         B, D, H, W = x.shape\n",
    "#         return x.permute(0,2,3,1).reshape(B, H*W, D)\n",
    "\n",
    "#     def _check_for_nan_inf(self, tensor, name):\n",
    "#         \"\"\"Debug helper to catch NaN/Inf\"\"\"\n",
    "#         if torch.isnan(tensor).any():\n",
    "#             print(f\"NaN detected in {name}\")\n",
    "#             return False\n",
    "#         if torch.isinf(tensor).any():\n",
    "#             print(f\"Inf detected in {name}\")\n",
    "#             return False\n",
    "#         return True\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         B, C, H_raw, W_raw = x.shape\n",
    "\n",
    "#         # 1) Backbone\n",
    "#         x_in = x if (H_raw == BACKBONE_IM_SIZE and W_raw == BACKBONE_IM_SIZE) else \\\n",
    "#                F.interpolate(x, size=(BACKBONE_IM_SIZE, BACKBONE_IM_SIZE), mode='bilinear', align_corners=False)\n",
    "\n",
    "#         tokens, (enc_h, enc_w) = self.backbone(x_in)\n",
    "#         feat = tokens.transpose(1, 2).reshape(B, self.backbone.embed_dim, enc_h, enc_w)\n",
    "        \n",
    "#         if not self._check_for_nan_inf(feat, \"backbone_feat\"):\n",
    "#             raise ValueError(\"NaN/Inf in backbone features\")\n",
    "\n",
    "#         # 2) Build pyramid\n",
    "#         C3 = F.interpolate(feat, size=(H_raw // 8,  W_raw // 8),  mode='bilinear', align_corners=False)\n",
    "#         x16 = self.ds16(C3)\n",
    "#         x32 = self.ds32(x16)\n",
    "#         C4 = F.interpolate(x16, size=(H_raw // 16, W_raw // 16), mode='bilinear', align_corners=False)\n",
    "#         C5 = F.interpolate(x32, size=(H_raw // 32, W_raw // 32), mode='bilinear', align_corners=False)\n",
    "\n",
    "#         # 3) Pixel decoder\n",
    "#         Fi1, _ = self.pixel_decoder([C3, C4, C5], (H_raw, W_raw))\n",
    "        \n",
    "#         if not self._check_for_nan_inf(Fi1, \"Fi1\"):\n",
    "#             raise ValueError(\"NaN/Inf in pixel decoder output\")\n",
    "\n",
    "#         # 4) STABLE: More robust transformer decoder\n",
    "#         # Simple positional encoding (no complex sin/cos to avoid numerical issues)\n",
    "#         def _simple_pos_encoding(x):\n",
    "#             B, D, H, W = x.shape\n",
    "#             pos_h = torch.linspace(-1, 1, H, device=x.device, dtype=x.dtype).view(1, 1, H, 1).expand(B, D//2, H, W)\n",
    "#             pos_w = torch.linspace(-1, 1, W, device=x.device, dtype=x.dtype).view(1, 1, 1, W).expand(B, D-D//2, H, W)\n",
    "#             return torch.cat([pos_h, pos_w], dim=1) * 0.1  # Small scale factor\n",
    "\n",
    "#         Fi1_pos = Fi1 + _simple_pos_encoding(Fi1)\n",
    "#         kv_seq = self._to_tokens(Fi1_pos).contiguous()\n",
    "#         kv_seq = self.kv_norm(kv_seq)\n",
    "        \n",
    "#         if not self._check_for_nan_inf(kv_seq, \"kv_seq\"):\n",
    "#             raise ValueError(\"NaN/Inf in KV sequence\")\n",
    "\n",
    "#         # Initialize queries\n",
    "#         queries = self.query_embed.expand(B, -1, -1) + self.query_pos.expand(B, -1, -1)\n",
    "        \n",
    "#         # STABLE: Add small noise to break symmetry without causing instability\n",
    "#         if self.training:\n",
    "#             queries = queries + torch.randn_like(queries) * 0.001\n",
    "        \n",
    "#         if not self._check_for_nan_inf(queries, \"initial_queries\"):\n",
    "#             raise ValueError(\"NaN/Inf in initial queries\")\n",
    "        \n",
    "#         # Transformer layers with careful gradient monitoring\n",
    "#         for i, blk in enumerate(self.transformer_decoder['cross_blocks']):\n",
    "#             queries_norm = self.query_norm(queries)\n",
    "#             if not self._check_for_nan_inf(queries_norm, f\"queries_norm_{i}\"):\n",
    "#                 raise ValueError(f\"NaN/Inf in queries_norm layer {i}\")\n",
    "            \n",
    "#             residual = queries\n",
    "#             queries = blk(queries_norm, kv_seq)\n",
    "            \n",
    "#             if not self._check_for_nan_inf(queries, f\"queries_after_block_{i}\"):\n",
    "#                 raise ValueError(f\"NaN/Inf in queries after block {i}\")\n",
    "            \n",
    "#             # Add residual connection with gradient clipping\n",
    "#             queries = residual + queries\n",
    "            \n",
    "#             # Gradient clipping within forward pass to prevent explosion\n",
    "#             if queries.requires_grad:\n",
    "#                 queries.register_hook(lambda grad: torch.clamp(grad, -10.0, 10.0))\n",
    "\n",
    "#         # 5) STABLE: Robust heads with numerical safeguards\n",
    "#         class_logits = self.class_head(queries)\n",
    "#         mask_features = self.mask_head(queries)\n",
    "        \n",
    "#         if not self._check_for_nan_inf(class_logits, \"class_logits\"):\n",
    "#             raise ValueError(\"NaN/Inf in class_logits\")\n",
    "#         if not self._check_for_nan_inf(mask_features, \"mask_features\"):\n",
    "#             raise ValueError(\"NaN/Inf in mask_features\")\n",
    "\n",
    "#         # STABLE: Very careful mask computation to avoid NaN in einsum\n",
    "#         Fi1_for_mask = Fi1_pos\n",
    "#         Fi1_flat = Fi1_for_mask.flatten(2)  # [B,D,HW']\n",
    "        \n",
    "#         # STABLE: L2 normalize with epsilon for numerical stability\n",
    "#         eps = 1e-8\n",
    "#         mask_features_norm = F.normalize(mask_features + eps, dim=-1, p=2, eps=eps)  # [B,Q,D]\n",
    "#         Fi1_flat_norm = F.normalize(Fi1_flat + eps, dim=1, p=2, eps=eps)  # [B,D,HW']\n",
    "        \n",
    "#         if not self._check_for_nan_inf(mask_features_norm, \"mask_features_norm\"):\n",
    "#             raise ValueError(\"NaN/Inf in mask_features_norm\")\n",
    "#         if not self._check_for_nan_inf(Fi1_flat_norm, \"Fi1_flat_norm\"):\n",
    "#             raise ValueError(\"NaN/Inf in Fi1_flat_norm\")\n",
    "        \n",
    "#         # STABLE: Use bmm instead of einsum for better numerical control\n",
    "#         # einsum('bqd,bdl->bql') equivalent to bmm\n",
    "#         mask_logits = torch.bmm(mask_features_norm, Fi1_flat_norm)  # [B,Q,HW']\n",
    "        \n",
    "#         if not self._check_for_nan_inf(mask_logits, \"mask_logits_raw\"):\n",
    "#             raise ValueError(\"NaN/Inf in raw mask_logits\")\n",
    "        \n",
    "#         # Apply temperature with clamping\n",
    "#         mask_logits = mask_logits * self.mask_temp.clamp(min=0.1, max=50.0)\n",
    "        \n",
    "#         # STABLE: Clamp logits to prevent extreme values\n",
    "#         mask_logits = torch.clamp(mask_logits, min=-50.0, max=50.0)\n",
    "\n",
    "#         # Reshape and upsample\n",
    "#         H_fi1, W_fi1 = Fi1.shape[-2], Fi1.shape[-1]\n",
    "#         mask_logits = mask_logits.reshape(B, self.num_queries, H_fi1, W_fi1)\n",
    "#         mask_logits = F.interpolate(mask_logits, size=(H_raw, W_raw), mode='bilinear', align_corners=False)\n",
    "        \n",
    "#         if not self._check_for_nan_inf(mask_logits, \"final_mask_logits\"):\n",
    "#             raise ValueError(\"NaN/Inf in final mask_logits\")\n",
    "\n",
    "#         return {'class_logits': class_logits, 'mask_logits': mask_logits}\n",
    "\n",
    "# # -------------------------\n",
    "# # STABLE LOSS FUNCTION\n",
    "# # -------------------------\n",
    "# def stable_semantic_ce(outputs, targets, *, ignore_index=IGNORE_INDEX, num_classes=None, label_smoothing=0.0):\n",
    "#     \"\"\"\n",
    "#     STABLE: More numerically robust mixture-of-queries loss\n",
    "#     \"\"\"\n",
    "#     class_logits = outputs['class_logits'].float()   # [B,Q,C]\n",
    "#     mask_logits  = outputs['mask_logits'].float()    # [B,Q,H,W]\n",
    "\n",
    "#     B, Q, C = class_logits.shape\n",
    "#     _, _, H, W = mask_logits.shape\n",
    "\n",
    "#     # STABLE: Clamp inputs to prevent extreme values\n",
    "#     class_logits = torch.clamp(class_logits, min=-50, max=50)\n",
    "#     mask_logits = torch.clamp(mask_logits, min=-50, max=50)\n",
    "    \n",
    "#     # STABLE: Use softmax with temperature for better numerical properties\n",
    "#     temp = 2.0  # Temperature to soften distributions\n",
    "#     class_probs = F.softmax(class_logits / temp, dim=-1)  # [B,Q,C]\n",
    "#     mask_probs = F.softmax(mask_logits / temp, dim=1)     # [B,Q,H,W]\n",
    "    \n",
    "#     # Check for NaN in probabilities\n",
    "#     if torch.isnan(class_probs).any() or torch.isnan(mask_probs).any():\n",
    "#         print(\"NaN detected in probabilities - using fallback loss\")\n",
    "#         # Fallback: just use class logits from first query\n",
    "#         first_query_logits = class_logits[:, 0]  # [B,C]\n",
    "#         first_query_logits = first_query_logits.unsqueeze(-1).unsqueeze(-1).expand(B, C, H, W)\n",
    "#         return F.cross_entropy(first_query_logits, targets, ignore_index=ignore_index)\n",
    "    \n",
    "#     # STABLE: Mixture computation with numerical safeguards\n",
    "#     class_probs_exp = class_probs.permute(0,2,1).unsqueeze(-1).unsqueeze(-1)  # [B,C,Q,1,1]\n",
    "    \n",
    "#     # Element-wise multiplication and sum\n",
    "#     pixel_probs = (class_probs_exp * mask_probs.unsqueeze(1)).sum(dim=2)  # [B,C,H,W]\n",
    "    \n",
    "#     # STABLE: Add small epsilon and renormalize\n",
    "#     eps = 1e-8\n",
    "#     pixel_probs = pixel_probs + eps\n",
    "#     pixel_probs = pixel_probs / pixel_probs.sum(dim=1, keepdim=True)\n",
    "#     pixel_probs = torch.clamp(pixel_probs, min=eps, max=1.0 - eps)\n",
    "    \n",
    "#     # Convert to logits\n",
    "#     pixel_logits = torch.log(pixel_probs / (1 - pixel_probs + eps))\n",
    "#     pixel_logits = torch.clamp(pixel_logits, min=-50, max=50)\n",
    "    \n",
    "#     if torch.isnan(pixel_logits).any():\n",
    "#         print(\"NaN in pixel_logits - using fallback\")\n",
    "#         # Another fallback\n",
    "#         return F.cross_entropy(class_logits.mean(1).unsqueeze(-1).unsqueeze(-1).expand(B, C, H, W), \n",
    "#                               targets, ignore_index=ignore_index)\n",
    "\n",
    "#     # Standard cross entropy\n",
    "#     targets = targets.long()\n",
    "#     if num_classes is not None:\n",
    "#         ok = (targets != ignore_index)\n",
    "#         if ok.any():\n",
    "#             targets = torch.where(ok, targets.clamp(0, num_classes - 1), targets)\n",
    "\n",
    "#     loss = F.cross_entropy(\n",
    "#         pixel_logits, targets,\n",
    "#         ignore_index=ignore_index,\n",
    "#         label_smoothing=label_smoothing\n",
    "#     )\n",
    "    \n",
    "#     return loss\n",
    "\n",
    "# # -------------------------\n",
    "# # UTILITIES (SIMPLIFIED TO AVOID ISSUES)\n",
    "# # -------------------------\n",
    "\n",
    "# def remap_and_sanitize_targets(t, num_classes=num_classes, ignore_index=IGNORE_INDEX):\n",
    "#     \"\"\"Clean up ADE-style labels\"\"\"\n",
    "#     t = t.long()\n",
    "#     has_numclass = (t == num_classes).any()\n",
    "#     valid = (t != ignore_index)\n",
    "#     max_lab = int(t[valid].max()) if valid.any() else -1\n",
    "#     one_based = has_numclass or (max_lab == num_classes)\n",
    "\n",
    "#     if one_based:\n",
    "#         zero_mask = (t == 0)\n",
    "#         t = torch.where((t >= 1) & (t <= num_classes), t - 1, t)\n",
    "#         t = torch.where(zero_mask, torch.as_tensor(ignore_index, device=t.device, dtype=t.dtype), t)\n",
    "\n",
    "#     bad_lo = (t < 0) & (t != ignore_index)\n",
    "#     bad_hi = (t >= num_classes) & (t != ignore_index)\n",
    "#     if bad_lo.any() or bad_hi.any():\n",
    "#         t = torch.where(bad_lo | bad_hi, torch.as_tensor(ignore_index, device=t.device, dtype=t.dtype), t)\n",
    "#     return t\n",
    "\n",
    "# # -------------------------\n",
    "# # VISUALIZATION FUNCTIONS\n",
    "# # -------------------------\n",
    "# def visualize_segmentation(images, targets, outputs, epoch, save_path):\n",
    "#     \"\"\"Create segmentation visualization: Original | Ground Truth | Prediction\"\"\"\n",
    "#     B = min(2, images.shape[0])\n",
    "    \n",
    "#     # Denormalize images for visualization\n",
    "#     mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(images.device)\n",
    "#     std  = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(images.device)\n",
    "#     images_vis = torch.clamp(images * std + mean, 0, 1)\n",
    "\n",
    "#     # Get predictions from model outputs\n",
    "#     class_logits = outputs['class_logits']  # [B,Q,C]\n",
    "#     mask_logits  = outputs['mask_logits']   # [B,Q,H,W]\n",
    "    \n",
    "#     # For each pixel, find the query with highest mask probability\n",
    "#     mask_probs = torch.sigmoid(mask_logits)  # [B,Q,H,W]\n",
    "#     best_queries = mask_probs.argmax(dim=1)  # [B,H,W] - which query wins each pixel\n",
    "    \n",
    "#     # Get class predictions for each query\n",
    "#     class_preds = class_logits.argmax(dim=-1)  # [B,Q] - class per query\n",
    "    \n",
    "#     # Create pixel-wise predictions\n",
    "#     H, W = best_queries.shape[-2:]\n",
    "#     pixel_preds = torch.zeros(B, H, W, device=targets.device, dtype=torch.long)\n",
    "#     for b in range(B):\n",
    "#         pixel_preds[b] = class_preds[b][best_queries[b]]\n",
    "\n",
    "#     # Create visualization\n",
    "#     fig, axes = plt.subplots(B, 3, figsize=(12, 4*B))\n",
    "#     if B == 1: \n",
    "#         axes = axes.reshape(1, -1)\n",
    "    \n",
    "#     for i in range(B):\n",
    "#         # Original image\n",
    "#         img = images_vis[i].permute(1, 2, 0).detach().cpu().numpy()\n",
    "#         axes[i, 0].imshow(img)\n",
    "#         axes[i, 0].set_title('Original')\n",
    "#         axes[i, 0].axis('off')\n",
    "\n",
    "#         # Ground truth\n",
    "#         gt = targets[i].detach().cpu().numpy()\n",
    "#         gt_vis = np.where(gt == IGNORE_INDEX, 0, gt % 20)  # Mod 20 for colormap\n",
    "#         im1 = axes[i, 1].imshow(gt_vis, cmap='tab20', vmin=0, vmax=19)\n",
    "#         axes[i, 1].set_title('Ground Truth')\n",
    "#         axes[i, 1].axis('off')\n",
    "\n",
    "#         # Prediction\n",
    "#         pred = pixel_preds[i].detach().cpu().numpy()\n",
    "#         pred_vis = pred % 20  # Mod 20 for colormap consistency\n",
    "#         im2 = axes[i, 2].imshow(pred_vis, cmap='tab20', vmin=0, vmax=19)\n",
    "#         axes[i, 2].set_title('Prediction')\n",
    "#         axes[i, 2].axis('off')\n",
    "\n",
    "#     plt.suptitle(f'Segmentation Results - Epoch {epoch}')\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(save_path, dpi=100, bbox_inches='tight')\n",
    "#     plt.close()\n",
    "\n",
    "# def compute_miou(outputs, targets, num_classes):\n",
    "#     \"\"\"Compute mean IoU for evaluation\"\"\"\n",
    "#     class_logits = outputs['class_logits']   # [B,Q,C]\n",
    "#     mask_logits  = outputs['mask_logits']    # [B,Q,H,W]\n",
    "\n",
    "#     mask_probs = torch.sigmoid(mask_logits)\n",
    "#     best_queries = mask_probs.argmax(dim=1)  # [B,H,W]\n",
    "\n",
    "#     B, H, W = best_queries.shape\n",
    "#     pixel_preds = torch.zeros(B, H, W, device=targets.device, dtype=torch.long)\n",
    "#     for b in range(B):\n",
    "#         class_preds = class_logits[b].argmax(dim=-1)  # [Q]\n",
    "#         pixel_preds[b] = class_preds[best_queries[b]]\n",
    "\n",
    "#     ious = []\n",
    "#     for cls in range(min(20, num_classes)):  # Limit to 20 classes for computational efficiency\n",
    "#         pred_mask = (pixel_preds == cls)\n",
    "#         target_mask = (targets == cls)\n",
    "#         inter = (pred_mask & target_mask).sum().float()\n",
    "#         union = (pred_mask | target_mask).sum().float()\n",
    "#         if union > 0:\n",
    "#             ious.append((inter / union).item())\n",
    "#     return float(np.mean(ious)) if ious else 0.0\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def evaluate_model(model, val_loader, num_classes):\n",
    "#     \"\"\"Evaluate model on validation set\"\"\"\n",
    "#     model.eval()\n",
    "#     total_miou, num_batches = 0.0, 0\n",
    "#     max_eval_batches = min(30, len(val_loader))\n",
    "    \n",
    "#     for batch_idx, batch in enumerate(val_loader):\n",
    "#         if batch_idx >= max_eval_batches:\n",
    "#             break\n",
    "#         try:\n",
    "#             images = batch['images'].to(device)\n",
    "#             masks = remap_and_sanitize_targets(batch['masks'].to(device), num_classes)\n",
    "            \n",
    "#             outputs = model(images)\n",
    "#             miou = compute_miou(outputs, masks, num_classes)\n",
    "#             total_miou += miou\n",
    "#             num_batches += 1\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error in evaluation batch {batch_idx}: {e}\")\n",
    "#             continue\n",
    "    \n",
    "#     return total_miou / num_batches if num_batches > 0 else 0.0\n",
    "\n",
    "# # Placeholder for load_pretrained_weights - use your existing implementation\n",
    "# def load_pretrained_weights(checkpoint_path, prefer_branch=\"online\"):\n",
    "#     print(f\"Loading pretrained weights from {checkpoint_path}\")\n",
    "#     ckpt = torch.load(checkpoint_path, map_location='cpu')\n",
    "    \n",
    "#     # Create modules (you need to ensure these classes are defined)\n",
    "#     backbone = ContextEncoder2D(model_name=\"vit_tiny_patch16_224\", pretrained=False)\n",
    "#     pixel_decoder = PixelDecoder2D(in_channels=backbone.embed_dim, embed_dim=backbone.embed_dim)\n",
    "#     embed_dim = backbone.embed_dim\n",
    "    \n",
    "#     # Load cross blocks\n",
    "#     saved_x_sd = ckpt.get(\"transformer_decoder_cross_blocks_state_dict\", None)\n",
    "#     n_x = 9  # or infer from saved state\n",
    "#     transformer_cross_blocks = nn.ModuleList([CrossAttentionBlock2D(embed_dim, num_heads=8) for _ in range(n_x)])\n",
    "    \n",
    "#     # Load weights (simplified - you can use your detailed version)\n",
    "#     if \"backbone_state_dict\" in ckpt:\n",
    "#         backbone.load_state_dict(ckpt[\"backbone_state_dict\"], strict=True)\n",
    "#     if \"pixel_decoder_state_dict\" in ckpt:\n",
    "#         pixel_decoder.load_state_dict(ckpt[\"pixel_decoder_state_dict\"], strict=False)\n",
    "#     if saved_x_sd:\n",
    "#         transformer_cross_blocks.load_state_dict(saved_x_sd, strict=False)\n",
    "    \n",
    "#     return backbone, pixel_decoder, transformer_cross_blocks\n",
    "\n",
    "# # -------------------------\n",
    "# # MAIN TRAINING LOOP\n",
    "# # -------------------------\n",
    "# print(\"=\"*60)\n",
    "# print(\"LOADING MASK-JEPA PRETRAINED WEIGHTS\")\n",
    "# print(\"=\"*60)\n",
    "\n",
    "# pretrained_path = \"/home/sks6nv/Projects/RL-JEPA/jepa_training_output//mask_jepa_pretrained_weights.pt\"\n",
    "# backbone, pixel_decoder, transformer_cross_blocks = load_pretrained_weights(pretrained_path)\n",
    "\n",
    "# # Use fewer queries initially for stability\n",
    "# model = SegmentationModel(backbone, pixel_decoder, transformer_cross_blocks, num_classes, num_queries=32).to(device)\n",
    "\n",
    "# # STABLE: More conservative parameter groups\n",
    "# head_params = []\n",
    "# head_params.extend(list(model.class_head.parameters()))\n",
    "# head_params.extend(list(model.mask_head.parameters()))\n",
    "# head_params.extend([model.query_embed, model.query_pos])\n",
    "\n",
    "# pretrained_params = []\n",
    "# pretrained_params.extend(list(model.backbone.parameters()))\n",
    "# pretrained_params.extend(list(model.pixel_decoder.parameters()))\n",
    "# pretrained_params.extend(list(model.transformer_decoder['cross_blocks'].parameters()))\n",
    "\n",
    "# # STABLE: Much more conservative learning rates\n",
    "# optimizer = AdamW([\n",
    "#     {\"params\": head_params, \"lr\": finetune_lr * 2, \"weight_decay\": finetune_weight_decay},\n",
    "#     {\"params\": pretrained_params, \"lr\": finetune_lr * 0.1, \"weight_decay\": finetune_weight_decay * 0.1},\n",
    "# ], betas=(0.9, 0.999), eps=1e-8)\n",
    "\n",
    "# scheduler = LambdaLR(optimizer, lr_lambda=lambda epoch: max(0.1, (1 - epoch / finetune_epochs) ** 0.5))\n",
    "# scaler = GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "# print(f\"Trainable params: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "# print(\"Starting training...\")\n",
    "\n",
    "# best_miou = 0.0\n",
    "\n",
    "# for epoch in range(finetune_epochs):\n",
    "#     model.train()\n",
    "#     epoch_loss, num_batches = 0.0, 0\n",
    "#     max_batches = min(150, len(downstream_train_loader))\n",
    "    \n",
    "#     pbar = tqdm(downstream_train_loader, desc=f\"Epoch {epoch+1}/{finetune_epochs}\", total=max_batches)\n",
    "\n",
    "#     for batch_idx, batch in enumerate(pbar):\n",
    "#         if batch_idx >= max_batches: \n",
    "#             break\n",
    "            \n",
    "#         try:\n",
    "#             images = batch['images'].to(device)\n",
    "#             masks = remap_and_sanitize_targets(batch['masks'].to(device), num_classes)\n",
    "\n",
    "#             optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "#             with autocast('cuda', enabled=torch.cuda.is_available()):\n",
    "#                 outputs = model(images)\n",
    "                \n",
    "#                 # STABLE: Simple loss without complex regularization initially\n",
    "#                 loss = stable_semantic_ce(outputs, masks, ignore_index=IGNORE_INDEX,\n",
    "#                                         num_classes=num_classes, label_smoothing=0.0)\n",
    "\n",
    "#             # Check for finite loss\n",
    "#             if not torch.isfinite(loss):\n",
    "#                 print(f\"Non-finite loss at batch {batch_idx}: {loss.item()}\")\n",
    "#                 continue\n",
    "\n",
    "#             scaler.scale(loss).backward()\n",
    "#             scaler.unscale_(optimizer)\n",
    "            \n",
    "#             # STABLE: More aggressive gradient clipping\n",
    "#             torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "            \n",
    "#             scaler.step(optimizer)\n",
    "#             scaler.update()\n",
    "\n",
    "#             epoch_loss += float(loss.item())\n",
    "#             num_batches += 1\n",
    "#             pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error in batch {batch_idx}: {e}\")\n",
    "#             # Clear any corrupted gradients\n",
    "#             optimizer.zero_grad(set_to_none=True)\n",
    "#             continue\n",
    "\n",
    "#     scheduler.step()\n",
    "#     avg_loss = epoch_loss / num_batches if num_batches > 0 else 0\n",
    "#     cur_lr = optimizer.param_groups[0]['lr']\n",
    "#     print(f\"\\nEpoch {epoch+1}/{finetune_epochs} - Loss: {avg_loss:.4f}, LR: {cur_lr:.2e}\")\n",
    "\n",
    "#     # Visualization every epoch\n",
    "#     if (epoch + 1) % 1 == 0:\n",
    "#         print(\"  Creating visualizations...\")\n",
    "#         model.eval()\n",
    "#         try:\n",
    "#             with torch.no_grad():\n",
    "#                 vis_batch = next(iter(downstream_val_loader))\n",
    "#                 vis_images = vis_batch['images'][:2].to(device)\n",
    "#                 vis_masks = remap_and_sanitize_targets(vis_batch['masks'][:2].to(device), num_classes)\n",
    "#                 vis_outputs = model(vis_images)\n",
    "                \n",
    "#                 vis_path = os.path.join(vis_dir, f\"seg_epoch_{epoch+1:03d}.png\")\n",
    "#                 visualize_segmentation(vis_images, vis_masks, vis_outputs, epoch+1, vis_path)\n",
    "#                 print(f\"    Saved visualization: {vis_path}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"    Visualization failed: {e}\")\n",
    "#         model.train()\n",
    "\n",
    "#     # Evaluation every 5 epochs\n",
    "#     if (epoch + 1) % 5 == 0:\n",
    "#         print(\"  Running evaluation...\")\n",
    "#         try:\n",
    "#             miou = evaluate_model(model, downstream_val_loader, num_classes)\n",
    "#             print(f\"  Validation mIoU: {miou:.4f}\")\n",
    "#             if miou > best_miou:\n",
    "#                 best_miou = miou\n",
    "#                 best_checkpoint_path = f\"./jepa_training_output/mask_jepa_finetuned_best.pt\"\n",
    "#                 torch.save({\n",
    "#                     'model_state_dict': model.state_dict(),\n",
    "#                     'best_miou': best_miou,\n",
    "#                     'epoch': epoch + 1\n",
    "#                 }, best_checkpoint_path)\n",
    "#                 print(f\"  ✓ New best model saved! mIoU: {best_miou:.4f}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"  Evaluation failed: {e}\")\n",
    "\n",
    "#     # Checkpoint saving\n",
    "#     if (epoch + 1) % 10 == 0:\n",
    "#         try:\n",
    "#             checkpoint_path = f\"./jepa_training_output/mask_jepa_finetuned_epoch_{epoch+1}.pt\"\n",
    "#             torch.save({\n",
    "#                 'model_state_dict': model.state_dict(),\n",
    "#                 'epoch': epoch + 1,\n",
    "#                 'loss': avg_loss\n",
    "#             }, checkpoint_path)\n",
    "#             print(f\"  ✓ Checkpoint saved: {checkpoint_path}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"  Failed to save checkpoint: {e}\")\n",
    "\n",
    "# print(\"Fine-tuning completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42c37c64-1f19-4026-a52e-c5cc9392b5c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADE20K uniques (peek): tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19])\n",
      "Loading pretrained JEPA model...\n",
      "WARNING: no JEPA weights found; FT from scratch\n",
      "Starting fine-tuning (Fusion head, CE only)...\n",
      "Train batches: 422, Val batches: 42\n",
      "Model trainable params: 78,578,721\n",
      "  Batch    0/422 | CE: 5.0341 | mIoU: 0.0003 | Dice: 0.0007\n",
      "  Batch   50/422 | CE: 3.2219 | mIoU: 0.0078 | Dice: 0.0128\n",
      "  Batch  100/422 | CE: 2.7778 | mIoU: 0.0150 | Dice: 0.0220\n",
      "  Batch  150/422 | CE: 2.5199 | mIoU: 0.0230 | Dice: 0.0327\n",
      "  Batch  200/422 | CE: 2.2715 | mIoU: 0.0281 | Dice: 0.0387\n",
      "  Batch  250/422 | CE: 2.1032 | mIoU: 0.0324 | Dice: 0.0453\n",
      "  Batch  300/422 | CE: 2.2173 | mIoU: 0.0363 | Dice: 0.0511\n",
      "  Batch  350/422 | CE: 2.0572 | mIoU: 0.0426 | Dice: 0.0597\n",
      "  Batch  400/422 | CE: 2.0698 | mIoU: 0.0459 | Dice: 0.0646\n",
      "Epoch 01/40 | Train CE: 2.5257 | Val CE: 1.9439 | mIoU: 0.0478 | Dice: 0.0665 | LR: 2.00e-04\n",
      "  New best mIoU! Saved -> ./jepa_finetuning_output/best_segmentation_model.pt\n",
      "  Saved visualization: ./jepa_finetuning_output/seg_epoch_001.png\n",
      "  Batch    0/422 | CE: 1.7189 | mIoU: 0.0736 | Dice: 0.0975\n",
      "  Batch   50/422 | CE: 1.9336 | mIoU: 0.0503 | Dice: 0.0703\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 279\u001b[0m\n\u001b[1;32m    277\u001b[0m msks \u001b[38;5;241m=\u001b[39m masks[i:i\u001b[38;5;241m+\u001b[39mchunk]\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m     loss   \u001b[38;5;241m=\u001b[39m criterion(logits, fix_masks(msks)) \u001b[38;5;241m/\u001b[39m num_micro\n\u001b[1;32m    282\u001b[0m \u001b[38;5;66;03m# update GPU metrics before backward (keeps VRAM usage similar)\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[18], line 121\u001b[0m, in \u001b[0;36mJEPASegmentationModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    120\u001b[0m     B, C, H, W \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m--> 121\u001b[0m     tokens, (enc_h, enc_w) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m                 \u001b[38;5;66;03m# [B, P, D]\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     feat \u001b[38;5;241m=\u001b[39m tokens\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(B, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, enc_h, enc_w)\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;66;03m# pyramid as in pretrain\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[6], line 41\u001b[0m, in \u001b[0;36mContextEncoder2D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m     35\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m    x: [B, C, H, W]  (H,W multiples of 32 recommended; 384 works)\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m    returns:\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m      tokens: [B, P, D]  (CLS-free)\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m      (Ht, Wt): token grid size at the final stage (~ H/32, W/32)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     feats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mswin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# [B, L, D] or sometimes [B, Ht, Wt, D] / [B, D, Ht, Wt]\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m feats\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:                    \u001b[38;5;66;03m# [B, L, D]\u001b[39;00m\n\u001b[1;32m     44\u001b[0m         tokens \u001b[38;5;241m=\u001b[39m feats\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.9/site-packages/timm/models/swin_transformer.py:830\u001b[0m, in \u001b[0;36mSwinTransformer.forward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    829\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_embed(x)\n\u001b[0;32m--> 830\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    831\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[1;32m    832\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.9/site-packages/timm/models/swin_transformer.py:559\u001b[0m, in \u001b[0;36mSwinTransformerStage.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    557\u001b[0m     x \u001b[38;5;241m=\u001b[39m checkpoint_seq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks, x)\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 559\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.9/site-packages/timm/models/swin_transformer.py:408\u001b[0m, in \u001b[0;36mSwinTransformerBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    406\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x)))\n\u001b[1;32m    407\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(B, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, C)\n\u001b[0;32m--> 408\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path2(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    409\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(B, H, W, C)\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.9/site-packages/timm/layers/mlp.py:44\u001b[0m, in \u001b[0;36mMlp.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 44\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(x)\n\u001b[1;32m     46\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop1(x)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.9/site-packages/torch/fx/traceback.py:175\u001b[0m, in \u001b[0;36mformat_stack\u001b[0;34m()\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [current_meta\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstack_trace\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# fallback to traceback.format_stack()\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m traceback\u001b[38;5;241m.\u001b[39mformat_list(\u001b[43mtraceback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.9/traceback.py:211\u001b[0m, in \u001b[0;36mextract_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m     f \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39m_getframe()\u001b[38;5;241m.\u001b[39mf_back\n\u001b[0;32m--> 211\u001b[0m stack \u001b[38;5;241m=\u001b[39m \u001b[43mStackSummary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwalk_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m stack\u001b[38;5;241m.\u001b[39mreverse()\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stack\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.9/traceback.py:359\u001b[0m, in \u001b[0;36mStackSummary.extract\u001b[0;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    358\u001b[0m         f_locals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m     result\u001b[38;5;241m.\u001b[39mappend(\u001b[43mFrameSummary\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlineno\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlookup_line\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlocals\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mf_locals\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m fnames:\n\u001b[1;32m    362\u001b[0m     linecache\u001b[38;5;241m.\u001b[39mcheckcache(filename)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.9/traceback.py:254\u001b[0m, in \u001b[0;36mFrameSummary.__init__\u001b[0;34m(self, filename, lineno, name, lookup_line, locals, line)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename, lineno, name, \u001b[38;5;241m*\u001b[39m, lookup_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    245\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Construct a FrameSummary.\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \n\u001b[1;32m    247\u001b[0m \u001b[38;5;124;03m    :param lookup_line: If True, `linecache` is consulted for the source\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;124;03m        the linecache.\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 254\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilename\u001b[49m \u001b[38;5;241m=\u001b[39m filename\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineno \u001b[38;5;241m=\u001b[39m lineno\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m name\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ==== Fine-tuning: Fusion head (Fi1 + F_last), CE-only, with fast GPU metrics & batch prints ====\n",
    "import os, gc, math, numpy as np\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.amp import GradScaler, autocast\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "IGNORE_INDEX = 255\n",
    "NUM_CLASSES  = 150\n",
    "\n",
    "# -----------------------\n",
    "# Helpers\n",
    "# -----------------------\n",
    "def _gn_groups(C):\n",
    "    for g in (32,16,8,4,2,1):\n",
    "        if C % g == 0: return g\n",
    "    return 1\n",
    "\n",
    "class DWSepResBlock(nn.Module):\n",
    "    def __init__(self, channels: int, dilation: int = 1):\n",
    "        super().__init__()\n",
    "        self.dw   = nn.Conv2d(channels, channels, 3, padding=dilation, dilation=dilation,\n",
    "                              groups=channels, bias=False)\n",
    "        self.dw_g = nn.GroupNorm(_gn_groups(channels), channels)\n",
    "        self.pw   = nn.Conv2d(channels, channels, 1, bias=False)\n",
    "        self.pw_g = nn.GroupNorm(_gn_groups(channels), channels)\n",
    "        self.act  = nn.ReLU(inplace=True)\n",
    "    def forward(self, x):\n",
    "        y = self.act(self.dw_g(self.dw(x)))\n",
    "        y = self.pw_g(self.pw(y))\n",
    "        return self.act(x + y)\n",
    "\n",
    "class SpatialGate(nn.Module):\n",
    "    def __init__(self, channels: int):\n",
    "        super().__init__()\n",
    "        # upgraded: depthwise 3x3 then pointwise 1x1 before sigmoid\n",
    "        self.dw = nn.Conv2d(channels, channels, 3, padding=1, groups=channels, bias=False)\n",
    "        self.pw = nn.Conv2d(channels, 1, 1, bias=True)\n",
    "    def forward(self, x):\n",
    "        a = self.pw(self.dw(x))\n",
    "        return x * torch.sigmoid(a)\n",
    "\n",
    "class ChannelSE(nn.Module):\n",
    "    \"\"\"Lightweight channel attention (squeeze-excite).\"\"\"\n",
    "    def __init__(self, channels: int, r: int = 8):\n",
    "        super().__init__()\n",
    "        m = max(1, channels // r)\n",
    "        self.fc1 = nn.Conv2d(channels, m, 1, bias=True)\n",
    "        self.fc2 = nn.Conv2d(m, channels, 1, bias=True)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "    def forward(self, x):\n",
    "        s = x.mean(dim=(2,3), keepdim=True)\n",
    "        s = self.act(self.fc1(s))\n",
    "        s = torch.sigmoid(self.fc2(s))\n",
    "        return x * s\n",
    "\n",
    "class FusionSegHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Fi1 (s/8) ↑ to s/4 + F_last (s/4) -> DW+PW fusion -> 3x DW-sep residual (dil=1,2,4)\n",
    "    -> spatial gate -> channel SE -> 1x1 classes\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, mid_channels: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.fi1_reduce   = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, 1, bias=False),\n",
    "            nn.GroupNorm(_gn_groups(mid_channels), mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.flast_reduce = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, 1, bias=False),\n",
    "            nn.GroupNorm(_gn_groups(mid_channels), mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        # upgraded fusion: depthwise 3x3 then pointwise 1x1\n",
    "        self.fuse_dw = nn.Conv2d(2*mid_channels, 2*mid_channels, 3, padding=1,\n",
    "                                 groups=2*mid_channels, bias=False)\n",
    "        self.fuse_pw = nn.Sequential(\n",
    "            nn.Conv2d(2*mid_channels, mid_channels, 1, bias=False),\n",
    "            nn.GroupNorm(_gn_groups(mid_channels), mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        # strengthened refinement: add a third block with dil=4\n",
    "        self.refine1 = DWSepResBlock(mid_channels, dilation=1)\n",
    "        self.refine2 = DWSepResBlock(mid_channels, dilation=2)\n",
    "        self.refine3 = DWSepResBlock(mid_channels, dilation=4)\n",
    "        # dual lightweight attention\n",
    "        self.spatial = SpatialGate(mid_channels)\n",
    "        self.channel = ChannelSE(mid_channels, r=8)\n",
    "        self.cls     = nn.Conv2d(mid_channels, num_classes, 1)\n",
    "\n",
    "    def forward(self, fi1, flast, out_hw):\n",
    "        H, W = out_hw\n",
    "        fi1_up = F.interpolate(fi1, size=flast.shape[-2:], mode='bilinear', align_corners=False)\n",
    "        z = torch.cat([self.fi1_reduce(fi1_up), self.flast_reduce(flast)], dim=1)\n",
    "        z = self.fuse_pw(self.fuse_dw(z))\n",
    "        z = self.refine1(z)\n",
    "        z = self.refine2(z)\n",
    "        z = self.refine3(z)\n",
    "        z = self.spatial(z)\n",
    "        z = self.channel(z)\n",
    "        logits_s4 = self.cls(z)\n",
    "        return F.interpolate(logits_s4, size=(H, W), mode='bilinear', align_corners=False)\n",
    "\n",
    "class JEPASegmentationModel(nn.Module):\n",
    "    \"\"\"\n",
    "    JEPA backbone + pixel decoder -> FusionSegHead (no ASPP, no aux).\n",
    "    \"\"\"\n",
    "    def __init__(self, backbone_model, num_classes=150, mid_channels=128):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone_model.context_encoder\n",
    "        self.pixel_decoder = backbone_model.pixel_decoder\n",
    "        self.embed_dim = backbone_model.embed_dim\n",
    "        self.ds16 = backbone_model.ds16\n",
    "        self.ds32 = backbone_model.ds32\n",
    "        self.head = FusionSegHead(self.embed_dim, mid_channels, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        tokens, (enc_h, enc_w) = self.backbone(x)                 # [B, P, D]\n",
    "        feat = tokens.transpose(1,2).reshape(B, self.embed_dim, enc_h, enc_w)\n",
    "\n",
    "        # pyramid as in pretrain\n",
    "        C3  = F.interpolate(feat, size=(H//8,  W//8),  mode='bilinear', align_corners=False)\n",
    "        x16 = self.ds16(C3)\n",
    "        x32 = self.ds32(x16)\n",
    "        C4  = F.interpolate(x16, size=(H//16, W//16), mode='bilinear', align_corners=False)\n",
    "        C5  = F.interpolate(x32, size=(H//32, W//32), mode='bilinear', align_corners=False)\n",
    "\n",
    "        Fi1, F_last = self.pixel_decoder([C3, C4, C5], (H, W))    # Fi1 ~ s/8, F_last ~ s/4\n",
    "        return self.head(Fi1, F_last, (H, W))                     # [B, K, H, W]\n",
    "\n",
    "def fix_masks(m):\n",
    "    return torch.where((m < 0) | (m >= NUM_CLASSES),\n",
    "                       torch.full_like(m, IGNORE_INDEX),\n",
    "                       m).long()\n",
    "\n",
    "class StreamingSegMetrics:\n",
    "    \"\"\"GPU-side streaming confusion matrix for mIoU/Dice (no CPU stalls).\"\"\"\n",
    "    def __init__(self, num_classes, ignore_index=255, device=None):\n",
    "        self.C = num_classes\n",
    "        self.ignore = ignore_index\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.conf = torch.zeros((self.C, self.C), dtype=torch.float64, device=self.device)\n",
    "    @torch.no_grad()\n",
    "    def update(self, logits, target):\n",
    "        pred = logits.argmax(1)\n",
    "        tgt  = target\n",
    "        valid = (tgt != self.ignore)\n",
    "        if valid.any():\n",
    "            pred = pred[valid]\n",
    "            tgt  = tgt[valid]\n",
    "            idx = tgt * self.C + pred\n",
    "            bins = torch.bincount(idx, minlength=self.C*self.C).reshape(self.C, self.C).to(self.conf.dtype)\n",
    "            self.conf += bins\n",
    "    @torch.no_grad()\n",
    "    def get(self):\n",
    "        h = self.conf\n",
    "        diag = torch.diag(h)\n",
    "        sum_row = h.sum(1)\n",
    "        sum_col = h.sum(0)\n",
    "        denom_iou = sum_row + sum_col - diag\n",
    "        iou  = torch.where(denom_iou > 0, diag / denom_iou, torch.nan)\n",
    "        dice = torch.where((sum_row + sum_col) > 0, (2*diag) / (sum_row + sum_col), torch.nan)\n",
    "        miou  = torch.nanmean(iou).item()\n",
    "        mdice = torch.nanmean(dice).item()\n",
    "        return miou, mdice\n",
    "    def reset(self):\n",
    "        self.conf.zero_()\n",
    "\n",
    "def visualize_segmentation(images, true_masks, logits, epoch, save_path, num_samples=4):\n",
    "    pred = logits.argmax(1)\n",
    "    fig, axes = plt.subplots(3, num_samples, figsize=(16, 12))\n",
    "    mean = torch.tensor([0.485,0.456,0.406], device=images.device).view(3,1,1)\n",
    "    std  = torch.tensor([0.229,0.224,0.225], device=images.device).view(3,1,1)\n",
    "    for i in range(min(num_samples, images.size(0))):\n",
    "        img = torch.clamp(images[i]*std + mean, 0, 1).permute(1,2,0).cpu().numpy()\n",
    "        axes[0,i].imshow(img); axes[0,i].set_title(f\"Original {i+1}\"); axes[0,i].axis('off')\n",
    "        gt = true_masks[i].cpu().numpy(); gt_rgb = np.zeros((*gt.shape,3))\n",
    "        pr = pred[i].cpu().numpy();      pr_rgb = np.zeros((*pr.shape,3))\n",
    "        for cls in range(NUM_CLASSES):\n",
    "            m1 = (gt==cls); m2 = (pr==cls)\n",
    "            if m1.any(): gt_rgb[m1] = plt.cm.tab20(cls%20)[:3]\n",
    "            if m2.any(): pr_rgb[m2] = plt.cm.tab20(cls%20)[:3]\n",
    "        axes[1,i].imshow(gt_rgb); axes[1,i].set_title(f\"Ground Truth {i+1}\"); axes[1,i].axis('off')\n",
    "        axes[2,i].imshow(pr_rgb); axes[2,i].set_title(f\"Prediction {i+1}\"); axes[2,i].axis('off')\n",
    "    plt.suptitle(f\"Epoch {epoch} - Segmentation Results\", fontsize=16)\n",
    "    plt.tight_layout(); plt.savefig(save_path, dpi=150, bbox_inches='tight'); plt.close()\n",
    "\n",
    "# -----------------------\n",
    "# One-time ADE sanity peek\n",
    "# -----------------------\n",
    "try:\n",
    "    peek = next(iter(downstream_train_loader))\n",
    "    print(\"ADE20K uniques (peek):\", torch.unique(peek[\"masks\"])[:20].cpu())\n",
    "except Exception as e:\n",
    "    print(\"ADE peek skipped:\", e)\n",
    "\n",
    "# -----------------------\n",
    "# Load JEPA parts & build model\n",
    "# -----------------------\n",
    "print(\"Loading pretrained JEPA model...\")\n",
    "jepa_model = MaskJEPA2D(\n",
    "    in_chans=3, tau=0.996, fi1_mask_ratio=0.5,\n",
    "    num_queries=50, num_cross_attn=5, num_self_attn=1, patch_size=8\n",
    ").to(device)\n",
    "\n",
    "weights_path = \"./jepa_training_output/jepa_model_epoch_1.pt\"\n",
    "if os.path.exists(weights_path):\n",
    "    ckpt = torch.load(weights_path, map_location=device)\n",
    "    jepa_model.context_encoder.load_state_dict(ckpt['backbone_state_dict'])\n",
    "    jepa_model.pixel_decoder.load_state_dict(ckpt['pixel_decoder_state_dict'])\n",
    "    if 'ds16_state_dict' in ckpt: jepa_model.ds16.load_state_dict(ckpt['ds16_state_dict'])\n",
    "    if 'ds32_state_dict' in ckpt: jepa_model.ds32.load_state_dict(ckpt['ds32_state_dict'])\n",
    "    print(\"Loaded: context_encoder, pixel_decoder\",\n",
    "          \"+ ds16\" if 'ds16_state_dict' in ckpt else \"\",\n",
    "          \"+ ds32\" if 'ds32_state_dict' in ckpt else \"\")\n",
    "else:\n",
    "    print(\"WARNING: no JEPA weights found; FT from scratch\")\n",
    "\n",
    "model = JEPASegmentationModel(jepa_model, num_classes=NUM_CLASSES, mid_channels=128).to(device)\n",
    "model = model.to(memory_format=torch.channels_last)\n",
    "\n",
    "# -----------------------\n",
    "# Train config\n",
    "# -----------------------\n",
    "num_epochs_ft = 40\n",
    "base_lr_ft    = 2e-4\n",
    "weight_decay  = 0.01\n",
    "print_every   = 50     # batch print interval (CE, mIoU, Dice)\n",
    "GRAD_ACCUM    = 1      # set >1 if you need micro-batching for memory\n",
    "\n",
    "def cosine(epoch):\n",
    "    return 0.5*(1 + np.cos(np.pi*epoch/num_epochs_ft))\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=base_lr_ft, weight_decay=weight_decay)\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=cosine)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=IGNORE_INDEX)\n",
    "scaler = GradScaler('cuda')\n",
    "\n",
    "ft_save_dir = \"./jepa_finetuning_output\"\n",
    "os.makedirs(ft_save_dir, exist_ok=True)\n",
    "\n",
    "print(\"Starting fine-tuning (Fusion head, CE only)...\")\n",
    "print(f\"Train batches: {len(downstream_train_loader)}, Val batches: {len(downstream_val_loader)}\")\n",
    "print(f\"Model trainable params: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "best_miou = 0.0\n",
    "train_ce_hist, val_ce_hist, val_miou_hist, val_dice_hist = [], [], [], []\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "for epoch in range(num_epochs_ft):\n",
    "    model.train()\n",
    "    epoch_ce_accum = 0.0\n",
    "\n",
    "    # per-epoch meters\n",
    "    epoch_meter = StreamingSegMetrics(NUM_CLASSES, IGNORE_INDEX, device=device)\n",
    "    print_meter = StreamingSegMetrics(NUM_CLASSES, IGNORE_INDEX, device=device)\n",
    "\n",
    "    for bidx, batch in enumerate(downstream_train_loader):\n",
    "        images = batch[\"images\"].to(device, non_blocking=True).to(memory_format=torch.channels_last)\n",
    "        masks  = batch[\"masks\"].to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # micro-batching if GRAD_ACCUM > 1\n",
    "        num_micro = max(1, GRAD_ACCUM)\n",
    "        chunk = math.ceil(images.size(0) / num_micro)\n",
    "\n",
    "        loss_sum = 0.0\n",
    "        for i in range(0, images.size(0), chunk):\n",
    "            imgs = images[i:i+chunk]\n",
    "            msks = masks[i:i+chunk]\n",
    "            with autocast('cuda'):\n",
    "                logits = model(imgs)\n",
    "                loss   = criterion(logits, fix_masks(msks)) / num_micro\n",
    "\n",
    "            # update GPU metrics before backward (keeps VRAM usage similar)\n",
    "            with torch.no_grad():\n",
    "                epoch_meter.update(logits, fix_masks(msks))\n",
    "                print_meter.update(logits, fix_masks(msks))\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            loss_sum += float(loss.detach())\n",
    "\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer); scaler.update()\n",
    "\n",
    "        epoch_ce_accum += loss_sum\n",
    "\n",
    "        if (bidx % print_every) == 0:\n",
    "            miou_b, dice_b = print_meter.get()  # only 2 scalar syncs\n",
    "            print(f\"  Batch {bidx:4d}/{len(downstream_train_loader)} | CE: {loss_sum:.4f} | mIoU: {miou_b:.4f} | Dice: {dice_b:.4f}\")\n",
    "            print_meter.reset()\n",
    "\n",
    "        del images, masks, logits, loss\n",
    "\n",
    "    epoch_ce = epoch_ce_accum / max(1, len(downstream_train_loader))\n",
    "    train_ce_hist.append(epoch_ce)\n",
    "    scheduler.step()\n",
    "\n",
    "    # ---- Validation ----\n",
    "    model.eval()\n",
    "    val_ce = 0.0\n",
    "    val_meter = StreamingSegMetrics(NUM_CLASSES, IGNORE_INDEX, device=device)\n",
    "    with torch.no_grad():\n",
    "        for batch in downstream_val_loader:\n",
    "            images = batch[\"images\"].to(device, non_blocking=True).to(memory_format=torch.channels_last)\n",
    "            masks  = batch[\"masks\"].to(device, non_blocking=True)\n",
    "            with autocast('cuda'):\n",
    "                logits = model(images)\n",
    "                ce = criterion(logits, fix_masks(masks))\n",
    "            val_ce += ce.item()\n",
    "            val_meter.update(logits, fix_masks(masks))\n",
    "            del images, masks, logits, ce\n",
    "\n",
    "    val_ce /= max(1, len(downstream_val_loader))\n",
    "    mean_miou, mean_dice = val_meter.get()\n",
    "    val_ce_hist.append(val_ce); val_miou_hist.append(mean_miou); val_dice_hist.append(mean_dice)\n",
    "\n",
    "    print(f\"Epoch {epoch+1:02d}/{num_epochs_ft} | Train CE: {epoch_ce:.4f} | Val CE: {val_ce:.4f} | mIoU: {mean_miou:.4f} | Dice: {mean_dice:.4f} | LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "\n",
    "    # Save best by mIoU\n",
    "    if mean_miou > best_miou:\n",
    "        best_miou = mean_miou\n",
    "        best_path = os.path.join(ft_save_dir, \"best_segmentation_model.pt\")\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'miou': mean_miou,\n",
    "            'dice': mean_dice\n",
    "        }, best_path)\n",
    "        print(f\"  New best mIoU! Saved -> {best_path}\")\n",
    "\n",
    "    # Visualizations every 2 epochs\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        with torch.no_grad():\n",
    "            vis_batch = next(iter(downstream_val_loader))\n",
    "            vis_images = vis_batch[\"images\"].to(device).to(memory_format=torch.channels_last)\n",
    "            vis_masks  = vis_batch[\"masks\"].to(device)\n",
    "            with autocast('cuda'):\n",
    "                vis_logits = model(vis_images)\n",
    "            vis_path = os.path.join(ft_save_dir, f\"seg_epoch_{epoch+1:03d}.png\")\n",
    "            visualize_segmentation(vis_images, fix_masks(vis_masks), vis_logits, epoch+1, vis_path)\n",
    "            print(f\"  Saved visualization: {vis_path}\")\n",
    "            del vis_batch, vis_images, vis_masks, vis_logits\n",
    "\n",
    "    torch.cuda.empty_cache(); gc.collect()\n",
    "\n",
    "print(\"Fine-tuning completed.\")\n",
    "print(f\"Best mIoU: {best_miou:.4f}\")\n",
    "\n",
    "# Save final\n",
    "final_path = os.path.join(ft_save_dir, \"final_segmentation_model.pt\")\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'train_ce_losses': train_ce_hist,\n",
    "    'val_ce_losses': val_ce_hist,\n",
    "    'val_mious': val_miou_hist,\n",
    "    'val_dices': val_dice_hist,\n",
    "    'best_miou': best_miou\n",
    "}, final_path)\n",
    "print(f\"Final model saved: {final_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd48be0-5c57-4f2a-a3cc-8ca588a4fe5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a3e0f1-abbd-4fbb-b663-b9e8c26ee23e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b1651e-50d4-45be-b3ed-f6d50a30e5ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
