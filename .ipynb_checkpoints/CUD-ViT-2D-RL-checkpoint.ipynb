{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7ca0ae4-dfec-45d9-be52-2da2be04f654",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f61c141b-d594-4ab5-b119-17cb37c381a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "ImageNet-100 already exists. Skipping download.\n",
      "ADE20K already exists. Skipping download.\n",
      "Ready.\n"
     ]
    }
   ],
   "source": [
    "import os, torch\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # syncs CUDA so you see the true Python stack\n",
    "torch.autograd.set_detect_anomaly(True)   # pinpoints the backward op\n",
    "\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import socket\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Check if datasets already exist\n",
    "\n",
    "imagenet_path = os.path.join('.', 'imagenet-100')\n",
    "ade_path = os.path.join('.', 'ADEChallengeData2016')\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "\n",
    "# Load ImageNet-100 and save to visible folder\n",
    "if os.path.exists(imagenet_path):\n",
    "    print(\"ImageNet-100 already exists. Skipping download.\")\n",
    "else:\n",
    "    print(\"Downloading ImageNet-100...\")\n",
    "    try:\n",
    "        img100 = load_dataset(\"clane9/imagenet-100\")\n",
    "        img100.save_to_disk(imagenet_path)\n",
    "        print(\"ImageNet-100 saved to ./imagenet-100/\")\n",
    "        print(f\"Train samples: {len(img100['train'])}\")\n",
    "        print(f\"Val samples: {len(img100['validation'])}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ImageNet-100 failed: {e}\")\n",
    "\n",
    "# Download ADE20K manually if not exists\n",
    "if os.path.exists(ade_path):\n",
    "    print(\"ADE20K already exists. Skipping download.\")\n",
    "else:\n",
    "    print(\"Downloading ADE20K...\")\n",
    "    try:\n",
    "        socket.setdefaulttimeout(60)\n",
    "        url = \"http://data.csail.mit.edu/places/ADEchallenge/ADEChallengeData2016.zip\"\n",
    "        zip_path = \"ADEChallengeData2016.zip\"\n",
    "        urllib.request.urlretrieve(url, zip_path)\n",
    "        \n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall('.')\n",
    "        os.remove(zip_path)\n",
    "        print(\"ADE20K downloaded\")\n",
    "    except Exception as e:\n",
    "        print(f\"ADE20K download failed: {e}\")\n",
    "\n",
    "print(\"Ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d3c5b2f-9fdd-45de-ac71-2b36f25cbe62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from PIL import Image\n",
    "from datasets import load_from_disk\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from math import ceil\n",
    "\n",
    "# Reproducibility\n",
    "seed = 1337\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "IMG_SIZE = 384\n",
    "\n",
    "# =============================================================================\n",
    "# JEPA ImageNet Dataset for Pretraining\n",
    "# =============================================================================\n",
    "class JEPADataset(Dataset):\n",
    "    def __init__(self, root=\"./imagenet-100\", split=\"train\", img_size=384):\n",
    "        self.dataset = load_from_disk(root)[split]\n",
    "        self.img_size = img_size\n",
    "        # Image transforms\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(img_size, scale=(0.2, 1.0)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load and transform image\n",
    "        img = self.dataset[idx][\"image\"].convert(\"RGB\")\n",
    "        img_tensor = self.transform(img)\n",
    "        return {\n",
    "            \"image\": img_tensor\n",
    "        }\n",
    "\n",
    "# =============================================================================\n",
    "# ADE20K Dataset for Segmentation\n",
    "# =============================================================================\n",
    "class ADE20KDataset(Dataset):\n",
    "    def __init__(self, root=\"ADEChallengeData2016\", split=\"training\", img_size=384):\n",
    "        self.img_dir = os.path.join(root, \"images\", split)\n",
    "        self.ann_dir = os.path.join(root, \"annotations\", split)\n",
    "        self.img_size = img_size\n",
    "        self.items = []\n",
    "        \n",
    "        # Find image-mask pairs\n",
    "        for img_path in glob.glob(os.path.join(self.img_dir, \"*.jpg\")):\n",
    "            stem = os.path.splitext(os.path.basename(img_path))[0]\n",
    "            ann_path = os.path.join(self.ann_dir, stem + \".png\")\n",
    "            if os.path.exists(ann_path):\n",
    "                self.items.append((img_path, ann_path))\n",
    "        self.items.sort()\n",
    "        \n",
    "        # Image transforms\n",
    "        self.img_transform = transforms.Compose([\n",
    "            transforms.Resize((img_size, img_size), interpolation=Image.BILINEAR),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, ann_path = self.items[idx]\n",
    "        \n",
    "        # Load image and mask\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(ann_path)\n",
    "        \n",
    "        # Apply transforms\n",
    "        img = self.img_transform(img)\n",
    "        mask = mask.resize((self.img_size, self.img_size), resample=Image.NEAREST)\n",
    "        mask = torch.from_numpy(np.array(mask, dtype=\"int64\"))\n",
    "        \n",
    "        return img, mask\n",
    "\n",
    "def jepa_collate(batch):\n",
    "    images = torch.stack([item[\"image\"] for item in batch])\n",
    "    return {\"images\": images}\n",
    "\n",
    "def ade_collate(batch):\n",
    "    imgs, masks = zip(*batch)\n",
    "    return {\"images\": torch.stack(imgs), \"masks\": torch.stack(masks)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1b4e19-61ea-4ecc-b5cc-805adefcfd79",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Setup Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c1a374-0879-4594-87f2-4393d27b32bc",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b07abc4-50b4-4d6c-86df-40de563a7aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def compute_patch_grid(image_shape, patch_size):\n",
    "    \"\"\"\n",
    "    image_shape expected: (C, H, W)\n",
    "    \"\"\"\n",
    "    _, H, W = image_shape\n",
    "    n_h = H // patch_size\n",
    "    n_w = W // patch_size\n",
    "    P = n_h * n_w\n",
    "    cropped_shape = (n_h * patch_size, n_w * patch_size)\n",
    "    return n_h, n_w, P, cropped_shape\n",
    "\n",
    "\n",
    "def extract_patch_embeddings_from_feature_map(feats: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    feats: [N, D, n_h, n_w]  -> returns [N, P, D]  (P = n_h * n_w)\n",
    "    \"\"\"\n",
    "    N, D, n_h, n_w = feats.shape\n",
    "    # Move D to last, then flatten spatial dims\n",
    "    return feats.permute(0, 2, 3, 1).reshape(N, -1, D)\n",
    "\n",
    "\n",
    "def compute_denoising_loss(self, denoised_prediction, original_input):\n",
    "    # Downsample target to match denoised prediction size\n",
    "    target_downsampled = F.interpolate(\n",
    "        original_input, \n",
    "        size=denoised_prediction.shape[-2:], \n",
    "        mode='bilinear', \n",
    "        align_corners=False\n",
    "    )\n",
    "    return F.mse_loss(denoised_prediction, target_downsampled)\n",
    "\n",
    "def compute_reconstruction_loss(preds: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute reconstruction loss for predicted vs target embeddings\n",
    "    \"\"\"\n",
    "    return F.mse_loss(preds, targets, reduction=\"mean\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def update_ema(target_net: nn.Module, online_net: nn.Module, tau: float):\n",
    "    \"\"\"\n",
    "    Update target network parameters using exponential moving average\n",
    "    \"\"\"\n",
    "    for t_param, s_param in zip(target_net.parameters(), online_net.parameters()):\n",
    "        t_param.data.mul_(tau).add_(s_param.data, alpha=1 - tau)\n",
    "\n",
    "def unpatchify_embeddings(emb: torch.Tensor, n_h: int, n_w: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Convert patch embeddings back to 2D feature map\n",
    "    emb: [N, P, D] -> [N, D, n_h, n_w]\n",
    "    \"\"\"\n",
    "    N, P, D = emb.shape\n",
    "    emb_4d = emb.view(N, n_h, n_w, D)\n",
    "    return emb_4d.permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "def generate_fi1_mask(fi1_shape: tuple, mask_ratio: float = 0.5, patch_size: int = 8, device='cuda'):\n",
    "    B, D, H8, W8 = fi1_shape  # e.g., [B, D, 28, 28] for 224x224 images\n",
    "    \n",
    "    # Calculate number of patches\n",
    "    n_patches_h = H8 // patch_size  # 28/8 = 3\n",
    "    n_patches_w = W8 // patch_size  # 28/8 = 3\n",
    "    total_patches = n_patches_h * n_patches_w  # 9 patches total\n",
    "    \n",
    "    num_masked = int(mask_ratio * total_patches)  # e.g., 4 patches masked\n",
    "    \n",
    "    # Generate mask\n",
    "    fi1_mask = torch.zeros(B, H8 * W8, dtype=torch.bool, device=device)\n",
    "    \n",
    "    for b in range(B):\n",
    "        # Randomly select which patches to mask\n",
    "        masked_patch_ids = torch.randperm(total_patches, device=device)[:num_masked]\n",
    "        \n",
    "        for patch_id in masked_patch_ids:\n",
    "            # Convert patch_id to patch coordinates\n",
    "            ph = patch_id // n_patches_w\n",
    "            pw = patch_id % n_patches_w\n",
    "            \n",
    "            # Convert to pixel coordinates in Fi1\n",
    "            h_start = ph * patch_size\n",
    "            h_end = min(h_start + patch_size, H8)\n",
    "            w_start = pw * patch_size  \n",
    "            w_end = min(w_start + patch_size, W8)\n",
    "            \n",
    "            # Mask this patch in flattened Fi1\n",
    "            for h in range(h_start, h_end):\n",
    "                for w in range(w_start, w_end):\n",
    "                    fi1_mask[b, h * W8 + w] = True\n",
    "    \n",
    "    return fi1_mask  # [B, H8*W8]\n",
    "\n",
    "def apply_fi1_mask_tokens(fi1_features: torch.Tensor, fi1_mask: torch.Tensor, mask_token: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Apply masking to Fi1 features using learned mask tokens\n",
    "    \n",
    "    Args:\n",
    "        fi1_features: (B, D, H8, W8) Fi1 feature maps\n",
    "        fi1_mask: (B, H8*W8) boolean mask\n",
    "        mask_token: (1, D, 1, 1) learned mask token\n",
    "    \n",
    "    Returns:\n",
    "        masked_fi1: Fi1 with mask tokens at masked positions\n",
    "    \"\"\"\n",
    "    B, D, H8, W8 = fi1_features.shape\n",
    "    \n",
    "    # Reshape mask to match feature dimensions\n",
    "    mask_2d = fi1_mask.reshape(B, H8, W8).unsqueeze(1).expand(-1, D, -1, -1)\n",
    "    \n",
    "    # Replace masked positions with mask token\n",
    "    masked_fi1 = torch.where(mask_2d, mask_token.expand(B, D, H8, W8), fi1_features)\n",
    "    \n",
    "    return masked_fi1\n",
    "\n",
    "def visualize_jepa_patch_quality(\n",
    "    original: torch.Tensor,\n",
    "    predicted_features: torch.Tensor,\n",
    "    target_features: torch.Tensor,\n",
    "    patch_mask: torch.Tensor,\n",
    "    epoch: int,\n",
    "    save_path: str,\n",
    "    patch_size: int = 16,\n",
    "):\n",
    "\n",
    "    # ----- robust image-to-display -----\n",
    "    def _to_display_img(x: torch.Tensor) -> np.ndarray:\n",
    "        x = x.detach().cpu()\n",
    "        if x.ndim == 3 and x.shape[0] in (1, 3):\n",
    "            xc = x.clone()\n",
    "            mn, mx = float(xc.min()), float(xc.max())\n",
    "            if 0.0 <= mn and mx <= 1.0:\n",
    "                pass  # already [0,1]\n",
    "            elif -3.5 <= mn <= 3.5 and -3.5 <= mx <= 3.5:\n",
    "                # assume ImageNet norm\n",
    "                mean = torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n",
    "                std  = torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)\n",
    "                xc = xc * std + mean\n",
    "            else:\n",
    "                # min-max to [0,1]\n",
    "                xc = (xc - mn) / (max(mx - mn, 1e-6))\n",
    "            img = xc.permute(1, 2, 0).numpy()\n",
    "        else:\n",
    "            arr = x.numpy()\n",
    "            arr = (arr - arr.min()) / (arr.max() - arr.min() + 1e-6)\n",
    "            img = arr\n",
    "        return np.clip(img, 0.0, 1.0)\n",
    "\n",
    "    # ---- prep first image ----\n",
    "    original_np = _to_display_img(original[0])\n",
    "    H, W = original_np.shape[:2]\n",
    "    n_h, n_w = H // patch_size, W // patch_size\n",
    "\n",
    "    # ---- per-masked-tile losses ----\n",
    "    pred0 = predicted_features[0].detach().cpu().numpy()   # [M, D]\n",
    "    tgt0  = target_features[0].detach().cpu().numpy()      # [M, D]\n",
    "    if pred0.size == 0:\n",
    "        per_patch_losses = np.zeros((0,), dtype=np.float32)\n",
    "    else:\n",
    "        diff = pred0 - tgt0\n",
    "        per_patch_losses = (diff * diff).mean(axis=-1)     # [M]\n",
    "\n",
    "    if per_patch_losses.size > 0:\n",
    "        lo, hi = float(per_patch_losses.min()), float(per_patch_losses.max())\n",
    "        denom = (hi - lo) if (hi > lo) else 1.0\n",
    "        normalized_quality = 1.0 - ((per_patch_losses - lo) / denom)\n",
    "    else:\n",
    "        normalized_quality = np.zeros((0,), dtype=np.float32)\n",
    "\n",
    "    # ---- masked indices ----\n",
    "    mask0 = patch_mask[0].detach().cpu().view(-1)          # [P]\n",
    "    masked_indices = mask0.nonzero(as_tuple=False).squeeze(1).numpy()  # [K]\n",
    "\n",
    "    # ---- figure ----\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(14, 5))\n",
    "\n",
    "    # Left: original\n",
    "    axs[0].imshow(original_np, interpolation='nearest')\n",
    "    axs[0].set_title(\"Original Image\")\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    # Center: mask overlay (red) with black grid\n",
    "    masked_img = original_np.copy()\n",
    "    overlay = masked_img.copy()\n",
    "    red = np.array([1.0, 0.0, 0.0], dtype=np.float32)\n",
    "\n",
    "    for pidx in masked_indices:\n",
    "        if pidx < 0 or pidx >= n_h * n_w:\n",
    "            continue\n",
    "        ih, iw = divmod(int(pidx), n_w)\n",
    "        h0, h1 = ih * patch_size, (ih + 1) * patch_size\n",
    "        w0, w1 = iw * patch_size, (iw + 1) * patch_size\n",
    "        overlay[h0:h1, w0:w1, :] = red\n",
    "\n",
    "    alpha_center = 0.35\n",
    "    masked_img = (1 - alpha_center) * masked_img + alpha_center * overlay\n",
    "    axs[1].imshow(np.clip(masked_img, 0.0, 1.0), interpolation='nearest')\n",
    "    axs[1].set_title(f\"Epoch {epoch} - JEPA Analysis\\nMasked Patches (Red)\\n{int(mask0.sum())}/{mask0.numel()} masked\")\n",
    "    axs[1].axis('off')\n",
    "\n",
    "    # Right: reconstruction quality (bold colored squares)\n",
    "    quality_img = original_np.copy()\n",
    "    colormap = plt.get_cmap('RdYlGn')  # green=good, red=bad\n",
    "    alpha_patch = 0.85                 # strong overlay for bold squares\n",
    "    grid_thick = max(1, patch_size // 16)  # thicker grid lines\n",
    "\n",
    "    limit = min(len(normalized_quality), len(masked_indices))\n",
    "    for idx in range(limit):\n",
    "        patch_idx = int(masked_indices[idx])\n",
    "        if patch_idx < 0 or patch_idx >= n_h * n_w:\n",
    "            continue\n",
    "\n",
    "        ih, iw = divmod(patch_idx, n_w)\n",
    "        h0, h1 = ih * patch_size, (ih + 1) * patch_size\n",
    "        w0, w1 = iw * patch_size, (iw + 1) * patch_size\n",
    "\n",
    "        q = float(np.asarray(normalized_quality[idx]).mean())\n",
    "        if not np.isfinite(q):\n",
    "            q = 0.0\n",
    "        q = float(np.clip(q, 0.0, 1.0))\n",
    "\n",
    "        color = np.asarray(colormap(q))[:3]  # (3,)\n",
    "        patch = quality_img[h0:h1, w0:w1, :]\n",
    "        quality_img[h0:h1, w0:w1, :] = (1 - alpha_patch) * patch + alpha_patch * color[None, None, :]\n",
    "\n",
    "        # thicker black grid lines\n",
    "        quality_img[h0:h0+grid_thick, w0:w1, :] = 0.0\n",
    "        quality_img[h0:h1, w0:w0+grid_thick, :] = 0.0\n",
    "\n",
    "    axs[2].imshow(np.clip(quality_img, 0.0, 1.0), interpolation='nearest')\n",
    "    axs[2].set_title(\"Reconstruction Quality\\n(Green=Good, Red=Poor)\")\n",
    "    axs[2].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc29848b-d9ec-470f-ab48-cf0ef0ebc91e",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1482914c-c5cc-4bdc-a542-9a16af4d0681",
   "metadata": {},
   "source": [
    "#### Patch Embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32492f0b-d551-454b-98a1-8eaeba92041b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PatchEmbed2D: remove undefined pos_embed add (or implement it properly) ---\n",
    "class PatchEmbed2D(nn.Module):\n",
    "    def __init__(self, in_chans: int, embed_dim: int, patch_size: int):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)                              # [B, D, n_h, n_w]\n",
    "        B, D, n_h, n_w = x.shape\n",
    "        x = x.view(B, D, n_h * n_w).transpose(1, 2)   # [B, P, D]\n",
    "        # (no pos_embed here)\n",
    "        return self.norm(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c48cc20-ccaa-406d-96b5-152b82a28c9c",
   "metadata": {},
   "source": [
    "#### Context Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a0b79e6-3f28-41d4-8c84-b24b0d0ee056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D Image → Patches → 2D Feature Map → ViT → 2D Feature Map → Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "092cafac-bc3b-4eb5-8023-14af08ef28a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "\n",
    "class ContextEncoder2D(nn.Module):\n",
    "    \"\"\"\n",
    "    Swin Transformer v2 Tiny context encoder configured for 384×384.\n",
    "    Returns patch tokens (no CLS) and the token grid size (Ht, Wt).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"swinv2_tiny_window8_256\",\n",
    "        pretrained: bool = True,\n",
    "        img_size: int = 384,\n",
    "        strict_img_size: bool = False,\n",
    "        dynamic_img_pad: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Build timm Swin v2; set img_size=384 and allow dynamic padding\n",
    "        self.swin = timm.create_model(\n",
    "            model_name,\n",
    "            pretrained=pretrained,\n",
    "            img_size=img_size,          # <- run at 384\n",
    "            num_classes=0,              # no classifier head\n",
    "            global_pool='',             # keep token grid\n",
    "            features_only=False,\n",
    "            strict_img_size=strict_img_size,\n",
    "            dynamic_img_pad=dynamic_img_pad,\n",
    "        )\n",
    "        self.embed_dim = self.swin.num_features\n",
    "        self.patch_size = 4  # Swin uses patch4 embed\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        x: [B, C, H, W]  (H,W multiples of 32 recommended; 384 works)\n",
    "        returns:\n",
    "          tokens: [B, P, D]  (CLS-free)\n",
    "          (Ht, Wt): token grid size at the final stage (~ H/32, W/32)\n",
    "        \"\"\"\n",
    "        feats = self.swin.forward_features(x)   # [B, L, D] or sometimes [B, Ht, Wt, D] / [B, D, Ht, Wt]\n",
    "\n",
    "        if feats.dim() == 3:                    # [B, L, D]\n",
    "            tokens = feats\n",
    "            P = tokens.shape[1]\n",
    "            Ht = int(math.sqrt(P))\n",
    "            Wt = P // Ht\n",
    "        else:\n",
    "            # Handle both [B, D, Ht, Wt] and [B, Ht, Wt, D]\n",
    "            if feats.shape[1] == self.embed_dim:        # [B, D, Ht, Wt]\n",
    "                B, D, Ht, Wt = feats.shape\n",
    "                tokens = feats.permute(0, 2, 3, 1).reshape(B, Ht * Wt, D)\n",
    "            else:                                       # [B, Ht, Wt, D]\n",
    "                B, Ht, Wt, D = feats.shape\n",
    "                tokens = feats.reshape(B, Ht * Wt, D)\n",
    "\n",
    "        return tokens, (Ht, Wt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daef4278-2a43-4310-a0e8-3b84c86fbd77",
   "metadata": {},
   "source": [
    "#### Pixel Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0ed2a18-45ac-4a06-8a93-857858d792d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "def _simple_pos_embed_2d(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Simple 2D positional embedding - just scaled coordinates\"\"\"\n",
    "    B, D, H, W = x.shape\n",
    "    device, dtype = x.device, x.dtype\n",
    "    \n",
    "    # Create coordinate grids\n",
    "    y_coords = torch.linspace(0, 1, H, device=device, dtype=dtype)\n",
    "    x_coords = torch.linspace(0, 1, W, device=device, dtype=dtype) \n",
    "    yy, xx = torch.meshgrid(y_coords, x_coords, indexing='ij')\n",
    "    \n",
    "    # Simple embedding: just use x,y coordinates repeated\n",
    "    pos_embed = torch.stack([xx, yy], dim=0).unsqueeze(0)  # [1, 2, H, W]\n",
    "    pos_embed = pos_embed.expand(B, -1, -1, -1)            # [B, 2, H, W]\n",
    "    \n",
    "    # Repeat to match channel dimension\n",
    "    pos_embed = pos_embed.repeat(1, D//2, 1, 1)            # [B, D, H, W]\n",
    "    if pos_embed.shape[1] < D:\n",
    "        pos_embed = F.pad(pos_embed, (0, 0, 0, 0, 0, D - pos_embed.shape[1]))\n",
    "    \n",
    "    return pos_embed * 0.1  # Scale down to not overwhelm features\n",
    "\n",
    "class PixelDecoder2D(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple FPN-style pixel decoder. Much cleaner than deformable attention.\n",
    "    Keeps same interface as the complex version.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 embed_dim: int,\n",
    "                 *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # Handle per-level channels (C3, C4, C5 might be different)\n",
    "        in_chs: Optional[Tuple[int,int,int]] = kwargs.get(\"in_channels_per_level\", None)\n",
    "        if in_chs is None:\n",
    "            in_chs = (in_channels, in_channels, in_channels)\n",
    "\n",
    "        # Project each level to common embedding dimension\n",
    "        self.lateral_convs = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(c, embed_dim, kernel_size=1),\n",
    "                nn.GroupNorm(32 if embed_dim >= 32 else 1, embed_dim),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ) for c in in_chs\n",
    "        ])\n",
    "\n",
    "        # FPN fusion layers (reduce aliasing during upsampling)\n",
    "        self.fpn_convs = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1),\n",
    "                nn.GroupNorm(32 if embed_dim >= 32 else 1, embed_dim),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ) for _ in range(3)\n",
    "        ])\n",
    "\n",
    "        # Output heads\n",
    "        self.fi1_head = nn.Sequential(\n",
    "            nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(32 if embed_dim >= 32 else 1, embed_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(embed_dim, embed_dim, kernel_size=1)\n",
    "        )\n",
    "        \n",
    "        self.flast_head = nn.Sequential(\n",
    "            nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(32 if embed_dim >= 32 else 1, embed_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(embed_dim, embed_dim, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, feats_multi: List[torch.Tensor], input_hw: Tuple[int, int]):\n",
    "        \"\"\"\n",
    "        Simple FPN forward pass.\n",
    "        \n",
    "        feats_multi: [C3, C4, C5] at strides [1/8, 1/16, 1/32]\n",
    "        input_hw: (H, W) full resolution size\n",
    "        \n",
    "        Returns:\n",
    "            Fi1: [B, D, H/8, W/8] - feature at stride 8\n",
    "            F_last: [B, D, H/4, W/4] - feature at stride 4\n",
    "        \"\"\"\n",
    "        H, W = input_hw\n",
    "        c3, c4, c5 = feats_multi\n",
    "        \n",
    "        # 1. Lateral connections - project to common dimension\n",
    "        p5 = self.lateral_convs[2](c5)  # [B, D, H/32, W/32]\n",
    "        p4 = self.lateral_convs[1](c4)  # [B, D, H/16, W/16]  \n",
    "        p3 = self.lateral_convs[0](c3)  # [B, D, H/8, W/8]\n",
    "\n",
    "        # Add simple positional encoding\n",
    "        p5 = p5 + _simple_pos_embed_2d(p5)\n",
    "        p4 = p4 + _simple_pos_embed_2d(p4)\n",
    "        p3 = p3 + _simple_pos_embed_2d(p3)\n",
    "\n",
    "        # 2. Top-down pathway (FPN fusion)\n",
    "        # P5 -> P4\n",
    "        p5_up = F.interpolate(p5, size=p4.shape[-2:], mode='bilinear', align_corners=False)\n",
    "        p4 = p4 + p5_up\n",
    "        p4 = self.fpn_convs[1](p4)\n",
    "\n",
    "        # P4 -> P3  \n",
    "        p4_up = F.interpolate(p4, size=p3.shape[-2:], mode='bilinear', align_corners=False)\n",
    "        p3 = p3 + p4_up\n",
    "        p3 = self.fpn_convs[0](p3)  # This is our main feature at 1/8 stride\n",
    "\n",
    "        # 3. Generate outputs\n",
    "        # Fi1 at stride 8 (same as p3)\n",
    "        Fi1 = self.fi1_head(p3)  # [B, D, H/8, W/8]\n",
    "\n",
    "        # F_last at stride 4 (upsample p3)\n",
    "        p3_upsampled = F.interpolate(p3, size=(H // 4, W // 4), mode='bilinear', align_corners=False)\n",
    "        F_last = self.flast_head(p3_upsampled)  # [B, D, H/4, W/4]\n",
    "\n",
    "        return Fi1, F_last"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc70616-0541-46bf-b758-f3a21837c90f",
   "metadata": {},
   "source": [
    "#### PredictorHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "134dab00-bff5-4170-8920-5edbd39a2d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CrossAttentionBlock2D(nn.Module):\n",
    "    \"\"\"Cross-attention block for JEPA predictor\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim: int, num_heads: int = 8):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.cross_attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Feedforward\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_dim * 4, embed_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, queries, features):\n",
    "        # Cross-attention: queries attend to features\n",
    "        attn_out, _ = self.cross_attn(queries, features, features)\n",
    "        queries = self.norm1(queries + attn_out)\n",
    "        \n",
    "        # Feedforward\n",
    "        ffn_out = self.ffn(queries)\n",
    "        queries = self.norm2(queries + ffn_out)\n",
    "        \n",
    "        return queries\n",
    "\n",
    "class SelfAttentionBlock2D(nn.Module):\n",
    "    \"\"\"Self-attention block for query refinement\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim: int, num_heads: int = 8):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Feedforward  \n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_dim * 4, embed_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, queries):\n",
    "        # Self-attention: queries attend to themselves\n",
    "        attn_out, _ = self.self_attn(queries, queries, queries)\n",
    "        queries = self.norm1(queries + attn_out)\n",
    "        \n",
    "        # Feedforward\n",
    "        ffn_out = self.ffn(queries)\n",
    "        queries = self.norm2(queries + ffn_out)\n",
    "        \n",
    "        return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8588178-79a2-4f49-a13c-096521121c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Predictor2D(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_dim: int,\n",
    "                 num_queries: int,\n",
    "                 num_heads: int = None,\n",
    "                 # accept both old and new arg names:\n",
    "                 num_cross_blocks: int = None,\n",
    "                 num_self_blocks: int = None,\n",
    "                 num_cross_attn: int = None,\n",
    "                 num_self_attn: int = None):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_queries = num_queries\n",
    "\n",
    "        # choose a valid num_heads if not provided\n",
    "        if num_heads is None:\n",
    "            for h in (16, 12, 8, 6, 4, 3, 2, 1):\n",
    "                if embed_dim % h == 0:\n",
    "                    num_heads = h\n",
    "                    break\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # harmonize naming: prefer *attn if provided, else *blocks, else paper defaults (9/2)\n",
    "        if num_cross_attn is not None:\n",
    "            L = num_cross_attn\n",
    "        elif num_cross_blocks is not None:\n",
    "            L = num_cross_blocks\n",
    "        else:\n",
    "            L = 9\n",
    "        if num_self_attn is not None:\n",
    "            M = num_self_attn\n",
    "        elif num_self_blocks is not None:\n",
    "            M = num_self_blocks\n",
    "        else:\n",
    "            M = 2\n",
    "\n",
    "        # learnable queries\n",
    "        self.query_embed = nn.Parameter(torch.zeros(1, num_queries, embed_dim))\n",
    "        nn.init.trunc_normal_(self.query_embed, std=0.02)\n",
    "\n",
    "        # Cross-attention blocks (Mask2Former-ish: norm -> cross-attn -> add, norm -> FFN -> add)\n",
    "        self.cross_blocks = nn.ModuleList([\n",
    "            nn.ModuleDict(dict(\n",
    "                attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True),\n",
    "                ffn  = nn.Sequential(\n",
    "                    nn.Linear(embed_dim, 4*embed_dim),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Linear(4*embed_dim, embed_dim)\n",
    "                ),\n",
    "                norm1 = nn.LayerNorm(embed_dim),\n",
    "                norm2 = nn.LayerNorm(embed_dim),\n",
    "            )) for _ in range(L)\n",
    "        ])\n",
    "\n",
    "        # Extra self-attention blocks on queries\n",
    "        self.self_blocks = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, batch_first=True)\n",
    "            for _ in range(M)\n",
    "        ])\n",
    "\n",
    "        # projection head f_L to map query outputs back to Fi1 embedding space\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        # learnable mask token for masked Fi1 tiles (used to build K/V when Fi1 is masked)\n",
    "        self.kv_mask_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        nn.init.trunc_normal_(self.kv_mask_token, std=0.02)\n",
    "\n",
    "    @staticmethod\n",
    "    def _add_2d_sincos_pos(feat_2d: torch.Tensor):\n",
    "        \"\"\"\n",
    "        feat_2d: [B, D, H, W] -> returns [B, H*W, D] with fixed 2D sin/cos pos enc added\n",
    "        \"\"\"\n",
    "        import torch.nn.functional as F\n",
    "        B, D, H, W = feat_2d.shape\n",
    "        device = feat_2d.device\n",
    "    \n",
    "        # build sincos grid\n",
    "        y = torch.linspace(-1, 1, steps=H, device=device)\n",
    "        x = torch.linspace(-1, 1, steps=W, device=device)\n",
    "        yy, xx = torch.meshgrid(y, x, indexing='ij')\n",
    "        pos = torch.stack([xx, yy], dim=-1).reshape(1, H, W, 2)  # [1,H,W,2]\n",
    "    \n",
    "        # project to channel dim D using sin/cos\n",
    "        half = D // 2\n",
    "        sin_in = pos[..., 0:1].repeat(1, 1, 1, half)\n",
    "        cos_in = pos[..., 1:2].repeat(1, 1, 1, D - half)\n",
    "        pos_embed = torch.cat([torch.sin(sin_in), torch.cos(cos_in)], dim=-1)  # [1,H,W,D]\n",
    "        if pos_embed.shape[-1] != D:\n",
    "            pos_embed = F.pad(pos_embed, (0, D - pos_embed.shape[-1]))[:, :, :, :D]\n",
    "    \n",
    "        # >>> key fix: match [B, D, H, W] before addition\n",
    "        pos_embed = pos_embed.permute(0, 3, 1, 2)  # [1, D, H, W]\n",
    "    \n",
    "        feat = feat_2d + pos_embed.to(feat_2d.dtype)  # [B, D, H, W]\n",
    "        feat = feat.flatten(2).transpose(1, 2)        # [B, H*W, D]\n",
    "        return feat\n",
    "\n",
    "    def forward(self, Fi1_online: torch.Tensor, Fi1_mask: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Fi1_online: [B, D, H8, W8]   (online pixel-decoder Fi1)\n",
    "        Fi1_mask:   [B, H8*W8] bool  (True=masked positions in Fi1 to reconstruct)\n",
    "        Returns:\n",
    "           pred_masked_feats: [B, M, D] predictions for masked Fi1 positions (M = #masked)\n",
    "           masked_indices:    [B, M] indices of the masked Fi1 positions (or -1 padded)\n",
    "           query_feats:       [B, Q, D] final query embeddings\n",
    "        \"\"\"\n",
    "        B, D, H8, W8 = Fi1_online.shape\n",
    "        Q = self.num_queries\n",
    "\n",
    "        # Build K/V from Fi1 with 2D sincos pos; replace masked tiles with kv_mask_token (NOT zeros)\n",
    "        kv = Fi1_online.clone()  # [B,D,H8,W8]\n",
    "        if Fi1_mask is not None:\n",
    "            mask_2d = Fi1_mask.reshape(B, H8, W8).unsqueeze(1).expand(-1, D, -1, -1)  # [B,D,H8,W8]\n",
    "            kv = torch.where(mask_2d, self.kv_mask_token.view(1, D, 1, 1).expand(B, D, H8, W8), kv)\n",
    "\n",
    "        kv_seq = self._add_2d_sincos_pos(kv)  # [B, H8*W8, D] as K/V\n",
    "\n",
    "        # Queries\n",
    "        q = self.query_embed.expand(B, Q, D)  # [B,Q,D]\n",
    "\n",
    "        # L cross-attention blocks\n",
    "        for blk in self.cross_blocks:\n",
    "            qn = blk['norm1'](q)\n",
    "            attn_out, _ = blk['attn'](qn, kv_seq, kv_seq)  # cross-attn to Fi1+pos\n",
    "            q = q + attn_out\n",
    "            q = q + blk['ffn'](blk['norm2'](q))\n",
    "\n",
    "        # M self-attn blocks on queries\n",
    "        for sblk in self.self_blocks:\n",
    "            q = sblk(q)\n",
    "\n",
    "        # map to Fi1 embedding space\n",
    "        q_proj = self.proj(q)  # [B,Q,D]\n",
    "\n",
    "        # Route query embeddings to masked tiles (simple attention routing)\n",
    "        scores = torch.einsum('bpd,bqd->bpq', kv_seq, q_proj) / (D ** 0.5)  # [B,P,Q]\n",
    "        probs = scores.softmax(dim=-1)  # over queries\n",
    "\n",
    "        if Fi1_mask is not None and Fi1_mask.any():\n",
    "            pred_list, idx_list = [], []\n",
    "            for b in range(B):\n",
    "                mask_b = Fi1_mask[b]  # [P]\n",
    "                if mask_b.any():\n",
    "                    prob_b = probs[b, mask_b]     # [Mb, Q]\n",
    "                    q_b    = q_proj[b]            # [Q, D]\n",
    "                    pred_b = prob_b @ q_b         # [Mb, D]\n",
    "                    pred_list.append(pred_b)\n",
    "                    idx_list.append(mask_b.nonzero(as_tuple=False).squeeze(1))\n",
    "                else:\n",
    "                    pred_list.append(q_proj.new_zeros((0, D)))\n",
    "                    idx_list.append(torch.zeros((0,), dtype=torch.long, device=q_proj.device))\n",
    "            maxM = max([p.size(0) for p in pred_list])\n",
    "            if maxM == 0:\n",
    "                pred_masked_feats = q_proj.new_zeros((B, 0, D))\n",
    "                masked_indices    = q_proj.new_zeros((B, 0), dtype=torch.long)\n",
    "            else:\n",
    "                pred_masked_feats, masked_indices = [], []\n",
    "                for b in range(B):\n",
    "                    m = pred_list[b].size(0)\n",
    "                    pad = maxM - m\n",
    "                    if pad > 0:\n",
    "                        pred_masked_feats.append(F.pad(pred_list[b], (0,0,0,pad)))\n",
    "                        masked_indices.append(F.pad(idx_list[b], (0,pad), value=-1))\n",
    "                    else:\n",
    "                        pred_masked_feats.append(pred_list[b])\n",
    "                        masked_indices.append(idx_list[b])\n",
    "                pred_masked_feats = torch.stack(pred_masked_feats, dim=0)  # [B, M, D]\n",
    "                masked_indices    = torch.stack(masked_indices, dim=0)     # [B, M]\n",
    "        else:\n",
    "            pred_masked_feats = q_proj.new_zeros((B, 0, D))\n",
    "            masked_indices    = q_proj.new_zeros((B, 0), dtype=torch.long)\n",
    "\n",
    "        return pred_masked_feats, masked_indices, q_proj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e858251-e914-43bb-a3ee-fed2dd2938a6",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b57873f-a33a-493c-8f12-7007d2f61b41",
   "metadata": {},
   "source": [
    "#### Dataset & Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9f9b608-bb95-4d53-9c4c-026dbcf591c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "684406633fe641c29792a10aa108c7bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes:\n",
      "JEPA (ImageNet): 126,689 samples\n",
      "ADE20K train: 20,210 samples\n",
      "ADE20K val: 2,000 samples\n",
      "\n",
      "DataLoader info:\n",
      "Pretrain: 1760 batches\n",
      "Train: 422 batches\n",
      "Val: 42 batches\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Create Datasets and DataLoaders\n",
    "# =============================================================================\n",
    "\n",
    "# Training configuration\n",
    "batch_size_pretrain = 72\n",
    "batch_size_downstream = 48\n",
    "\n",
    "# Create dataset instances\n",
    "jepa_dataset = JEPADataset()\n",
    "ade_train_dataset = ADE20KDataset(split=\"training\")\n",
    "ade_val_dataset = ADE20KDataset(split=\"validation\")\n",
    "\n",
    "print(f\"Dataset sizes:\")\n",
    "print(f\"JEPA (ImageNet): {len(jepa_dataset):,} samples\")\n",
    "print(f\"ADE20K train: {len(ade_train_dataset):,} samples\")\n",
    "print(f\"ADE20K val: {len(ade_val_dataset):,} samples\")\n",
    "\n",
    "# JEPA pretraining loader\n",
    "pretrain_loader = DataLoader(\n",
    "    jepa_dataset,\n",
    "    batch_size=batch_size_pretrain,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    collate_fn=jepa_collate\n",
    ")\n",
    "\n",
    "# Downstream fine-tuning loaders\n",
    "downstream_train_loader = DataLoader(\n",
    "    ade_train_dataset,\n",
    "    batch_size=batch_size_downstream,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    collate_fn=ade_collate\n",
    ")\n",
    "\n",
    "downstream_val_loader = DataLoader(\n",
    "    ade_val_dataset,\n",
    "    batch_size=batch_size_downstream,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    collate_fn=ade_collate\n",
    ")\n",
    "\n",
    "print(f\"\\nDataLoader info:\")\n",
    "print(f\"Pretrain: {len(pretrain_loader)} batches\")\n",
    "print(f\"Train: {len(downstream_train_loader)} batches\")\n",
    "print(f\"Val: {len(downstream_val_loader)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8cbec5c-70ba-4fc1-80c2-3913a1866666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([72, 3, 384, 384])\n"
     ]
    }
   ],
   "source": [
    "# Test batch shapes for 2D JEPA\n",
    "batch = next(iter(pretrain_loader))\n",
    "imgs = batch[\"images\"] # (B, C, H, W)\n",
    "\n",
    "print(imgs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dc9778-1db1-4f21-9545-0a9f3df83328",
   "metadata": {},
   "source": [
    "#### Model Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2745fb11-60b9-4acb-8952-522b1f38e261",
   "metadata": {},
   "source": [
    "#### Mask-JEPA setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "800eda25-be4d-4b9d-bace-308655ddba82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "from typing import Tuple, Optional\n",
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "class MaskJEPA2D(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_chans: int,\n",
    "                 num_queries: int = 32,\n",
    "                 num_cross_attn: int = 2,\n",
    "                 num_self_attn: int = 1,\n",
    "                 tau: float = 0.996,\n",
    "                 fi1_mask_ratio: float = 0.5,\n",
    "                 patch_size: int = 8,\n",
    "                 model_name: str = \"swin_tiny_patch4_window7_224\",\n",
    "                 pretrained: bool = True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        \n",
    "        # === Context encoder (timm ViT with pos_embed) ===\n",
    "        self.context_encoder = ContextEncoder2D(model_name=model_name, pretrained=pretrained)\n",
    "\n",
    "        # pull embed_dim & patch_size from the encoder backbone\n",
    "        self.embed_dim = self.context_encoder.embed_dim\n",
    "        self.in_chans = in_chans  # needed for denoising head output\n",
    "\n",
    "        # === Target encoder (EMA, frozen) ===\n",
    "        self.target_encoder = copy.deepcopy(self.context_encoder)\n",
    "        for p in self.target_encoder.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # === Pixel decoders ===\n",
    "        self.pixel_decoder = PixelDecoder2D(\n",
    "            in_channels=self.embed_dim,\n",
    "            embed_dim=self.embed_dim\n",
    "        )\n",
    "        self.pixel_decoder_ema = copy.deepcopy(self.pixel_decoder)\n",
    "        for p in self.pixel_decoder_ema.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # === Downsampling convs for C4 / C5 ===\n",
    "        self.ds16 = nn.Sequential(\n",
    "            nn.Conv2d(self.embed_dim, self.embed_dim, 3, stride=2, padding=1), nn.GELU(),\n",
    "            nn.Conv2d(self.embed_dim, self.embed_dim, 3, padding=1),           nn.GELU(),\n",
    "        )\n",
    "        self.ds32 = nn.Sequential(\n",
    "            nn.Conv2d(self.embed_dim, self.embed_dim, 3, stride=2, padding=1), nn.GELU(),\n",
    "            nn.Conv2d(self.embed_dim, self.embed_dim, 3, padding=1),           nn.GELU(),\n",
    "        )\n",
    "\n",
    "        # === JEPA Predictor ===\n",
    "        self.predictor = Predictor2D(\n",
    "            embed_dim=self.embed_dim,\n",
    "            num_queries=num_queries,\n",
    "            num_cross_attn=num_cross_attn,\n",
    "            num_self_attn=num_self_attn\n",
    "        )\n",
    "\n",
    "        # === Denoising head ===\n",
    "        self.denoising_head = nn.Conv2d(self.embed_dim, in_chans, kernel_size=1)\n",
    "\n",
    "        # === EMA tau config ===\n",
    "        self.tau = tau\n",
    "        self.tau_base  = tau\n",
    "        self.tau_final = 1.0\n",
    "\n",
    "        self.fi1_mask_ratio = fi1_mask_ratio\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Paper-correct noise path:\n",
    "          - Add Gaussian noise at s_last=4 and expand\n",
    "          - Online branch sees noisy input\n",
    "          - Target branch sees clean input\n",
    "        \"\"\"\n",
    "        B, C, H, W = x.shape\n",
    "        device = x.device\n",
    "    \n",
    "        # ---------- (A) add noise ----------\n",
    "        s_last = 4\n",
    "        H4, W4 = H // s_last, W // s_last\n",
    "        sigma = 0.4\n",
    "    \n",
    "        eps_lr = torch.randn(B, C, H4, W4, device=device, dtype=x.dtype) * sigma\n",
    "        eps_full = eps_lr.repeat_interleave(s_last, dim=2).repeat_interleave(s_last, dim=3)\n",
    "        x_noisy = x + eps_full\n",
    "    \n",
    "        # ---------- (B) ONLINE BRANCH ----------\n",
    "        tokens_online, (enc_h, enc_w) = self.context_encoder(x_noisy)  # [B,P,D], (Ht,Wt)\n",
    "        feat_online = tokens_online.transpose(1, 2).reshape(\n",
    "            B, self.embed_dim, enc_h, enc_w\n",
    "        )\n",
    "        \n",
    "        C3 = F.interpolate(feat_online, size=(H//8, W//8), mode='bilinear', align_corners=False)\n",
    "        x16 = self.ds16(C3)\n",
    "        x32 = self.ds32(x16)\n",
    "        C4  = F.interpolate(x16, size=(H//16, W//16), mode='bilinear', align_corners=False)\n",
    "        C5  = F.interpolate(x32, size=(H//32, W//32), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        f_i1_online, f_last_online = self.pixel_decoder([C3, C4, C5], (H, W))\n",
    "\n",
    "        # ---------- (C) TARGET BRANCH ----------\n",
    "        with torch.no_grad():\n",
    "            tokens_target, _ = self.target_encoder(x)\n",
    "            feat_target = tokens_target.transpose(1, 2).reshape(\n",
    "                B, self.embed_dim, enc_h, enc_w\n",
    "            )\n",
    "        \n",
    "            C3t = F.interpolate(feat_target, size=(H//8, W//8), mode='bilinear', align_corners=False)\n",
    "            x16t = self.ds16(C3t)\n",
    "            x32t = self.ds32(x16t)\n",
    "            C4t  = F.interpolate(x16t, size=(H//16, W//16), mode='bilinear', align_corners=False)\n",
    "            C5t  = F.interpolate(x32t, size=(H//32, W//32), mode='bilinear', align_corners=False)\n",
    "        \n",
    "            f_i1_target, _ = self.pixel_decoder_ema([C3t, C4t, C5t], (H, W))\n",
    "\n",
    "        # ---------- (D) Fi1 MASK ----------\n",
    "        fi1_mask = generate_fi1_mask(\n",
    "            fi1_shape=f_i1_online.shape,\n",
    "            mask_ratio=self.fi1_mask_ratio,\n",
    "            patch_size = self.patch_size,\n",
    "            device=device\n",
    "        )  # [B, H8*W8] bool\n",
    "    \n",
    "        # ---------- (E) PREDICTOR ----------\n",
    "        predicted_features, masked_indices, q_proj = self.predictor(f_i1_online, fi1_mask)\n",
    "    \n",
    "        # ---------- (F) TARGET GATHER + LN ----------\n",
    "        D = f_i1_target.shape[1]\n",
    "        fi1_h, fi1_w = f_i1_target.shape[-2:]\n",
    "        target_seq = f_i1_target.permute(0, 2, 3, 1).reshape(B, fi1_h * fi1_w, D)\n",
    "        target_seq = F.layer_norm(target_seq, (D,))\n",
    "    \n",
    "        if masked_indices.numel() > 0:\n",
    "            pad_mask = (masked_indices >= 0)\n",
    "            safe_idx = masked_indices.clamp_min(0)\n",
    "            b_idx = torch.arange(B, device=device).unsqueeze(-1).expand_as(safe_idx)\n",
    "            target_masked_full = target_seq[b_idx, safe_idx]\n",
    "            target_masked = target_masked_full * pad_mask.unsqueeze(-1).to(target_masked_full.dtype)\n",
    "        else:\n",
    "            target_masked = target_seq.new_zeros((B, 0, D))\n",
    "    \n",
    "        # ---------- (G) DENOISING ----------\n",
    "        denoised_prediction = self.denoising_head(f_last_online)\n",
    "    \n",
    "        return {\n",
    "            'predicted_features': predicted_features,\n",
    "            'target_masked':      target_masked,\n",
    "            'mask_info':          (q_proj, masked_indices),\n",
    "            'denoised_prediction': denoised_prediction,\n",
    "            'original_input':     x,\n",
    "            'fi1_mask':           fi1_mask,\n",
    "            'mask_indices':       masked_indices,\n",
    "            'eps_target':         eps_lr\n",
    "        }\n",
    "\n",
    "    def set_ema_tau(self, tau: float):\n",
    "        self.tau = float(tau)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_ema(self):\n",
    "        update_ema(self.target_encoder, self.context_encoder, tau=self.tau)\n",
    "        update_ema(self.pixel_decoder_ema, self.pixel_decoder, tau=self.tau)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82445529-dfbd-4c51-a79a-cdf937ce49e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_lambda(epoch: int) -> float:\n",
    "    if epoch < warmup_epochs:\n",
    "        # Linear warmup: scale from 0 → 1 over warmup_epochs\n",
    "        return float(epoch) / float(max(1, warmup_epochs))\n",
    "    elif epoch < warmup_epochs + 3:\n",
    "        # Aggressive phase: stay at full rate for 3 epochs after warmup\n",
    "        return 1.0\n",
    "    else:\n",
    "        # Exponential decay phase\n",
    "        decay_start = warmup_epochs + 3\n",
    "        decay_factor = 0.7 ** (epoch - decay_start)\n",
    "        return max(decay_factor, 0.3)  # Don't decay below 30% of base_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "893ffc30-3b03-4286-8a8c-92bd38599e77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model...\n",
      "Gradient checkpointing enabled\n",
      "Model parameters: 119,820,413\n",
      "Batch size: 72\n",
      "Model config: patch_size=8, mask_ratio=0.5, queries=8, cross_attn=5, self_attn=1\n",
      "Epoch 1/20\n",
      "[probe] target: mean=-0.018 std=1.196\n",
      "[probe] pred  : mean=-0.007 std=0.271\n",
      "  Batch 0/1760 - TotalSc: 2.8302, ReconSc: 1.3562, DenoiseSc: 1.4740\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 152\u001b[0m\n\u001b[1;32m    149\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m recon_loss \u001b[38;5;241m+\u001b[39m denoise_loss\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_loss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# Gradient clipping\u001b[39;00m\n\u001b[1;32m    155\u001b[0m scaler\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.9/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.9/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.9/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.amp import GradScaler, autocast\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gc\n",
    "import math\n",
    "import time   # for epoch timing\n",
    "use_bf16 = torch.cuda.is_bf16_supported()  # True on A100/RTX 40xx/etc\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Training configuration\n",
    "num_epochs = 20\n",
    "warmup_epochs = 0\n",
    "base_lr = 1e-4\n",
    "weight_decay = 0.05\n",
    "\n",
    "# Early stopping config\n",
    "early_stop_patience = 5\n",
    "best_total_sc = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "# Enable memory optimizations\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "\n",
    "def enable_gradient_checkpointing(model):\n",
    "    \"\"\"Enable gradient checkpointing for timm ViT blocks\"\"\"\n",
    "    if hasattr(model.context_encoder, 'vit') and hasattr(model.context_encoder.vit, 'blocks'):\n",
    "        for block in model.context_encoder.vit.blocks:\n",
    "            if hasattr(block, 'set_grad_checkpointing'):\n",
    "                block.set_grad_checkpointing(True)\n",
    "    if hasattr(model.target_encoder, 'vit') and hasattr(model.target_encoder.vit, 'blocks'):\n",
    "        for block in model.target_encoder.vit.blocks:\n",
    "            if hasattr(block, 'set_grad_checkpointing'):\n",
    "                block.set_grad_checkpointing(True)\n",
    "\n",
    "# Create model\n",
    "print(\"Creating model...\")\n",
    "model = MaskJEPA2D(\n",
    "    in_chans=3,\n",
    "    tau=0.996,\n",
    "    fi1_mask_ratio=0.5,\n",
    "    num_queries=8,\n",
    "    num_cross_attn=5,\n",
    "    num_self_attn=1,\n",
    "    patch_size=8\n",
    ").to(device)\n",
    "\n",
    "D = model.embed_dim\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "enable_gradient_checkpointing(model)\n",
    "print(\"Gradient checkpointing enabled\")\n",
    "\n",
    "# Mixed precision scaler\n",
    "scaler = GradScaler('cuda')\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=base_lr, weight_decay=weight_decay)\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "# Create save directory\n",
    "save_dir = \"./jepa_training_output\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "best_ckpt_path = os.path.join(save_dir, \"best_jepa_model.pt\")\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Memory cleanup\"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# Print model info\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "print(f\"Batch size: {batch_size_pretrain}\")\n",
    "\n",
    "ps = model.patch_size\n",
    "mr = model.fi1_mask_ratio\n",
    "nq = model.predictor.num_queries\n",
    "nca = len(model.predictor.cross_blocks)\n",
    "nsa = len(model.predictor.self_blocks)\n",
    "\n",
    "print(f\"Model config: patch_size={ps}, mask_ratio={mr}, queries={nq}, cross_attn={nca}, self_attn={nsa}\")\n",
    "\n",
    "# EMA ramp setup\n",
    "planned_updates_per_epoch = len(pretrain_loader)\n",
    "max_updates = num_epochs * planned_updates_per_epoch\n",
    "global_update = 0\n",
    "\n",
    "# Training loop\n",
    "train_losses = []\n",
    "best_snapshot = None  # will hold best weights for final save\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()   # start timer\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    epoch_recon_loss = 0.0\n",
    "    epoch_denoise_loss = 0.0\n",
    "    \n",
    "    clear_memory()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for batch_idx, batch in enumerate(pretrain_loader):\n",
    "        images = batch[\"images\"].to(device, non_blocking=True)\n",
    "        \n",
    "        # Forward pass with mixed precision (bf16 if available, else fp16)\n",
    "        with autocast(device_type='cuda', dtype=(torch.bfloat16 if use_bf16 else torch.float16)):\n",
    "            outputs = model(images)\n",
    "            \n",
    "            pred = outputs['predicted_features']\n",
    "            tgt = outputs['target_masked']\n",
    "            idx = outputs['mask_indices']\n",
    "            valid = (idx >= 0).unsqueeze(-1)\n",
    "            \n",
    "            # Reconstruction loss\n",
    "            if pred.numel() == 0 or valid.sum() == 0:\n",
    "                recon_loss = pred.new_zeros(())\n",
    "            else:\n",
    "                diff = (pred - tgt) * valid\n",
    "                recon_loss = diff.pow(2).sum() / valid.sum().clamp_min(1)\n",
    "\n",
    "            # Option 2: Predict clean image x (current default)\n",
    "            x4 = F.interpolate(\n",
    "                outputs['original_input'],\n",
    "                size=outputs['denoised_prediction'].shape[-2:],\n",
    "                mode='bilinear', \n",
    "                align_corners=False\n",
    "            )\n",
    "            denoise_loss = F.mse_loss(outputs['denoised_prediction'], x4)\n",
    "            \n",
    "            # Debug info (first batch only)\n",
    "            if batch_idx == 0:\n",
    "                td = F.interpolate(images, size=outputs['denoised_prediction'].shape[-2:],\n",
    "                                   mode='bilinear', align_corners=False)\n",
    "                pd = outputs['denoised_prediction'].detach()\n",
    "                print(f\"[probe] target: mean={td.mean().item():.3f} std={td.std().item():.3f}\")\n",
    "                print(f\"[probe] pred  : mean={pd.mean().item():.3f} std={pd.std().item():.3f}\")\n",
    "\n",
    "            total_loss = recon_loss + denoise_loss\n",
    "        \n",
    "        # Backward pass\n",
    "        scaler.scale(total_loss).backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        # Optimizer step\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # EMA update with ramp\n",
    "        progress = global_update / max(1, max_updates - 1)\n",
    "        tau_now = model.tau_base + (model.tau_final - model.tau_base) * progress\n",
    "        model.set_ema_tau(tau_now)\n",
    "        model.update_ema()\n",
    "        global_update += 1\n",
    "        \n",
    "        # Track losses\n",
    "        epoch_loss += total_loss.item()\n",
    "        epoch_recon_loss += recon_loss.item()\n",
    "        epoch_denoise_loss += denoise_loss.item()\n",
    "        \n",
    "        # Cleanup\n",
    "        del outputs, recon_loss, denoise_loss, total_loss, images\n",
    "        \n",
    "        # Progress logging\n",
    "        if batch_idx % 50 == 0:\n",
    "            recon_sc = (epoch_recon_loss / (batch_idx + 1)) / D\n",
    "            denoise_sc = (epoch_denoise_loss / (batch_idx + 1))\n",
    "            total_sc = recon_sc + denoise_sc\n",
    "            \n",
    "            print(f\"  Batch {batch_idx}/{len(pretrain_loader)} - \"\n",
    "                  f\"TotalSc: {total_sc:.4f}, ReconSc: {recon_sc:.4f}, DenoiseSc: {denoise_sc:.4f}\")\n",
    "    \n",
    "    # Average losses\n",
    "    epoch_loss /= len(pretrain_loader)\n",
    "    epoch_recon_loss /= len(pretrain_loader)\n",
    "    epoch_denoise_loss /= len(pretrain_loader)\n",
    "    \n",
    "    train_losses.append(epoch_loss)\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Epoch summary\n",
    "    recon_sc_epoch = epoch_recon_loss / D\n",
    "    denoise_sc_epoch = epoch_denoise_loss\n",
    "    total_sc_epoch = recon_sc_epoch + denoise_sc_epoch\n",
    "    \n",
    "    epoch_end_time = time.time()   # end timer\n",
    "    epoch_duration = epoch_end_time - epoch_start_time\n",
    "    print(f\"  Avg losses - TotalSc: {total_sc_epoch:.4f}, \"\n",
    "          f\"ReconSc: {recon_sc_epoch:.4f}, DenoiseSc: {denoise_sc_epoch:.4f}\")\n",
    "    print(f\"  LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "    print(f\"  GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f}GB\")\n",
    "    print(f\"  Time for epoch {epoch+1}: {epoch_duration/60:.2f} minutes\")\n",
    "    \n",
    "    # Periodic evaluation (every 1 epoch here)\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        print(\"  Running evaluation...\")\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            eval_batch = next(iter(pretrain_loader))\n",
    "            eval_images = eval_batch[\"images\"][:4].to(device)\n",
    "            \n",
    "            with autocast(device_type='cuda', dtype=(torch.bfloat16 if use_bf16 else torch.float16)):\n",
    "                eval_outputs = model(eval_images)\n",
    "            \n",
    "            # Visualization\n",
    "            H, W = eval_images.shape[-2:]\n",
    "            fi1_tile = max(H // (H // 8), 1)\n",
    "            \n",
    "            vis_path = os.path.join(save_dir, f\"reconstruction_epoch_{epoch+1:03d}.png\")\n",
    "            \n",
    "            visualize_jepa_patch_quality(\n",
    "                original=eval_images,\n",
    "                predicted_features=eval_outputs['predicted_features'].float(),\n",
    "                target_features=eval_outputs['target_masked'].float(),\n",
    "                patch_mask=eval_outputs['fi1_mask'],\n",
    "                epoch=epoch+1,\n",
    "                save_path=vis_path,\n",
    "                patch_size=fi1_tile\n",
    "            )\n",
    "            \n",
    "            del eval_batch, eval_images, eval_outputs\n",
    "        \n",
    "        print(f\"    Saved visualization: {vis_path}\")\n",
    "        model.train()\n",
    "        clear_memory()\n",
    "    \n",
    "    # ---- Save ONLY when we have a new best TotalSc ----\n",
    "    if total_sc_epoch < best_total_sc:\n",
    "        best_total_sc = total_sc_epoch\n",
    "        epochs_no_improve = 0\n",
    "        best_snapshot = {\n",
    "            'backbone_state_dict': model.context_encoder.state_dict(),\n",
    "            'pixel_decoder_state_dict': model.pixel_decoder.state_dict(),\n",
    "            'transformer_decoder_cross_blocks_state_dict': model.predictor.cross_blocks.state_dict(),\n",
    "        }\n",
    "        torch.save(best_snapshot, best_ckpt_path)\n",
    "        print(f\"    [best] New best TotalSc={best_total_sc:.4f}. Saved: {best_ckpt_path}\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"  [early-stop] No improvement ({epochs_no_improve}/{early_stop_patience}).\")\n",
    "        if epochs_no_improve >= early_stop_patience:\n",
    "            print(f\"  [early-stop] Patience exceeded. Stopping training early.\")\n",
    "            break\n",
    "    # --------------------------------------\n",
    "\n",
    "print(\"Training completed!\")\n",
    "\n",
    "# Save final pretrained weights for downstream use (best-only)\n",
    "final_weights_path = os.path.join(save_dir, 'mask_jepa_pretrained_weights.pt')\n",
    "if best_snapshot is not None:\n",
    "    torch.save(best_snapshot, final_weights_path)\n",
    "else:\n",
    "    # Fallback: save current (shouldn't happen unless no batches ran)\n",
    "    torch.save({\n",
    "        'backbone_state_dict': model.context_encoder.state_dict(),\n",
    "        'pixel_decoder_state_dict': model.pixel_decoder.state_dict(),\n",
    "        'transformer_decoder_cross_blocks_state_dict': model.predictor.cross_blocks.state_dict(),\n",
    "    }, final_weights_path)\n",
    "print(f\"Final pretrained weights saved (best-only): {final_weights_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e91e0e1-edab-4d02-977c-ae09378a1c5b",
   "metadata": {},
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c37c64-1f19-4026-a52e-c5cc9392b5c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ==== Fine-tuning: Fusion head (Fi1 + F_last), CE-only, with fast GPU metrics & batch prints ====\n",
    "import os, gc, math, numpy as np\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.amp import GradScaler, autocast\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "IGNORE_INDEX = 255\n",
    "NUM_CLASSES  = 150\n",
    "\n",
    "# -----------------------\n",
    "# Helpers\n",
    "# -----------------------\n",
    "def _gn_groups(C):\n",
    "    for g in (32,16,8,4,2,1):\n",
    "        if C % g == 0: return g\n",
    "    return 1\n",
    "\n",
    "class DWSepResBlock(nn.Module):\n",
    "    def __init__(self, channels: int, dilation: int = 1):\n",
    "        super().__init__()\n",
    "        self.dw   = nn.Conv2d(channels, channels, 3, padding=dilation, dilation=dilation,\n",
    "                              groups=channels, bias=False)\n",
    "        self.dw_g = nn.GroupNorm(_gn_groups(channels), channels)\n",
    "        self.pw   = nn.Conv2d(channels, channels, 1, bias=False)\n",
    "        self.pw_g = nn.GroupNorm(_gn_groups(channels), channels)\n",
    "        self.act  = nn.ReLU(inplace=True)\n",
    "    def forward(self, x):\n",
    "        y = self.act(self.dw_g(self.dw(x)))\n",
    "        y = self.pw_g(self.pw(y))\n",
    "        return self.act(x + y)\n",
    "\n",
    "class SpatialGate(nn.Module):\n",
    "    def __init__(self, channels: int):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(channels, 1, 3, padding=1)\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(self.conv(x))\n",
    "\n",
    "class FusionSegHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Fi1 (s/8) ↑ to s/4 + F_last (s/4) -> fuse -> 2x DW-sep residual (dil=1,2) -> spatial gate -> 1x1 classes\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, mid_channels: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.fi1_reduce   = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, 1, bias=False),\n",
    "            nn.GroupNorm(_gn_groups(mid_channels), mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.flast_reduce = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, 1, bias=False),\n",
    "            nn.GroupNorm(_gn_groups(mid_channels), mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.fuse   = nn.Sequential(\n",
    "            nn.Conv2d(2*mid_channels, mid_channels, 1, bias=False),\n",
    "            nn.GroupNorm(_gn_groups(mid_channels), mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.refine1 = DWSepResBlock(mid_channels, dilation=1)\n",
    "        self.refine2 = DWSepResBlock(mid_channels, dilation=2)\n",
    "        self.spatial = SpatialGate(mid_channels)\n",
    "        self.cls     = nn.Conv2d(mid_channels, num_classes, 1)\n",
    "\n",
    "    def forward(self, fi1, flast, out_hw):\n",
    "        H, W = out_hw\n",
    "        fi1_up = F.interpolate(fi1, size=flast.shape[-2:], mode='bilinear', align_corners=False)\n",
    "        z = torch.cat([self.fi1_reduce(fi1_up), self.flast_reduce(flast)], dim=1)\n",
    "        z = self.fuse(z)\n",
    "        z = self.refine1(z)\n",
    "        z = self.refine2(z)\n",
    "        z = self.spatial(z)\n",
    "        logits_s4 = self.cls(z)\n",
    "        return F.interpolate(logits_s4, size=(H, W), mode='bilinear', align_corners=False)\n",
    "\n",
    "class JEPASegmentationModel(nn.Module):\n",
    "    \"\"\"\n",
    "    JEPA backbone + pixel decoder -> FusionSegHead (no ASPP, no aux).\n",
    "    \"\"\"\n",
    "    def __init__(self, backbone_model, num_classes=150, mid_channels=128):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone_model.context_encoder\n",
    "        self.pixel_decoder = backbone_model.pixel_decoder\n",
    "        self.embed_dim = backbone_model.embed_dim\n",
    "        self.ds16 = backbone_model.ds16\n",
    "        self.ds32 = backbone_model.ds32\n",
    "        self.head = FusionSegHead(self.embed_dim, mid_channels, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        tokens, (enc_h, enc_w) = self.backbone(x)                 # [B, P, D]\n",
    "        feat = tokens.transpose(1,2).reshape(B, self.embed_dim, enc_h, enc_w)\n",
    "\n",
    "        # pyramid as in pretrain\n",
    "        C3  = F.interpolate(feat, size=(H//8,  W//8),  mode='bilinear', align_corners=False)\n",
    "        x16 = self.ds16(C3)\n",
    "        x32 = self.ds32(x16)\n",
    "        C4  = F.interpolate(x16, size=(H//16, W//16), mode='bilinear', align_corners=False)\n",
    "        C5  = F.interpolate(x32, size=(H//32, W//32), mode='bilinear', align_corners=False)\n",
    "\n",
    "        Fi1, F_last = self.pixel_decoder([C3, C4, C5], (H, W))    # Fi1 ~ s/8, F_last ~ s/4\n",
    "        return self.head(Fi1, F_last, (H, W))                     # [B, K, H, W]\n",
    "\n",
    "def fix_masks(m):\n",
    "    return torch.where((m < 0) | (m >= NUM_CLASSES),\n",
    "                       torch.full_like(m, IGNORE_INDEX),\n",
    "                       m).long()\n",
    "\n",
    "class StreamingSegMetrics:\n",
    "    \"\"\"GPU-side streaming confusion matrix for mIoU/Dice (no CPU stalls).\"\"\"\n",
    "    def __init__(self, num_classes, ignore_index=255, device=None):\n",
    "        self.C = num_classes\n",
    "        self.ignore = ignore_index\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.conf = torch.zeros((self.C, self.C), dtype=torch.float64, device=self.device)\n",
    "    @torch.no_grad()\n",
    "    def update(self, logits, target):\n",
    "        pred = logits.argmax(1)\n",
    "        tgt  = target\n",
    "        valid = (tgt != self.ignore)\n",
    "        if valid.any():\n",
    "            pred = pred[valid]\n",
    "            tgt  = tgt[valid]\n",
    "            idx = tgt * self.C + pred\n",
    "            bins = torch.bincount(idx, minlength=self.C*self.C).reshape(self.C, self.C).to(self.conf.dtype)\n",
    "            self.conf += bins\n",
    "    @torch.no_grad()\n",
    "    def get(self):\n",
    "        h = self.conf\n",
    "        diag = torch.diag(h)\n",
    "        sum_row = h.sum(1)\n",
    "        sum_col = h.sum(0)\n",
    "        denom_iou = sum_row + sum_col - diag\n",
    "        iou  = torch.where(denom_iou > 0, diag / denom_iou, torch.nan)\n",
    "        dice = torch.where((sum_row + sum_col) > 0, (2*diag) / (sum_row + sum_col), torch.nan)\n",
    "        miou  = torch.nanmean(iou).item()\n",
    "        mdice = torch.nanmean(dice).item()\n",
    "        return miou, mdice\n",
    "    def reset(self):\n",
    "        self.conf.zero_()\n",
    "\n",
    "def visualize_segmentation(images, true_masks, logits, epoch, save_path, num_samples=4):\n",
    "    pred = logits.argmax(1)\n",
    "    fig, axes = plt.subplots(3, num_samples, figsize=(16, 12))\n",
    "    mean = torch.tensor([0.485,0.456,0.406], device=images.device).view(3,1,1)\n",
    "    std  = torch.tensor([0.229,0.224,0.225], device=images.device).view(3,1,1)\n",
    "    for i in range(min(num_samples, images.size(0))):\n",
    "        img = torch.clamp(images[i]*std + mean, 0, 1).permute(1,2,0).cpu().numpy()\n",
    "        axes[0,i].imshow(img); axes[0,i].set_title(f\"Original {i+1}\"); axes[0,i].axis('off')\n",
    "        gt = true_masks[i].cpu().numpy(); gt_rgb = np.zeros((*gt.shape,3))\n",
    "        pr = pred[i].cpu().numpy();      pr_rgb = np.zeros((*pr.shape,3))\n",
    "        for cls in range(NUM_CLASSES):\n",
    "            m1 = (gt==cls); m2 = (pr==cls)\n",
    "            if m1.any(): gt_rgb[m1] = plt.cm.tab20(cls%20)[:3]\n",
    "            if m2.any(): pr_rgb[m2] = plt.cm.tab20(cls%20)[:3]\n",
    "        axes[1,i].imshow(gt_rgb); axes[1,i].set_title(f\"Ground Truth {i+1}\"); axes[1,i].axis('off')\n",
    "        axes[2,i].imshow(pr_rgb); axes[2,i].set_title(f\"Prediction {i+1}\"); axes[2,i].axis('off')\n",
    "    plt.suptitle(f\"Epoch {epoch} - Segmentation Results\", fontsize=16)\n",
    "    plt.tight_layout(); plt.savefig(save_path, dpi=150, bbox_inches='tight'); plt.close()\n",
    "\n",
    "# -----------------------\n",
    "# One-time ADE sanity peek\n",
    "# -----------------------\n",
    "try:\n",
    "    peek = next(iter(downstream_train_loader))\n",
    "    print(\"ADE20K uniques (peek):\", torch.unique(peek[\"masks\"])[:20].cpu())\n",
    "except Exception as e:\n",
    "    print(\"ADE peek skipped:\", e)\n",
    "\n",
    "# -----------------------\n",
    "# Load JEPA parts & build model\n",
    "# -----------------------\n",
    "print(\"Loading pretrained JEPA model...\")\n",
    "jepa_model = MaskJEPA2D(\n",
    "    in_chans=3, tau=0.996, fi1_mask_ratio=0.5,\n",
    "    num_queries=50, num_cross_attn=5, num_self_attn=1, patch_size=8\n",
    ").to(device)\n",
    "\n",
    "weights_path = \"./jepa_training_output/jepa_model_epoch_1.pt\"\n",
    "if os.path.exists(weights_path):\n",
    "    ckpt = torch.load(weights_path, map_location=device)\n",
    "    jepa_model.context_encoder.load_state_dict(ckpt['backbone_state_dict'])\n",
    "    jepa_model.pixel_decoder.load_state_dict(ckpt['pixel_decoder_state_dict'])\n",
    "    if 'ds16_state_dict' in ckpt: jepa_model.ds16.load_state_dict(ckpt['ds16_state_dict'])\n",
    "    if 'ds32_state_dict' in ckpt: jepa_model.ds32.load_state_dict(ckpt['ds32_state_dict'])\n",
    "    print(\"Loaded: context_encoder, pixel_decoder\",\n",
    "          \"+ ds16\" if 'ds16_state_dict' in ckpt else \"\",\n",
    "          \"+ ds32\" if 'ds32_state_dict' in ckpt else \"\")\n",
    "else:\n",
    "    print(\"WARNING: no JEPA weights found; FT from scratch\")\n",
    "\n",
    "model = JEPASegmentationModel(jepa_model, num_classes=NUM_CLASSES, mid_channels=128).to(device)\n",
    "model = model.to(memory_format=torch.channels_last)\n",
    "\n",
    "# -----------------------\n",
    "# Train config\n",
    "# -----------------------\n",
    "num_epochs_ft = 40\n",
    "base_lr_ft    = 2e-4\n",
    "weight_decay  = 0.01\n",
    "print_every   = 50     # batch print interval (CE, mIoU, Dice)\n",
    "GRAD_ACCUM    = 1      # set >1 if you need micro-batching for memory\n",
    "\n",
    "def cosine(epoch):\n",
    "    return 0.5*(1 + np.cos(np.pi*epoch/num_epochs_ft))\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=base_lr_ft, weight_decay=weight_decay)\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=cosine)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=IGNORE_INDEX)\n",
    "scaler = GradScaler('cuda')\n",
    "\n",
    "ft_save_dir = \"./jepa_finetuning_output\"\n",
    "os.makedirs(ft_save_dir, exist_ok=True)\n",
    "\n",
    "print(\"Starting fine-tuning (Fusion head, CE only)...\")\n",
    "print(f\"Train batches: {len(downstream_train_loader)}, Val batches: {len(downstream_val_loader)}\")\n",
    "print(f\"Model trainable params: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "best_miou = 0.0\n",
    "train_ce_hist, val_ce_hist, val_miou_hist, val_dice_hist = [], [], [], []\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "for epoch in range(num_epochs_ft):\n",
    "    model.train()\n",
    "    epoch_ce_accum = 0.0\n",
    "\n",
    "    # per-epoch meters\n",
    "    epoch_meter = StreamingSegMetrics(NUM_CLASSES, IGNORE_INDEX, device=device)\n",
    "    print_meter = StreamingSegMetrics(NUM_CLASSES, IGNORE_INDEX, device=device)\n",
    "\n",
    "    for bidx, batch in enumerate(downstream_train_loader):\n",
    "        images = batch[\"images\"].to(device, non_blocking=True).to(memory_format=torch.channels_last)\n",
    "        masks  = batch[\"masks\"].to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # micro-batching if GRAD_ACCUM > 1\n",
    "        num_micro = max(1, GRAD_ACCUM)\n",
    "        chunk = math.ceil(images.size(0) / num_micro)\n",
    "\n",
    "        loss_sum = 0.0\n",
    "        for i in range(0, images.size(0), chunk):\n",
    "            imgs = images[i:i+chunk]\n",
    "            msks = masks[i:i+chunk]\n",
    "            with autocast('cuda'):\n",
    "                logits = model(imgs)\n",
    "                loss   = criterion(logits, fix_masks(msks)) / num_micro\n",
    "\n",
    "            # update GPU metrics before backward (keeps VRAM usage similar)\n",
    "            with torch.no_grad():\n",
    "                epoch_meter.update(logits, fix_masks(msks))\n",
    "                print_meter.update(logits, fix_masks(msks))\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            loss_sum += float(loss.detach())\n",
    "\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer); scaler.update()\n",
    "\n",
    "        epoch_ce_accum += loss_sum\n",
    "\n",
    "        if (bidx % print_every) == 0:\n",
    "            miou_b, dice_b = print_meter.get()  # only 2 scalar syncs\n",
    "            print(f\"  Batch {bidx:4d}/{len(downstream_train_loader)} | CE: {loss_sum:.4f} | mIoU: {miou_b:.4f} | Dice: {dice_b:.4f}\")\n",
    "            print_meter.reset()\n",
    "\n",
    "        del images, masks, logits, loss\n",
    "\n",
    "    epoch_ce = epoch_ce_accum / max(1, len(downstream_train_loader))\n",
    "    train_ce_hist.append(epoch_ce)\n",
    "    scheduler.step()\n",
    "\n",
    "    # ---- Validation ----\n",
    "    model.eval()\n",
    "    val_ce = 0.0\n",
    "    val_meter = StreamingSegMetrics(NUM_CLASSES, IGNORE_INDEX, device=device)\n",
    "    with torch.no_grad():\n",
    "        for batch in downstream_val_loader:\n",
    "            images = batch[\"images\"].to(device, non_blocking=True).to(memory_format=torch.channels_last)\n",
    "            masks  = batch[\"masks\"].to(device, non_blocking=True)\n",
    "            with autocast('cuda'):\n",
    "                logits = model(images)\n",
    "                ce = criterion(logits, fix_masks(masks))\n",
    "            val_ce += ce.item()\n",
    "            val_meter.update(logits, fix_masks(masks))\n",
    "            del images, masks, logits, ce\n",
    "\n",
    "    val_ce /= max(1, len(downstream_val_loader))\n",
    "    mean_miou, mean_dice = val_meter.get()\n",
    "    val_ce_hist.append(val_ce); val_miou_hist.append(mean_miou); val_dice_hist.append(mean_dice)\n",
    "\n",
    "    print(f\"Epoch {epoch+1:02d}/{num_epochs_ft} | Train CE: {epoch_ce:.4f} | Val CE: {val_ce:.4f} | mIoU: {mean_miou:.4f} | Dice: {mean_dice:.4f} | LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "\n",
    "    # Save best by mIoU\n",
    "    if mean_miou > best_miou:\n",
    "        best_miou = mean_miou\n",
    "        best_path = os.path.join(ft_save_dir, \"best_segmentation_model.pt\")\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'miou': mean_miou,\n",
    "            'dice': mean_dice\n",
    "        }, best_path)\n",
    "        print(f\"  New best mIoU! Saved -> {best_path}\")\n",
    "\n",
    "    # Visualizations every 2 epochs\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        with torch.no_grad():\n",
    "            vis_batch = next(iter(downstream_val_loader))\n",
    "            vis_images = vis_batch[\"images\"].to(device).to(memory_format=torch.channels_last)\n",
    "            vis_masks  = vis_batch[\"masks\"].to(device)\n",
    "            with autocast('cuda'):\n",
    "                vis_logits = model(vis_images)\n",
    "            vis_path = os.path.join(ft_save_dir, f\"seg_epoch_{epoch+1:03d}.png\")\n",
    "            visualize_segmentation(vis_images, fix_masks(vis_masks), vis_logits, epoch+1, vis_path)\n",
    "            print(f\"  Saved visualization: {vis_path}\")\n",
    "            del vis_batch, vis_images, vis_masks, vis_logits\n",
    "\n",
    "    torch.cuda.empty_cache(); gc.collect()\n",
    "\n",
    "print(\"Fine-tuning completed.\")\n",
    "print(f\"Best mIoU: {best_miou:.4f}\")\n",
    "\n",
    "# Save final\n",
    "final_path = os.path.join(ft_save_dir, \"final_segmentation_model.pt\")\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'train_ce_losses': train_ce_hist,\n",
    "    'val_ce_losses': val_ce_hist,\n",
    "    'val_mious': val_miou_hist,\n",
    "    'val_dices': val_dice_hist,\n",
    "    'best_miou': best_miou\n",
    "}, final_path)\n",
    "print(f\"Final model saved: {final_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd48be0-5c57-4f2a-a3cc-8ca588a4fe5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a3e0f1-abbd-4fbb-b663-b9e8c26ee23e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b1651e-50d4-45be-b3ed-f6d50a30e5ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
